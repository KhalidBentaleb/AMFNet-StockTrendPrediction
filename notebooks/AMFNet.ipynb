{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83695862-2eae-4a9a-b498-94e274388333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 1: Imports & Configuration\n",
    "# ============================================================\n",
    "# This block imports the required libraries for deep learning, \n",
    "# data processing, and reproducibility.\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import yfinance as yf\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility setup\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration (aligned with the paper)\n",
    "# ------------------------------------------------------------\n",
    "config = {\n",
    "    # Model dimensions\n",
    "    \"d_model\": 64,          # shared embedding dimension\n",
    "    \"dropout_tech\": 0.2,    # dropout for technical MLP\n",
    "    \"dropout_sent\": 0.1,    # dropout for sentiment transformer\n",
    "    \"n_heads\": 4,           # transformer heads\n",
    "    \"n_layers\": 2,          # transformer layers\n",
    "    \n",
    "    # Training setup\n",
    "    \"learning_rate\": 1e-3,  # Adam optimizer with cosine annealing\n",
    "    \"batch_size\": 128,      # as specified in the paper\n",
    "    \"max_epochs\": 200,      # max epochs with early stopping patience=20\n",
    "    \"early_stop_patience\": 20,\n",
    "    \n",
    "    # Task setup\n",
    "    \"n_classes\": 3,         # {-1, 0, +1} → mapped internally to {0,1,2}\n",
    "    \"delta\": 0.002,         # threshold for neutral class (can be tuned)\n",
    "}\n",
    "print(\"[INFO] Global configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8639e-2230-481b-ab57-540cf3a0690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 2: Technical Encoder\n",
    "# ============================================================\n",
    "# This module encodes daily technical indicators into a latent \n",
    "# representation using a two-layer MLP. \n",
    "# \n",
    "# Paper reference:\n",
    "# - Input:  x_t^tech ∈ R^7 (7 domain-standard indicators)\n",
    "# - Output: H_tech ∈ R^64 (shared latent dimension)\n",
    "# - Architecture: 2-layer MLP with ReLU activations and dropout=0.2\n",
    "# ============================================================\n",
    "\n",
    "class TechnicalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode technical indicators.\n",
    "        Args:\n",
    "            x: [batch_size, input_dim] tensor of technical features.\n",
    "        Returns:\n",
    "            [batch_size, hidden_dim] latent representation.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebec777-1aad-4c9d-a74c-e8b6879bff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 3: Sentiment Encoder\n",
    "# ============================================================\n",
    "# This module encodes document-level sentiment using a \n",
    "# Transformer encoder and integrates an uncertainty signal.\n",
    "#\n",
    "# Paper reference:\n",
    "# - Input:  S_t ∈ R^{n_docs × n_models} (n_models=3: FinGPT, HAN, VADER)\n",
    "# - Encoder: 2-layer Transformer, 4 heads, d_model=64, dropout=0.1\n",
    "# - Pooling: mean pooling across documents (masked)\n",
    "# - Uncertainty: average std across models, then averaged across documents\n",
    "# - Fusion: concatenate H_sent with u_t, then project back to hidden_dim\n",
    "# ============================================================\n",
    "\n",
    "class SentimentEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: project sentiment scores into latent space\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Step 2: transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Step 6: projection after concatenating uncertainty\n",
    "        self.linear_proj = nn.Linear(hidden_dim + 1, hidden_dim)\n",
    "\n",
    "    def compute_uncertainty(self, St, lengths):\n",
    "        \"\"\"\n",
    "        Compute uncertainty score as described in the paper (Eq. 7).\n",
    "        Args:\n",
    "            St: [batch_size, n_docs, n_models]\n",
    "            lengths: [batch_size] actual number of docs per sample\n",
    "        Returns:\n",
    "            u_t: [batch_size, 1] scalar uncertainty per sample\n",
    "        \"\"\"\n",
    "        # std across sentiment models, shape = [B, n_docs]\n",
    "        std_doc = St.std(dim=-1, unbiased=False)\n",
    "        \n",
    "        # mask padding documents\n",
    "        mask = torch.arange(St.size(1), device=St.device)[None, :] >= lengths[:, None]\n",
    "        std_doc = std_doc.masked_fill(mask, 0.0)\n",
    "        \n",
    "        # average across valid documents\n",
    "        denom = lengths.clamp_min(1).float()\n",
    "        u_t = std_doc.sum(dim=1) / denom\n",
    "        return u_t.unsqueeze(1)  # [B,1]\n",
    "\n",
    "    def forward(self, St, lengths):\n",
    "        \"\"\"\n",
    "        Encode daily sentiment matrix.\n",
    "        Args:\n",
    "            St: [B, n_docs, n_models] sentiment scores\n",
    "            lengths: [B] number of valid docs per sample\n",
    "        Returns:\n",
    "            H_sent*: [B, hidden_dim] enriched sentiment representation\n",
    "                     (includes uncertainty)\n",
    "        \"\"\"\n",
    "        # If no-news case: return None to rely on technical encoder only\n",
    "        if lengths is None or (lengths == 0).all():\n",
    "            return None\n",
    "\n",
    "        # Step 1: embed sentiments\n",
    "        H = self.embedding(St)  # [B, n_docs, hidden_dim]\n",
    "        \n",
    "        # Step 2: encode with transformer (mask padded docs)\n",
    "        mask = torch.arange(St.size(1), device=St.device)[None, :] >= lengths[:, None]\n",
    "        H = self.transformer(H, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Step 3: mean pooling across valid documents\n",
    "        denom = lengths.clamp_min(1).float().unsqueeze(1)\n",
    "        H_sum = H.masked_fill(mask.unsqueeze(-1), 0.0).sum(dim=1)\n",
    "        H_sent = H_sum / denom  # [B, hidden_dim]\n",
    "        \n",
    "        # Step 4: compute uncertainty\n",
    "        uncertainty = self.compute_uncertainty(St, lengths)  # [B,1]\n",
    "        \n",
    "        # Step 5: concatenate uncertainty\n",
    "        H_sent_aug = torch.cat([H_sent, uncertainty], dim=-1)  # [B, hidden_dim+1]\n",
    "        \n",
    "        # Step 6: project back to hidden_dim\n",
    "        return self.linear_proj(H_sent_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1731c1f-7f82-4e11-88d0-7c7164eba15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 4: Fusion Gate\n",
    "# ============================================================\n",
    "# This block adaptively fuses the outputs of the technical and\n",
    "# sentiment branches using a gating mechanism equivalent to \n",
    "# attention weights (Eq. 10–11 in the paper).\n",
    "#\n",
    "# Paper reference:\n",
    "# - Input:  [H_tech, H_sent] ∈ R^d\n",
    "# - Gate:   α = softmax(W_att · [H_tech || H_sent] + b), α ∈ R^2\n",
    "# - Output: H_fused = α1 * H_tech + α2 * H_sent\n",
    "# - No-news case: α = [1, 0] → rely entirely on technical branch\n",
    "# ============================================================\n",
    "\n",
    "class FusionGate(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Linear layer projects concatenated [Htech || Hsent] into 2 logits\n",
    "        self.Watt = nn.Linear(hidden_dim * 2, 2)\n",
    "\n",
    "    def forward(self, Htech, Hsent=None):\n",
    "        \"\"\"\n",
    "        Adaptive fusion of technical and sentiment encoders.\n",
    "        Args:\n",
    "            Htech: [B, hidden_dim] technical representation\n",
    "            Hsent: [B, hidden_dim] sentiment representation or None\n",
    "        Returns:\n",
    "            Hfused: [B, hidden_dim] fused representation\n",
    "            alpha : [B, 2] gating weights [w_tech, w_sent]\n",
    "        \"\"\"\n",
    "        # Case: no-news (sentiment branch missing)\n",
    "        if Hsent is None:\n",
    "            B, d = Htech.size()\n",
    "            alpha = torch.zeros(B, 2, device=Htech.device)\n",
    "            alpha[:, 0] = 1.0  # full weight to technical branch\n",
    "            return Htech, alpha\n",
    "\n",
    "        # Case: both modalities available\n",
    "        combined = torch.cat([Htech, Hsent], dim=-1)  # [B, 2*hidden_dim]\n",
    "        alpha = F.softmax(self.Watt(combined), dim=-1)  # [B, 2]\n",
    "        \n",
    "        # Weighted sum across modalities\n",
    "        Hfused = alpha[:, 0].unsqueeze(1) * Htech + alpha[:, 1].unsqueeze(1) * Hsent\n",
    "        return Hfused, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8574966-ac0f-48a2-a50e-651f67d483ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 5: AMFNet (Full Model)\n",
    "# ============================================================\n",
    "# This class implements the full Adaptive Modality Fusion \n",
    "# Network (AMFNet) as described in the paper.\n",
    "#\n",
    "# Paper reference:\n",
    "# - Technical encoder: MLP (7 → 64)\n",
    "# - Sentiment encoder: Transformer (2 layers, 4 heads, d=64)\n",
    "# - Fusion gate: attention-like gating (Eq. 10–11)\n",
    "# - Prediction: MLP head, softmax over 3 classes (Eq. 12)\n",
    "#\n",
    "# Input:\n",
    "#   xtech   : [B, tech_dim]        daily technical indicators\n",
    "#   St      : [B, n_docs, 3]      daily sentiment matrix (padded)\n",
    "#   lengths : [B]                 number of valid docs per sample\n",
    "#\n",
    "# Output:\n",
    "#   logits : [B, n_classes]       class scores before softmax\n",
    "#   alpha  : [B, 2]               fusion weights [w_tech, w_sent]\n",
    "# ============================================================\n",
    "\n",
    "class AMFNet(nn.Module):\n",
    "    def __init__(self, tech_dim=7, sent_dim=3, hidden_dim=64, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.tech_encoder = TechnicalEncoder(input_dim=tech_dim, hidden_dim=hidden_dim)\n",
    "        self.sent_encoder = SentimentEncoder(input_dim=sent_dim, hidden_dim=hidden_dim)\n",
    "        self.fusion = FusionGate(hidden_dim=hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, xtech, St=None, lengths=None):\n",
    "        \"\"\"\n",
    "        Forward pass through AMFNet.\n",
    "        Args:\n",
    "            xtech:   [B, tech_dim]\n",
    "            St:      [B, n_docs, sent_dim] sentiment matrix (optional)\n",
    "            lengths: [B] number of valid documents (for masking)\n",
    "        Returns:\n",
    "            logits: [B, n_classes] classification scores\n",
    "            alpha:  [B, 2] fusion weights\n",
    "        \"\"\"\n",
    "        # Step 1: encode technical features\n",
    "        Htech = self.tech_encoder(xtech)  # [B, hidden_dim]\n",
    "\n",
    "        # Step 2: encode sentiment features (if available)\n",
    "        Hsent = self.sent_encoder(St, lengths) if (St is not None) else None\n",
    "\n",
    "        # Step 3: fuse modalities (handles no-news internally)\n",
    "        Hfused, alpha = self.fusion(Htech, Hsent)\n",
    "\n",
    "        # Step 4: classification head (Eq. 12)\n",
    "        logits = self.classifier(Hfused)  # [B, n_classes]\n",
    "\n",
    "        return logits, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b53070-3226-41ce-8f73-d27835b8e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 6: Example Usage\n",
    "# ============================================================\n",
    "# We test AMFNet with dummy data to ensure the forward pass \n",
    "# runs correctly. Shapes are consistent with the paper:\n",
    "# - Technical features: 7 indicators\n",
    "# - Sentiment matrix: n_docs × 3 models (FinGPT, HAN, VADER)\n",
    "# ============================================================\n",
    "\n",
    "# Dummy batch setup\n",
    "batch_size, tech_dim, n_docs, sent_dim = 8, 7, 5, 3\n",
    "xtech = torch.randn(batch_size, tech_dim).to(device)          # [B, 7]\n",
    "St = torch.randn(batch_size, n_docs, sent_dim).to(device)     # [B, n_docs, 3]\n",
    "lengths = torch.full((batch_size,), n_docs, dtype=torch.long).to(device)  # [B]\n",
    "\n",
    "# Initialize model\n",
    "model = AMFNet(tech_dim=tech_dim, sent_dim=sent_dim, hidden_dim=config[\"d_model\"], n_classes=config[\"n_classes\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits, alpha = model(xtech, St, lengths)\n",
    "\n",
    "# Logs\n",
    "print(\"[TEST] Logits shape:\", logits.shape)      # Expected: [B, 3]\n",
    "print(\"[TEST] Fusion weights shape:\", alpha.shape)  # Expected: [B, 2]\n",
    "print(\"[TEST] Fusion weights example (first sample):\", alpha[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654621c-b3aa-4c04-9eaa-51841daee982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block X: StockDataset\n",
    "# ============================================================\n",
    "# This dataset prepares daily samples for AMFNet:\n",
    "#   - Technical indicators (xtech): 7 features\n",
    "#   - Sentiment matrix (St): [n_docs_max × n_models] (padded)\n",
    "#   - lengths: number of valid documents per sample\n",
    "#   - Label y: {0=Neutral, 1=Buy, 2=Sell}\n",
    "#   - Realized next-day returns (r_t1): float\n",
    "#\n",
    "# Paper reference:\n",
    "# - Labels defined by Eq. (2) with threshold delta\n",
    "# - Sentiment scores normalized into [-1, +1] (Eq. 3–4)\n",
    "# - Zero-doc case handled explicitly (fallback to technical branch)\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df, n_docs_max=10, n_models=3, delta=0.002):\n",
    "        self.df = df.copy()\n",
    "        self.n_docs_max = n_docs_max\n",
    "        self.n_models = n_models\n",
    "        self.delta = delta\n",
    "\n",
    "        # --- Technical indicators (7 features) ---\n",
    "        self.xtech = self.df[[\"RSI\", \"MACD\", \"VWAP\", \"BB_Width\", \"ATR\", \"OBV\", \"ROC\"]].values.astype(np.float32)\n",
    "\n",
    "        # --- Next-day returns and labels (Eq. 2) ---\n",
    "        close = self.df[\"Close\"].values\n",
    "        r_t1 = (np.roll(close, -1) - close) / close\n",
    "\n",
    "        # Labeling with neutral threshold delta\n",
    "        y = np.zeros_like(r_t1, dtype=np.int64)\n",
    "        y[r_t1 > delta] = 1  # Buy\n",
    "        y[r_t1 < -delta] = 2 # Sell\n",
    "\n",
    "        # Drop last row (no next-day return available)\n",
    "        self.y = y[:-1]\n",
    "        self.r_t1 = r_t1[:-1].astype(np.float32)\n",
    "        self.xtech = self.xtech[:-1]\n",
    "\n",
    "        # --- Sentiment matrices (dummy data here) ---\n",
    "        n_days = len(self.y)\n",
    "        self.St = []\n",
    "        self.lengths = []\n",
    "\n",
    "        # MinMax scaler for normalization ([-1,1]), one per model\n",
    "        scalers = [MinMaxScaler(feature_range=(-1, 1)) for _ in range(n_models)]\n",
    "\n",
    "        for _ in range(n_days):\n",
    "            # Variable number of documents [0..n_docs_max]\n",
    "            n_docs = np.random.randint(0, n_docs_max + 1)\n",
    "            self.lengths.append(n_docs)\n",
    "\n",
    "            # Initialize matrix with padding\n",
    "            mat = np.zeros((n_docs_max, n_models), dtype=np.float32)\n",
    "\n",
    "            if n_docs > 0:\n",
    "                # Simulate raw sentiment scores\n",
    "                raw = np.random.uniform(-1, 1, size=(n_docs, n_models))\n",
    "\n",
    "                # Normalize each model column with its own scaler (fit_transform here just for dummy data)\n",
    "                normed = np.zeros_like(raw)\n",
    "                for j in range(n_models):\n",
    "                    normed[:, j] = scalers[j].fit_transform(raw[:, j].reshape(-1, 1)).squeeze()\n",
    "\n",
    "                mat[:n_docs] = normed\n",
    "\n",
    "            self.St.append(mat)\n",
    "\n",
    "        self.St = np.stack(self.St, axis=0)\n",
    "        self.lengths = np.array(self.lengths, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xtech = torch.tensor(self.xtech[idx], dtype=torch.float32)     # [7]\n",
    "        St = torch.tensor(self.St[idx], dtype=torch.float32)           # [n_docs_max, n_models]\n",
    "        length = torch.tensor(self.lengths[idx], dtype=torch.long)     # scalar\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long).squeeze()      # scalar\n",
    "        return xtech, St, length, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e56e8-c9c9-40dd-9b84-2293c43ac68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 8: Training Setup\n",
    "# ============================================================\n",
    "# Parameters:\n",
    "#   - 12 US large-cap tickers (2015–2024)\n",
    "#   - Technical indicators: 7 features\n",
    "#   - Sentiment: 3 models (dummy matrices here, normalized)\n",
    "#   - Label threshold delta=0.002\n",
    "#   - Rolling window: 24m train / 3m val / 3m test (12 folds)\n",
    "#   - Hidden dim=64, batch=128, max_epochs=200, early stop=20\n",
    "#   - Optimizer: Adam, lr=1e-3, CosineAnnealingLR scheduler\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Parameters\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"NVDA\",\"JPM\",\"BAC\",\"JNJ\",\"PFE\",\"XOM\",\"CVX\",\"BA\",\"CAT\",\"KO\",\"PG\"]\n",
    "\n",
    "start_date = \"2015-01-01\"\n",
    "end_date   = \"2024-01-01\"\n",
    "n_docs_max, n_models = 20, 3\n",
    "delta = config[\"delta\"]\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "max_epochs = config[\"max_epochs\"]\n",
    "patience   = config[\"early_stop_patience\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Technical indicators\n",
    "# ----------------------------\n",
    "def compute_indicators(df):\n",
    "    df = df.copy()\n",
    "    # RSI\n",
    "    delta_p = df[\"Close\"].diff()\n",
    "    gain = (delta_p.where(delta_p > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta_p.where(delta_p < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = ema12 - ema26\n",
    "\n",
    "    # VWAP\n",
    "    df[\"VWAP\"] = (df[\"Close\"] * df[\"Volume\"]).cumsum() / df[\"Volume\"].cumsum()\n",
    "\n",
    "    # Bollinger Band Width\n",
    "    mean20 = df[\"Close\"].rolling(20).mean()\n",
    "    std20 = df[\"Close\"].rolling(20).std()\n",
    "    df[\"BB_Width\"] = 4 * std20\n",
    "\n",
    "    # ATR\n",
    "    high_low = df[\"High\"] - df[\"Low\"]\n",
    "    high_close = (df[\"High\"] - df[\"Close\"].shift()).abs()\n",
    "    low_close = (df[\"Low\"] - df[\"Close\"].shift()).abs()\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df[\"ATR\"] = tr.rolling(14).mean()\n",
    "\n",
    "    # OBV\n",
    "    obv = (np.sign(df[\"Close\"].diff()) * df[\"Volume\"]).fillna(0).cumsum()\n",
    "    df[\"OBV\"] = obv\n",
    "\n",
    "    # ROC\n",
    "    df[\"ROC\"] = df[\"Close\"].pct_change(periods=12)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Rolling Window Splits\n",
    "# ----------------------------\n",
    "def rolling_windows(df, train_months=24, val_months=3, test_months=3):\n",
    "    \"\"\"Generate rolling windows: 24m train / 3m val / 3m test.\"\"\"\n",
    "    folds = []\n",
    "    start = df.index.min()\n",
    "    while True:\n",
    "        train_start = start\n",
    "        train_end   = train_start + pd.DateOffset(months=train_months)\n",
    "        val_end     = train_end   + pd.DateOffset(months=val_months)\n",
    "        test_end    = val_end     + pd.DateOffset(months=test_months)\n",
    "\n",
    "        if test_end > df.index.max():\n",
    "            break\n",
    "\n",
    "        train_idx = (df.index >= train_start) & (df.index < train_end)\n",
    "        val_idx   = (df.index >= train_end)   & (df.index < val_end)\n",
    "        test_idx  = (df.index >= val_end)     & (df.index < test_end)\n",
    "\n",
    "        folds.append((df[train_idx], df[val_idx], df[test_idx]))\n",
    "        start = val_end  # slide window\n",
    "\n",
    "    return folds\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Build datasets\n",
    "# ----------------------------\n",
    "all_folds = []\n",
    "for t in tickers:\n",
    "    df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "    df = compute_indicators(df).dropna()\n",
    "\n",
    "    folds = rolling_windows(df)\n",
    "    for train_df, val_df, test_df in folds:\n",
    "        # scale on train only\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[[\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]])\n",
    "        for part_df in [train_df, val_df, test_df]:\n",
    "            part_df.loc[:, [\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]] = scaler.transform(\n",
    "                part_df[[\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]]\n",
    "            )\n",
    "\n",
    "        train_set = StockDataset(train_df, n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "        val_set   = StockDataset(val_df,   n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "        test_set  = StockDataset(test_df,  n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "\n",
    "        all_folds.append((train_set, val_set, test_set))\n",
    "\n",
    "print(f\"[INFO] Generated {len(all_folds)} rolling folds across {len(tickers)} tickers.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: DataLoader builder\n",
    "# ----------------------------\n",
    "def my_collate_fn(batch):\n",
    "    xtechs, Sts, lengths, ys = zip(*batch)\n",
    "    return (torch.stack(xtechs),\n",
    "            torch.stack(Sts),\n",
    "            torch.stack(lengths),\n",
    "            torch.stack(ys))\n",
    "\n",
    "def make_loaders(train_set, val_set, test_set):\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331e2a2-a768-4a76-a3a4-908f9b51becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 9: Training & Evaluation Loop\n",
    "# ============================================================\n",
    "# This block implements the training pipeline for AMFNet:\n",
    "#   - Optimizer: Adam (lr=1e-3)\n",
    "#   - Scheduler: CosineAnnealingLR\n",
    "#   - Loss: CrossEntropyLoss\n",
    "#   - Early stopping: patience=20 on validation F1\n",
    "#   - Metrics: Accuracy, Macro-F1\n",
    "#   - Protocol: rolling windows (24m train / 3m val / 3m test)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate model on given loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xtech, St, lengths, y in loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return total_loss/len(loader), accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "def train_one_fold(train_set, val_set, test_set, fold_id=0):\n",
    "    \"\"\"Train AMFNet on a single rolling window fold.\"\"\"\n",
    "    print(f\"\\n[INFO] Starting Fold {fold_id+1}/{len(all_folds)}\")\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader, val_loader, test_loader = make_loaders(train_set, val_set, test_set)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = AMFNet(tech_dim=7, sent_dim=3, hidden_dim=config[\"d_model\"], n_classes=config[\"n_classes\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"max_epochs\"])\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_f1, best_state, patience_counter = -1, None, 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"max_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        for xtech, St, lengths, y in train_loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config['max_epochs']} | \"\n",
    "              f\"Train Loss={train_loss:.4f} Acc={train_acc:.4f} F1={train_f1:.4f} || \"\n",
    "              f\"Val Loss={val_loss:.4f} Acc={val_acc:.4f} F1={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config[\"early_stop_patience\"]:\n",
    "            print(f\"[INFO] Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best state\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # Final Test\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
    "    print(f\"[RESULT] Fold {fold_id+1}: Test Loss={test_loss:.4f} Acc={test_acc:.4f} F1={test_f1:.4f}\")\n",
    "\n",
    "    return test_loss, test_acc, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07252df8-aafb-4243-90cd-9e17c07c4e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 10: Run All Rolling Window Folds\n",
    "# ============================================================\n",
    "# Train and evaluate AMFNet across all rolling windows.\n",
    "# Report mean ± std across folds (as in the paper).\n",
    "# ============================================================\n",
    "\n",
    "all_results = []\n",
    "for i, (train_set, val_set, test_set) in enumerate(all_folds):\n",
    "    test_loss, test_acc, test_f1 = train_one_fold(train_set, val_set, test_set, fold_id=i)\n",
    "    all_results.append((test_loss, test_acc, test_f1))\n",
    "\n",
    "all_results = np.array(all_results)\n",
    "mean_results = all_results.mean(axis=0)\n",
    "std_results  = all_results.std(axis=0)\n",
    "\n",
    "print(\"\\n[SUMMARY] Final Results across folds:\")\n",
    "print(f\"  Test Loss = {mean_results[0]:.4f} ± {std_results[0]:.4f}\")\n",
    "print(f\"  Test Acc  = {mean_results[1]:.4f} ± {std_results[1]:.4f}\")\n",
    "print(f\"  Test F1   = {mean_results[2]:.4f} ± {std_results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997e35e-9c81-4281-acf4-d41b44a0184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block A: Classification Metrics (Accuracy, Precision, Recall, F1)\n",
    "# ============================================================\n",
    "# This block evaluates AMFNet on classification performance \n",
    "# exactly as reported in the article (Section IV.C).\n",
    "#\n",
    "# Metrics:\n",
    "#   - Global Accuracy\n",
    "#   - Per-class Precision, Recall, F1\n",
    "# Classes:\n",
    "#   {0 = Neutral, 1 = Buy, 2 = Sell}\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classification_metrics(model, loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics (Accuracy, Precision, Recall, F1) \n",
    "    on a given dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model  : trained AMFNet model\n",
    "        loader : DataLoader (val/test split)\n",
    "        device : \"cuda\" or \"cpu\"\n",
    "    Returns:\n",
    "        dict with accuracy, per-class precision/recall/F1, \n",
    "        and raw predictions/labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for xtech, St, lengths, y in loader:\n",
    "        xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "        logits, _ = model(xtech, St, lengths)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Global Accuracy\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Per-class metrics (0=Neutral, 1=Buy, 2=Sell)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None, labels=[0,1,2], zero_division=0\n",
    "    )\n",
    "\n",
    "    # Logging\n",
    "    print(\"== Classification Results ==\")\n",
    "    print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    print(\"Per-class (0=Neutral, 1=Buy, 2=Sell):\")\n",
    "    print(\"Class | Precision | Recall | F1\")\n",
    "    for cls, p, r, f in zip([0,1,2], precision, recall, f1):\n",
    "        print(f\"  {cls:5d} |   {p:.4f}   |  {r:.4f}  | {f:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_per_class\": precision.tolist(),\n",
    "        \"recall_per_class\": recall.tolist(),\n",
    "        \"f1_per_class\": f1.tolist(),\n",
    "        \"y_true\": all_labels,\n",
    "        \"y_pred\": all_preds,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Example usage (after training one fold)\n",
    "# ============================================================\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(train_set, val_set, test_set)\n",
    "\n",
    "metrics_test = evaluate_classification_metrics(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f004f80-d6bf-47fa-83ee-ade2dd29b222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block B: Statistical Validation (multi-seed + 95% CIs)\n",
    "# ============================================================\n",
    "# - Train AMFNet multiple times with different seeds\n",
    "# - Select best checkpoint per seed by validation F1\n",
    "# - Report mean ± std and 95% CI for Accuracy and F1\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    \"\"\"Ensure reproducibility across runs.\"\"\"\n",
    "    import random, numpy as _np, torch as _torch\n",
    "    random.seed(seed); _np.random.seed(seed); _torch.manual_seed(seed)\n",
    "    if _torch.cuda.is_available():\n",
    "        _torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ci95(x):\n",
    "    \"\"\"95% confidence interval under normal approximation.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mean = x.mean()\n",
    "    std = x.std(ddof=1) if len(x) > 1 else 0.0\n",
    "    half = 1.96 * (std / np.sqrt(len(x))) if len(x) > 1 else 0.0\n",
    "    return mean, std, (mean - half, mean + half)\n",
    "\n",
    "def train_one_run(train_loader, val_loader, test_loader, device, epochs=200, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train AMFNet once with a given seed.\n",
    "    Save best checkpoint based on validation F1.\n",
    "    \"\"\"\n",
    "    # Fresh model\n",
    "    model = AMFNet(tech_dim=7, sent_dim=3, hidden_dim=64).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- training ---\n",
    "        model.train()\n",
    "        for xtech, St, lengths, y in train_loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        # --- validation (F1) ---\n",
    "        model.eval()\n",
    "        val = evaluate_classification_metrics(model, val_loader, device)\n",
    "        f1_val = np.mean(val[\"f1_per_class\"])  # macro-F1 across 3 classes\n",
    "        if f1_val > best_val_f1:\n",
    "            best_val_f1 = f1_val\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # Reload best checkpoint\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    # --- final test evaluation ---\n",
    "    test = evaluate_classification_metrics(model, test_loader, device)\n",
    "    f1_test = np.mean(test[\"f1_per_class\"])\n",
    "    return test[\"accuracy\"], f1_test\n",
    "\n",
    "def statistical_validation(train_loader, val_loader, test_loader, device,\n",
    "                           seeds=(42, 43, 44, 45, 46), epochs=200):\n",
    "    \"\"\"\n",
    "    Run multiple seeds and report mean ± std and 95% CI.\n",
    "    \"\"\"\n",
    "    accs, f1s = [], []\n",
    "    for s in seeds:\n",
    "        set_global_seed(s)\n",
    "        acc, f1 = train_one_run(train_loader, val_loader, test_loader, device, epochs=epochs)\n",
    "        accs.append(acc); f1s.append(f1)\n",
    "\n",
    "    acc_mean, acc_std, acc_ci = ci95(accs)\n",
    "    f1_mean, f1_std, f1_ci = ci95(f1s)\n",
    "\n",
    "    print(\"== Statistical Validation (multi-seed) ==\")\n",
    "    print(f\"Accuracy : {acc_mean:.4f} ± {acc_std:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})\")\n",
    "    print(f\"F1       : {f1_mean:.4f} ± {f1_std:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})\")\n",
    "\n",
    "    return {\"accs\": accs, \"f1s\": f1s, \"acc_ci\": acc_ci, \"f1_ci\": f1_ci}\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (after defining loaders for a fold):\n",
    "# ----------------------------\n",
    "stats = statistical_validation(train_loader, val_loader, test_loader,\n",
    "                               device, seeds=[11,12,13,14,15], epochs=50)  # shorter for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311670e-0e9a-400a-a87a-79eb0bc82f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block C: Portfolio-based Evaluation\n",
    "# ============================================================\n",
    "# Converts class predictions into trading signals {-1, 0, +1},\n",
    "# computes portfolio metrics (CR, Sharpe, MDD, HR),\n",
    "# both with and without transaction costs (10 bps).\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import yfinance as yf\n",
    "\n",
    "def signals_from_preds(y_pred):\n",
    "    \"\"\"Map class predictions {0=Neutral, 1=Buy, 2=Sell} -> trading signals {0,+1,-1}\"\"\"\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    sig = np.zeros_like(y_pred, dtype=np.int8)\n",
    "    sig[y_pred == 1] = +1   # Buy\n",
    "    sig[y_pred == 2] = -1   # Sell\n",
    "    return sig\n",
    "\n",
    "def portfolio_metrics(strategy_rets):\n",
    "    \"\"\"Compute CR, Sharpe, MDD, HR from daily strategy returns.\"\"\"\n",
    "    strategy_rets = np.asarray(strategy_rets, dtype=float)\n",
    "    strategy_rets = np.nan_to_num(strategy_rets, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    strategy_rets = np.clip(strategy_rets, -0.5, 0.5)  # safeguard\n",
    "\n",
    "    # Cumulative return\n",
    "    cum_curve = np.cumprod(1.0 + strategy_rets)\n",
    "    cr = cum_curve[-1] - 1.0\n",
    "\n",
    "    # Sharpe ratio\n",
    "    sr = strategy_rets.mean() / (strategy_rets.std(ddof=1) + 1e-12)\n",
    "\n",
    "    # Max Drawdown\n",
    "    peak = np.maximum.accumulate(cum_curve)\n",
    "    mdd = (cum_curve / peak - 1.0).min()\n",
    "\n",
    "    # Hit Ratio\n",
    "    hr = (strategy_rets > 0).mean()\n",
    "\n",
    "    return {\"CR\": cr, \"Sharpe\": sr, \"MDD\": mdd, \"HitRatio\": hr}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_portfolio(model, loader, base_dataset, costs_bps=10,\n",
    "                       device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate portfolio performance using AMFNet predictions.\n",
    "    Returns metrics with and without transaction costs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, rets = [], []\n",
    "    idx_offset = getattr(loader.dataset, \"indices\", None)\n",
    "    running_index = 0\n",
    "\n",
    "    for xtech, St, length, y in loader:\n",
    "        xtech, St, length = xtech.to(device), St.to(device), length.to(device)\n",
    "        logits, _ = model(xtech, St, length)\n",
    "        batch_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "        preds.extend(batch_pred)\n",
    "\n",
    "        bsz = len(batch_pred)\n",
    "        if idx_offset is not None:\n",
    "            batch_idx = loader.dataset.indices[running_index:running_index+bsz]\n",
    "            batch_rets = base_dataset.r_t1[batch_idx]\n",
    "        else:\n",
    "            batch_rets = base_dataset.r_t1[running_index:running_index+bsz]\n",
    "        rets.extend(batch_rets)\n",
    "        running_index += bsz\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    rets = np.asarray(rets, dtype=float)\n",
    "\n",
    "    # Strategy returns without costs\n",
    "    sig = signals_from_preds(preds)\n",
    "    strat_rets_raw = sig * rets\n",
    "\n",
    "    # Strategy returns with transaction costs\n",
    "    costs = costs_bps / 10000.0\n",
    "    sig_shift = np.roll(sig, 1); sig_shift[0] = 0\n",
    "    trades = (sig != sig_shift).astype(int)\n",
    "    strat_rets_cost = strat_rets_raw - trades * costs\n",
    "\n",
    "    # Portfolio metrics\n",
    "    no_cost = portfolio_metrics(strat_rets_raw)\n",
    "    with_cost = portfolio_metrics(strat_rets_cost)\n",
    "\n",
    "    print(\"== Portfolio metrics (no costs) ==\")\n",
    "    print(f\"CR: {no_cost['CR']*100:6.2f}% | Sharpe: {no_cost['Sharpe']:.2f} | \"\n",
    "          f\"MDD: {no_cost['MDD']*100:6.2f}% | HR: {no_cost['HitRatio']*100:5.1f}%\")\n",
    "    print(\"== Portfolio metrics (10 bps costs) ==\")\n",
    "    print(f\"CR: {with_cost['CR']*100:6.2f}% | Sharpe: {with_cost['Sharpe']:.2f} | \"\n",
    "          f\"MDD: {with_cost['MDD']*100:6.2f}% | HR: {with_cost['HitRatio']*100:5.1f}%\")\n",
    "\n",
    "    return {\n",
    "        \"no_cost\": {**no_cost, \"strategy_rets\": strat_rets_raw},\n",
    "        \"with_cost\": {**with_cost, \"strategy_rets\": strat_rets_cost},\n",
    "        \"signals\": sig,\n",
    "        \"rets\": rets\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (AAPL demo)\n",
    "# ----------------------------\n",
    "df_aapl = yf.download(\"AAPL\", start=\"2015-01-01\", end=\"2024-01-01\", auto_adjust=False)\n",
    "df_aapl = compute_indicators(df_aapl)  # function from Block 8\n",
    "ds_aapl = StockDataset(df_aapl, n_docs_max=20, n_models=3, delta=0.002)\n",
    "\n",
    "# use last 15% as test set\n",
    "test_set_aapl = Subset(ds_aapl, range(int(0.85*len(ds_aapl)), len(ds_aapl)))\n",
    "test_loader_aapl = DataLoader(test_set_aapl, batch_size=64, shuffle=False)\n",
    "\n",
    "portfolio_results = evaluate_portfolio(model, test_loader_aapl, ds_aapl, costs_bps=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4711efc-0f29-4c6a-9102-433acffdd2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block Cbis: Multi-ticker Portfolio Evaluation\n",
    "# ============================================================\n",
    "# Runs evaluate_portfolio for each ticker and aggregates results\n",
    "# across all tickers: mean ± std of CR, Sharpe, MDD, HR.\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_portfolio_multi(model, tickers, start_date=\"2015-01-01\", end_date=\"2024-01-01\",\n",
    "                             costs_bps=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    results_no_cost, results_with_cost = [], []\n",
    "\n",
    "    for t in tickers:\n",
    "        print(f\"\\n=== Evaluating {t} ===\")\n",
    "        df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "        df = compute_indicators(df)  # from Block 8\n",
    "\n",
    "        ds_t = StockDataset(df, n_docs_max=20, n_models=3, delta=0.002)\n",
    "        test_set_t = Subset(ds_t, range(int(0.85*len(ds_t)), len(ds_t)))  # last 15% = test\n",
    "        test_loader_t = DataLoader(test_set_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        res = evaluate_portfolio(model, test_loader_t, ds_t, costs_bps=costs_bps, device=device)\n",
    "        results_no_cost.append(res[\"no_cost\"])\n",
    "        results_with_cost.append(res[\"with_cost\"])\n",
    "\n",
    "    def agg_metrics(res_list):\n",
    "        metrics = {}\n",
    "        for key in [\"CR\", \"Sharpe\", \"MDD\", \"HitRatio\"]:\n",
    "            values = [r[key] for r in res_list]\n",
    "            metrics[key] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"std\": np.std(values, ddof=1)\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    agg_no_cost = agg_metrics(results_no_cost)\n",
    "    agg_with_cost = agg_metrics(results_with_cost)\n",
    "\n",
    "    print(\"\\n=== Aggregated Portfolio Results (across tickers) ===\")\n",
    "    print(\"No Costs:\")\n",
    "    for k, v in agg_no_cost.items():\n",
    "        print(f\"{k:8s}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
    "    print(\"\\nWith Costs (10 bps):\")\n",
    "    for k, v in agg_with_cost.items():\n",
    "        print(f\"{k:8s}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
    "\n",
    "    return {\"no_cost\": agg_no_cost, \"with_cost\": agg_with_cost}\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage on multiple tickers\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"MSFT\",\"GOOG\",\"AMZN\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"BAC\",\"XOM\",\"PFE\",\"INTC\"]\n",
    "portfolio_summary = evaluate_portfolio_multi(model, tickers, costs_bps=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddf8f0-7a3e-4834-ab2c-490a05f5e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block D: Average Modality Attention per Sector\n",
    "# ============================================================\n",
    "# Computes average attention weights (α_tech, α_sent) per sector\n",
    "# for the given tickers and plots a grouped bar chart.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_plot_attention_by_sector(model, tickers, sector_map,\n",
    "                                          start_date=\"2015-01-01\", end_date=\"2024-01-01\",\n",
    "                                          device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                          title=\"Average Modality Attention by Sector (2023–2024)\"):\n",
    "    results = []\n",
    "\n",
    "    for t in tickers:\n",
    "        df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "        df = compute_indicators(df).dropna()\n",
    "\n",
    "        ds_t = StockDataset(df, n_docs_max=20, n_models=3, delta=0.002)\n",
    "        test_set_t = Subset(ds_t, range(int(0.85*len(ds_t)), len(ds_t)))  # last 15% as test\n",
    "        test_loader_t = DataLoader(test_set_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "        alphas = []\n",
    "\n",
    "        for xtech, St, lengths, y in test_loader_t:\n",
    "            xtech, St, lengths = xtech.to(device), St.to(device), lengths.to(device)\n",
    "            _, alpha = model(xtech, St, lengths)  # alpha: [B, 2]\n",
    "            alphas.append(alpha.cpu().numpy())\n",
    "\n",
    "        if alphas:\n",
    "            alphas = np.concatenate(alphas, axis=0)\n",
    "            mean_alpha = alphas.mean(axis=0)  # [2] -> [α_tech, α_sent]\n",
    "            results.append({\n",
    "                \"Ticker\": t,\n",
    "                \"Sector\": sector_map[t],  # strict mapping\n",
    "                \"Alpha_tech\": mean_alpha[0],\n",
    "                \"Alpha_sent\": mean_alpha[1]\n",
    "            })\n",
    "\n",
    "    # Aggregate per sector\n",
    "    df_alpha = pd.DataFrame(results)\n",
    "    df_sector = df_alpha.groupby(\"Sector\")[[\"Alpha_tech\", \"Alpha_sent\"]].mean().reset_index()\n",
    "\n",
    "    print(\"\\n== Average Modality Attention per Sector ==\")\n",
    "    print(df_sector)\n",
    "\n",
    "    # -------- Plot grouped bar chart --------\n",
    "    sectors = df_sector[\"Sector\"].tolist()\n",
    "    alpha_sent = df_sector[\"Alpha_sent\"].values\n",
    "    alpha_tech = df_sector[\"Alpha_tech\"].values\n",
    "\n",
    "    x = np.arange(len(sectors))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(x - width/2, alpha_sent, width, label=\"Sentiment Attention\")\n",
    "    ax.bar(x + width/2, alpha_tech, width, label=\"Technical Attention\")\n",
    "\n",
    "    ax.set_ylabel(\"Average Attention Weight\")\n",
    "    ax.set_xlabel(\"Sector\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sectors, rotation=20, ha=\"right\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_alpha, df_sector\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"NVDA\",\"JPM\",\"BAC\",\"JNJ\",\"PFE\",\"XOM\",\"CVX\",\"BA\",\"CAT\",\"KO\",\"PG\"]\n",
    "sector_map = {\n",
    "    # Technology\n",
    "    \"AAPL\": \"Technology\",\n",
    "    \"NVDA\": \"Technology\",\n",
    "\n",
    "    # Finance\n",
    "    \"JPM\": \"Finance\",\n",
    "    \"BAC\": \"Finance\",\n",
    "\n",
    "    # Healthcare\n",
    "    \"JNJ\": \"Healthcare\",\n",
    "    \"PFE\": \"Healthcare\",\n",
    "\n",
    "    # Energy\n",
    "    \"XOM\": \"Energy\",\n",
    "    \"CVX\": \"Energy\",\n",
    "\n",
    "    # Industrials\n",
    "    \"BA\": \"Industrials\",\n",
    "    \"CAT\": \"Industrials\",\n",
    "\n",
    "    # Consumer Staples\n",
    "    \"KO\": \"Consumer Staples\",\n",
    "    \"PG\": \"Consumer Staples\"\n",
    "}\n",
    "df_alpha, df_sector = evaluate_and_plot_attention_by_sector(model, tickers, sector_map, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd141d0-a18c-460b-9ff2-5828babddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block E: Quarterly F1-score + Market Regime Analysis\n",
    "# ============================================================\n",
    "# 1. Computes quarterly F1-score of AMFNet over the 2023–2024 test period.\n",
    "# 2. Computes average modality attention weights (α_sent, α_tech)\n",
    "#    under different market regimes (Stable vs Volatile),\n",
    "#    and reports the Modality Ratio (α_sent / α_tech).\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def quarterly_f1_scores(model, base_dataset, test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute quarterly macro-F1 scores for the test set.\n",
    "    Args:\n",
    "        model        : trained AMFNet model\n",
    "        base_dataset : StockDataset (provides aligned dates)\n",
    "        test_loader  : DataLoader for the test split\n",
    "        device       : \"cuda\" or \"cpu\"\n",
    "    Returns:\n",
    "        pandas.Series with quarterly macro-F1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_dates = [], [], []\n",
    "\n",
    "    # Dates aligned with returns (shifted because of r_t1)\n",
    "    dates = base_dataset.df.index[1:]\n",
    "    test_indices = test_loader.dataset.indices if hasattr(test_loader.dataset, \"indices\") else range(len(test_loader.dataset))\n",
    "\n",
    "    for batch_i, (xtech, St, lengths, y) in enumerate(test_loader):\n",
    "        xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "        logits, _ = model(xtech, St, lengths)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "\n",
    "        start = batch_i * test_loader.batch_size\n",
    "        end = start + len(preds)\n",
    "        batch_dates = dates[test_indices[start:end]]\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        all_dates.extend(batch_dates)\n",
    "\n",
    "    df_eval = pd.DataFrame({\n",
    "        \"Date\": pd.to_datetime(all_dates),\n",
    "        \"y_true\": all_labels,\n",
    "        \"y_pred\": all_preds\n",
    "    })\n",
    "    df_eval[\"Quarter\"] = df_eval[\"Date\"].dt.to_period(\"Q\")\n",
    "\n",
    "    # Compute macro-F1 per quarter\n",
    "    f1_quarter = df_eval.groupby(\"Quarter\").apply(\n",
    "        lambda g: f1_score(g[\"y_true\"], g[\"y_pred\"], average=\"macro\")\n",
    "    )\n",
    "    return f1_quarter\n",
    "\n",
    "def plot_quarterly_f1(f1_quarter, title=\"Quarterly F1-score of AMFNet (2023–2024)\"):\n",
    "    \"\"\"\n",
    "    Plot a line chart of quarterly macro-F1 scores.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(f1_quarter.index.astype(str), f1_quarter.values, marker=\"o\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Quarter\")\n",
    "    ax.set_ylabel(\"F1-score\")\n",
    "    ax.set_ylim(0.84, 0.89)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def attention_by_market_regime(model, base_dataset, test_loader, \n",
    "                               stable_period=(\"2023Q1\", \"2023Q1\"), \n",
    "                               volatile_period=(\"2023Q2\", \"2024Q1\"),\n",
    "                               device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute average α_sent and modality ratio (α_sent/α_tech)\n",
    "    for stable vs volatile market regimes.\n",
    "    Args:\n",
    "        model        : trained AMFNet\n",
    "        base_dataset : StockDataset with aligned dates\n",
    "        test_loader  : DataLoader for test split\n",
    "        stable_period: tuple (\"start_quarter\", \"end_quarter\")\n",
    "        volatile_period: tuple (\"start_quarter\", \"end_quarter\")\n",
    "    Returns:\n",
    "        pandas.DataFrame with regime statistics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    alphas, all_dates = [], []\n",
    "\n",
    "    dates = base_dataset.df.index[1:]\n",
    "    test_indices = test_loader.dataset.indices if hasattr(test_loader.dataset, \"indices\") else range(len(test_loader.dataset))\n",
    "\n",
    "    running_index = 0\n",
    "    for xtech, St, lengths, y in test_loader:\n",
    "        xtech, St, lengths = xtech.to(device), St.to(device), lengths.to(device)\n",
    "        _, alpha = model(xtech, St, lengths)  # [B,2]\n",
    "        bsz = len(alpha)\n",
    "        batch_dates = dates[test_indices[running_index:running_index+bsz]]\n",
    "        running_index += bsz\n",
    "\n",
    "        alphas.append(alpha.cpu().numpy())\n",
    "        all_dates.extend(batch_dates)\n",
    "\n",
    "    alphas = np.concatenate(alphas, axis=0)\n",
    "    df_alpha = pd.DataFrame(alphas, columns=[\"Alpha_tech\",\"Alpha_sent\"])\n",
    "    df_alpha[\"Date\"] = pd.to_datetime(all_dates)\n",
    "    df_alpha[\"Quarter\"] = df_alpha[\"Date\"].dt.to_period(\"Q\")\n",
    "\n",
    "    # Stable period\n",
    "    df_stable = df_alpha[df_alpha[\"Quarter\"].astype(str).between(stable_period[0], stable_period[1])]\n",
    "    alpha_sent_stable = df_stable[\"Alpha_sent\"].mean()\n",
    "    alpha_tech_stable = df_stable[\"Alpha_tech\"].mean()\n",
    "    ratio_stable = alpha_sent_stable / (alpha_tech_stable + 1e-12)\n",
    "\n",
    "    # Volatile period\n",
    "    df_vol = df_alpha[df_alpha[\"Quarter\"].astype(str).between(volatile_period[0], volatile_period[1])]\n",
    "    alpha_sent_vol = df_vol[\"Alpha_sent\"].mean()\n",
    "    alpha_tech_vol = df_vol[\"Alpha_tech\"].mean()\n",
    "    ratio_vol = alpha_sent_vol / (alpha_tech_vol + 1e-12)\n",
    "\n",
    "    # Create results table\n",
    "    table = pd.DataFrame([\n",
    "        {\"Market Period\": \"Stable (Q1 2023)\", \"α_sent\": round(alpha_sent_stable,2), \"ModalityRatio\": round(ratio_stable,2)},\n",
    "        {\"Market Period\": \"Volatile (Q2 2023–Q1 2024)\", \"α_sent\": round(alpha_sent_vol,2), \"ModalityRatio\": round(ratio_vol,2)}\n",
    "    ])\n",
    "    print(\"\\n== Average attention weights under market regimes ==\")\n",
    "    print(table.to_string(index=False))\n",
    "    return table\n",
    "\n",
    "# Quarterly F1-score\n",
    "f1_quarter = quarterly_f1_scores(model, ds_aapl, test_loader_aapl, device=device)\n",
    "plot_quarterly_f1(f1_quarter)\n",
    "\n",
    "# Market regime attention\n",
    "table_regimes = attention_by_market_regime(model, ds_aapl, test_loader_aapl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a325c-36cf-48f2-8755-b16c6ce20602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
