{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83695862-2eae-4a9a-b498-94e274388333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[INFO] Global configuration loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block 1: Imports & Configuration\n",
    "# ============================================================\n",
    "# This block imports the required libraries for deep learning, \n",
    "# data processing, and reproducibility.\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import yfinance as yf\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility setup\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration (aligned with the paper)\n",
    "# ------------------------------------------------------------\n",
    "config = {\n",
    "    # Model dimensions\n",
    "    \"d_model\": 64,          # shared embedding dimension\n",
    "    \"dropout_tech\": 0.2,    # dropout for technical MLP\n",
    "    \"dropout_sent\": 0.1,    # dropout for sentiment transformer\n",
    "    \"n_heads\": 4,           # transformer heads\n",
    "    \"n_layers\": 2,          # transformer layers\n",
    "    \n",
    "    # Training setup\n",
    "    \"learning_rate\": 1e-3,  # Adam optimizer with cosine annealing\n",
    "    \"batch_size\": 128,      # as specified in the paper\n",
    "    \"max_epochs\": 200,      # max epochs with early stopping patience=20\n",
    "    \"early_stop_patience\": 20,\n",
    "    \n",
    "    # Task setup\n",
    "    \"n_classes\": 3,         # {-1, 0, +1} → mapped internally to {0,1,2}\n",
    "    \"delta\": 0.002,         # threshold for neutral class (can be tuned)\n",
    "}\n",
    "print(\"[INFO] Global configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d8639e-2230-481b-ab57-540cf3a0690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 2: Technical Encoder\n",
    "# ============================================================\n",
    "# This module encodes daily technical indicators into a latent \n",
    "# representation using a two-layer MLP. \n",
    "# \n",
    "# Paper reference:\n",
    "# - Input:  x_t^tech ∈ R^7 (7 domain-standard indicators)\n",
    "# - Output: H_tech ∈ R^64 (shared latent dimension)\n",
    "# - Architecture: 2-layer MLP with ReLU activations and dropout=0.2\n",
    "# ============================================================\n",
    "\n",
    "class TechnicalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode technical indicators.\n",
    "        Args:\n",
    "            x: [batch_size, input_dim] tensor of technical features.\n",
    "        Returns:\n",
    "            [batch_size, hidden_dim] latent representation.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebec777-1aad-4c9d-a74c-e8b6879bff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 3: Sentiment Encoder\n",
    "# ============================================================\n",
    "# This module encodes document-level sentiment using a \n",
    "# Transformer encoder and integrates an uncertainty signal.\n",
    "#\n",
    "# Paper reference:\n",
    "# - Input:  S_t ∈ R^{n_docs × n_models} (n_models=3: FinGPT, HAN, VADER)\n",
    "# - Encoder: 2-layer Transformer, 4 heads, d_model=64, dropout=0.1\n",
    "# - Pooling: mean pooling across documents (masked)\n",
    "# - Uncertainty: average std across models, then averaged across documents\n",
    "# - Fusion: concatenate H_sent with u_t, then project back to hidden_dim\n",
    "# ============================================================\n",
    "\n",
    "class SentimentEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: project sentiment scores into latent space\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Step 2: transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Step 6: projection after concatenating uncertainty\n",
    "        self.linear_proj = nn.Linear(hidden_dim + 1, hidden_dim)\n",
    "\n",
    "    def compute_uncertainty(self, St, lengths):\n",
    "        \"\"\"\n",
    "        Compute uncertainty score as described in the paper (Eq. 7).\n",
    "        Args:\n",
    "            St: [batch_size, n_docs, n_models]\n",
    "            lengths: [batch_size] actual number of docs per sample\n",
    "        Returns:\n",
    "            u_t: [batch_size, 1] scalar uncertainty per sample\n",
    "        \"\"\"\n",
    "        # std across sentiment models, shape = [B, n_docs]\n",
    "        std_doc = St.std(dim=-1, unbiased=False)\n",
    "        \n",
    "        # mask padding documents\n",
    "        mask = torch.arange(St.size(1), device=St.device)[None, :] >= lengths[:, None]\n",
    "        std_doc = std_doc.masked_fill(mask, 0.0)\n",
    "        \n",
    "        # average across valid documents\n",
    "        denom = lengths.clamp_min(1).float()\n",
    "        u_t = std_doc.sum(dim=1) / denom\n",
    "        return u_t.unsqueeze(1)  # [B,1]\n",
    "\n",
    "    def forward(self, St, lengths):\n",
    "        \"\"\"\n",
    "        Encode daily sentiment matrix.\n",
    "        Args:\n",
    "            St: [B, n_docs, n_models] sentiment scores\n",
    "            lengths: [B] number of valid docs per sample\n",
    "        Returns:\n",
    "            H_sent*: [B, hidden_dim] enriched sentiment representation\n",
    "                     (includes uncertainty)\n",
    "        \"\"\"\n",
    "        # If no-news case: return None to rely on technical encoder only\n",
    "        if lengths is None or (lengths == 0).all():\n",
    "            return None\n",
    "\n",
    "        # Step 1: embed sentiments\n",
    "        H = self.embedding(St)  # [B, n_docs, hidden_dim]\n",
    "        \n",
    "        # Step 2: encode with transformer (mask padded docs)\n",
    "        mask = torch.arange(St.size(1), device=St.device)[None, :] >= lengths[:, None]\n",
    "        H = self.transformer(H, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Step 3: mean pooling across valid documents\n",
    "        denom = lengths.clamp_min(1).float().unsqueeze(1)\n",
    "        H_sum = H.masked_fill(mask.unsqueeze(-1), 0.0).sum(dim=1)\n",
    "        H_sent = H_sum / denom  # [B, hidden_dim]\n",
    "        \n",
    "        # Step 4: compute uncertainty\n",
    "        uncertainty = self.compute_uncertainty(St, lengths)  # [B,1]\n",
    "        \n",
    "        # Step 5: concatenate uncertainty\n",
    "        H_sent_aug = torch.cat([H_sent, uncertainty], dim=-1)  # [B, hidden_dim+1]\n",
    "        \n",
    "        # Step 6: project back to hidden_dim\n",
    "        return self.linear_proj(H_sent_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1731c1f-7f82-4e11-88d0-7c7164eba15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 4: Fusion Gate\n",
    "# ============================================================\n",
    "# This block adaptively fuses the outputs of the technical and\n",
    "# sentiment branches using a gating mechanism equivalent to \n",
    "# attention weights (Eq. 10–11 in the paper).\n",
    "#\n",
    "# Paper reference:\n",
    "# - Input:  [H_tech, H_sent] ∈ R^d\n",
    "# - Gate:   α = softmax(W_att · [H_tech || H_sent] + b), α ∈ R^2\n",
    "# - Output: H_fused = α1 * H_tech + α2 * H_sent\n",
    "# - No-news case: α = [1, 0] → rely entirely on technical branch\n",
    "# ============================================================\n",
    "\n",
    "class FusionGate(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Linear layer projects concatenated [Htech || Hsent] into 2 logits\n",
    "        self.Watt = nn.Linear(hidden_dim * 2, 2)\n",
    "\n",
    "    def forward(self, Htech, Hsent=None):\n",
    "        \"\"\"\n",
    "        Adaptive fusion of technical and sentiment encoders.\n",
    "        Args:\n",
    "            Htech: [B, hidden_dim] technical representation\n",
    "            Hsent: [B, hidden_dim] sentiment representation or None\n",
    "        Returns:\n",
    "            Hfused: [B, hidden_dim] fused representation\n",
    "            alpha : [B, 2] gating weights [w_tech, w_sent]\n",
    "        \"\"\"\n",
    "        # Case: no-news (sentiment branch missing)\n",
    "        if Hsent is None:\n",
    "            B, d = Htech.size()\n",
    "            alpha = torch.zeros(B, 2, device=Htech.device)\n",
    "            alpha[:, 0] = 1.0  # full weight to technical branch\n",
    "            return Htech, alpha\n",
    "\n",
    "        # Case: both modalities available\n",
    "        combined = torch.cat([Htech, Hsent], dim=-1)  # [B, 2*hidden_dim]\n",
    "        alpha = F.softmax(self.Watt(combined), dim=-1)  # [B, 2]\n",
    "        \n",
    "        # Weighted sum across modalities\n",
    "        Hfused = alpha[:, 0].unsqueeze(1) * Htech + alpha[:, 1].unsqueeze(1) * Hsent\n",
    "        return Hfused, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8574966-ac0f-48a2-a50e-651f67d483ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 5: AMFNet (Full Model)\n",
    "# ============================================================\n",
    "# This class implements the full Adaptive Modality Fusion \n",
    "# Network (AMFNet) as described in the paper.\n",
    "#\n",
    "# Paper reference:\n",
    "# - Technical encoder: MLP (7 → 64)\n",
    "# - Sentiment encoder: Transformer (2 layers, 4 heads, d=64)\n",
    "# - Fusion gate: attention-like gating (Eq. 10–11)\n",
    "# - Prediction: MLP head, softmax over 3 classes (Eq. 12)\n",
    "#\n",
    "# Input:\n",
    "#   xtech   : [B, tech_dim]        daily technical indicators\n",
    "#   St      : [B, n_docs, 3]      daily sentiment matrix (padded)\n",
    "#   lengths : [B]                 number of valid docs per sample\n",
    "#\n",
    "# Output:\n",
    "#   logits : [B, n_classes]       class scores before softmax\n",
    "#   alpha  : [B, 2]               fusion weights [w_tech, w_sent]\n",
    "# ============================================================\n",
    "\n",
    "class AMFNet(nn.Module):\n",
    "    def __init__(self, tech_dim=7, sent_dim=3, hidden_dim=64, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.tech_encoder = TechnicalEncoder(input_dim=tech_dim, hidden_dim=hidden_dim)\n",
    "        self.sent_encoder = SentimentEncoder(input_dim=sent_dim, hidden_dim=hidden_dim)\n",
    "        self.fusion = FusionGate(hidden_dim=hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, xtech, St=None, lengths=None):\n",
    "        \"\"\"\n",
    "        Forward pass through AMFNet.\n",
    "        Args:\n",
    "            xtech:   [B, tech_dim]\n",
    "            St:      [B, n_docs, sent_dim] sentiment matrix (optional)\n",
    "            lengths: [B] number of valid documents (for masking)\n",
    "        Returns:\n",
    "            logits: [B, n_classes] classification scores\n",
    "            alpha:  [B, 2] fusion weights\n",
    "        \"\"\"\n",
    "        # Step 1: encode technical features\n",
    "        Htech = self.tech_encoder(xtech)  # [B, hidden_dim]\n",
    "\n",
    "        # Step 2: encode sentiment features (if available)\n",
    "        Hsent = self.sent_encoder(St, lengths) if (St is not None) else None\n",
    "\n",
    "        # Step 3: fuse modalities (handles no-news internally)\n",
    "        Hfused, alpha = self.fusion(Htech, Hsent)\n",
    "\n",
    "        # Step 4: classification head (Eq. 12)\n",
    "        logits = self.classifier(Hfused)  # [B, n_classes]\n",
    "\n",
    "        return logits, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b53070-3226-41ce-8f73-d27835b8e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Logits shape: torch.Size([8, 3])\n",
      "[TEST] Fusion weights shape: torch.Size([8, 2])\n",
      "[TEST] Fusion weights example (first sample): [0.74196595 0.25803402]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block 6: Example Usage\n",
    "# ============================================================\n",
    "# We test AMFNet with dummy data to ensure the forward pass \n",
    "# runs correctly. Shapes are consistent with the paper:\n",
    "# - Technical features: 7 indicators\n",
    "# - Sentiment matrix: n_docs × 3 models (FinGPT, HAN, VADER)\n",
    "# ============================================================\n",
    "\n",
    "# Dummy batch setup\n",
    "batch_size, tech_dim, n_docs, sent_dim = 8, 7, 5, 3\n",
    "xtech = torch.randn(batch_size, tech_dim).to(device)          # [B, 7]\n",
    "St = torch.randn(batch_size, n_docs, sent_dim).to(device)     # [B, n_docs, 3]\n",
    "lengths = torch.full((batch_size,), n_docs, dtype=torch.long).to(device)  # [B]\n",
    "\n",
    "# Initialize model\n",
    "model = AMFNet(tech_dim=tech_dim, sent_dim=sent_dim, hidden_dim=config[\"d_model\"], n_classes=config[\"n_classes\"]).to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits, alpha = model(xtech, St, lengths)\n",
    "\n",
    "# Logs\n",
    "print(\"[TEST] Logits shape:\", logits.shape)      # Expected: [B, 3]\n",
    "print(\"[TEST] Fusion weights shape:\", alpha.shape)  # Expected: [B, 2]\n",
    "print(\"[TEST] Fusion weights example (first sample):\", alpha[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b654621c-b3aa-4c04-9eaa-51841daee982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block X: StockDataset\n",
    "# ============================================================\n",
    "# This dataset prepares daily samples for AMFNet:\n",
    "#   - Technical indicators (xtech): 7 features\n",
    "#   - Sentiment matrix (St): [n_docs_max × n_models] (padded)\n",
    "#   - lengths: number of valid documents per sample\n",
    "#   - Label y: {0=Neutral, 1=Buy, 2=Sell}\n",
    "#   - Realized next-day returns (r_t1): float\n",
    "#\n",
    "# Paper reference:\n",
    "# - Labels defined by Eq. (2) with threshold delta\n",
    "# - Sentiment scores normalized into [-1, +1] (Eq. 3–4)\n",
    "# - Zero-doc case handled explicitly (fallback to technical branch)\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df, n_docs_max=10, n_models=3, delta=0.002):\n",
    "        self.df = df.copy()\n",
    "        self.n_docs_max = n_docs_max\n",
    "        self.n_models = n_models\n",
    "        self.delta = delta\n",
    "\n",
    "        # --- Technical indicators (7 features) ---\n",
    "        self.xtech = self.df[[\"RSI\", \"MACD\", \"VWAP\", \"BB_Width\", \"ATR\", \"OBV\", \"ROC\"]].values.astype(np.float32)\n",
    "\n",
    "        # --- Next-day returns and labels (Eq. 2) ---\n",
    "        close = self.df[\"Close\"].values\n",
    "        r_t1 = (np.roll(close, -1) - close) / close\n",
    "\n",
    "        # Labeling with neutral threshold delta\n",
    "        y = np.zeros_like(r_t1, dtype=np.int64)\n",
    "        y[r_t1 > delta] = 1  # Buy\n",
    "        y[r_t1 < -delta] = 2 # Sell\n",
    "\n",
    "        # Drop last row (no next-day return available)\n",
    "        self.y = y[:-1]\n",
    "        self.r_t1 = r_t1[:-1].astype(np.float32)\n",
    "        self.xtech = self.xtech[:-1]\n",
    "\n",
    "        # --- Sentiment matrices (dummy data here) ---\n",
    "        n_days = len(self.y)\n",
    "        self.St = []\n",
    "        self.lengths = []\n",
    "\n",
    "        # MinMax scaler for normalization ([-1,1]), one per model\n",
    "        scalers = [MinMaxScaler(feature_range=(-1, 1)) for _ in range(n_models)]\n",
    "\n",
    "        for _ in range(n_days):\n",
    "            # Variable number of documents [0..n_docs_max]\n",
    "            n_docs = np.random.randint(0, n_docs_max + 1)\n",
    "            self.lengths.append(n_docs)\n",
    "\n",
    "            # Initialize matrix with padding\n",
    "            mat = np.zeros((n_docs_max, n_models), dtype=np.float32)\n",
    "\n",
    "            if n_docs > 0:\n",
    "                # Simulate raw sentiment scores\n",
    "                raw = np.random.uniform(-1, 1, size=(n_docs, n_models))\n",
    "\n",
    "                # Normalize each model column with its own scaler (fit_transform here just for dummy data)\n",
    "                normed = np.zeros_like(raw)\n",
    "                for j in range(n_models):\n",
    "                    normed[:, j] = scalers[j].fit_transform(raw[:, j].reshape(-1, 1)).squeeze()\n",
    "\n",
    "                mat[:n_docs] = normed\n",
    "\n",
    "            self.St.append(mat)\n",
    "\n",
    "        self.St = np.stack(self.St, axis=0)\n",
    "        self.lengths = np.array(self.lengths, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xtech = torch.tensor(self.xtech[idx], dtype=torch.float32)     # [7]\n",
    "        St = torch.tensor(self.St[idx], dtype=torch.float32)           # [n_docs_max, n_models]\n",
    "        length = torch.tensor(self.lengths[idx], dtype=torch.long)     # scalar\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long).squeeze()      # scalar\n",
    "        return xtech, St, length, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265e56e8-c9c9-40dd-9b84-2293c43ac68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 36 rolling folds across 12 tickers.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block 8: Training Setup\n",
    "# ============================================================\n",
    "# Parameters:\n",
    "#   - 12 US large-cap tickers (2015–2024)\n",
    "#   - Technical indicators: 7 features\n",
    "#   - Sentiment: 3 models (dummy matrices here, normalized)\n",
    "#   - Label threshold delta=0.002\n",
    "#   - Rolling window: 24m train / 3m val / 3m test (12 folds)\n",
    "#   - Hidden dim=64, batch=128, max_epochs=200, early stop=20\n",
    "#   - Optimizer: Adam, lr=1e-3, CosineAnnealingLR scheduler\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Parameters\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"NVDA\",\"JPM\",\"BAC\",\"JNJ\",\"PFE\",\"XOM\",\"CVX\",\"BA\",\"CAT\",\"KO\",\"PG\"]\n",
    "\n",
    "start_date = \"2015-01-01\"\n",
    "end_date   = \"2024-01-01\"\n",
    "n_docs_max, n_models = 20, 3\n",
    "delta = config[\"delta\"]\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "max_epochs = config[\"max_epochs\"]\n",
    "patience   = config[\"early_stop_patience\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Technical indicators\n",
    "# ----------------------------\n",
    "def compute_indicators(df):\n",
    "    df = df.copy()\n",
    "    # RSI\n",
    "    delta_p = df[\"Close\"].diff()\n",
    "    gain = (delta_p.where(delta_p > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta_p.where(delta_p < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = ema12 - ema26\n",
    "\n",
    "    # VWAP\n",
    "    df[\"VWAP\"] = (df[\"Close\"] * df[\"Volume\"]).cumsum() / df[\"Volume\"].cumsum()\n",
    "\n",
    "    # Bollinger Band Width\n",
    "    mean20 = df[\"Close\"].rolling(20).mean()\n",
    "    std20 = df[\"Close\"].rolling(20).std()\n",
    "    df[\"BB_Width\"] = 4 * std20\n",
    "\n",
    "    # ATR\n",
    "    high_low = df[\"High\"] - df[\"Low\"]\n",
    "    high_close = (df[\"High\"] - df[\"Close\"].shift()).abs()\n",
    "    low_close = (df[\"Low\"] - df[\"Close\"].shift()).abs()\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df[\"ATR\"] = tr.rolling(14).mean()\n",
    "\n",
    "    # OBV\n",
    "    obv = (np.sign(df[\"Close\"].diff()) * df[\"Volume\"]).fillna(0).cumsum()\n",
    "    df[\"OBV\"] = obv\n",
    "\n",
    "    # ROC\n",
    "    df[\"ROC\"] = df[\"Close\"].pct_change(periods=12)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Rolling Window Splits\n",
    "# ----------------------------\n",
    "def rolling_windows(df, train_months=24, val_months=3, test_months=3):\n",
    "    \"\"\"Generate rolling windows: 24m train / 3m val / 3m test.\"\"\"\n",
    "    folds = []\n",
    "    start = df.index.min()\n",
    "    while True:\n",
    "        train_start = start\n",
    "        train_end   = train_start + pd.DateOffset(months=train_months)\n",
    "        val_end     = train_end   + pd.DateOffset(months=val_months)\n",
    "        test_end    = val_end     + pd.DateOffset(months=test_months)\n",
    "\n",
    "        if test_end > df.index.max():\n",
    "            break\n",
    "\n",
    "        train_idx = (df.index >= train_start) & (df.index < train_end)\n",
    "        val_idx   = (df.index >= train_end)   & (df.index < val_end)\n",
    "        test_idx  = (df.index >= val_end)     & (df.index < test_end)\n",
    "\n",
    "        folds.append((df[train_idx], df[val_idx], df[test_idx]))\n",
    "        start = val_end  # slide window\n",
    "\n",
    "    return folds\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Build datasets\n",
    "# ----------------------------\n",
    "all_folds = []\n",
    "for t in tickers:\n",
    "    df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "    df = compute_indicators(df).dropna()\n",
    "\n",
    "    folds = rolling_windows(df)\n",
    "    for train_df, val_df, test_df in folds:\n",
    "        # scale on train only\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[[\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]])\n",
    "        for part_df in [train_df, val_df, test_df]:\n",
    "            part_df.loc[:, [\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]] = scaler.transform(\n",
    "                part_df[[\"RSI\",\"MACD\",\"VWAP\",\"BB_Width\",\"ATR\",\"OBV\",\"ROC\"]]\n",
    "            )\n",
    "\n",
    "        train_set = StockDataset(train_df, n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "        val_set   = StockDataset(val_df,   n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "        test_set  = StockDataset(test_df,  n_docs_max=n_docs_max, n_models=n_models, delta=delta)\n",
    "\n",
    "        all_folds.append((train_set, val_set, test_set))\n",
    "\n",
    "print(f\"[INFO] Generated {len(all_folds)} rolling folds across {len(tickers)} tickers.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: DataLoader builder\n",
    "# ----------------------------\n",
    "def my_collate_fn(batch):\n",
    "    xtechs, Sts, lengths, ys = zip(*batch)\n",
    "    return (torch.stack(xtechs),\n",
    "            torch.stack(Sts),\n",
    "            torch.stack(lengths),\n",
    "            torch.stack(ys))\n",
    "\n",
    "def make_loaders(train_set, val_set, test_set):\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a331e2a2-a768-4a76-a3a4-908f9b51becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Block 9: Training & Evaluation Loop\n",
    "# ============================================================\n",
    "# This block implements the training pipeline for AMFNet:\n",
    "#   - Optimizer: Adam (lr=1e-3)\n",
    "#   - Scheduler: CosineAnnealingLR\n",
    "#   - Loss: CrossEntropyLoss\n",
    "#   - Early stopping: patience=20 on validation F1\n",
    "#   - Metrics: Accuracy, Macro-F1\n",
    "#   - Protocol: rolling windows (24m train / 3m val / 3m test)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate model on given loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xtech, St, lengths, y in loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return total_loss/len(loader), accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "def train_one_fold(train_set, val_set, test_set, fold_id=0):\n",
    "    \"\"\"Train AMFNet on a single rolling window fold.\"\"\"\n",
    "    print(f\"\\n[INFO] Starting Fold {fold_id+1}/{len(all_folds)}\")\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader, val_loader, test_loader = make_loaders(train_set, val_set, test_set)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = AMFNet(tech_dim=7, sent_dim=3, hidden_dim=config[\"d_model\"], n_classes=config[\"n_classes\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"max_epochs\"])\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_f1, best_state, patience_counter = -1, None, 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"max_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        for xtech, St, lengths, y in train_loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config['max_epochs']} | \"\n",
    "              f\"Train Loss={train_loss:.4f} Acc={train_acc:.4f} F1={train_f1:.4f} || \"\n",
    "              f\"Val Loss={val_loss:.4f} Acc={val_acc:.4f} F1={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config[\"early_stop_patience\"]:\n",
    "            print(f\"[INFO] Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best state\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # Final Test\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
    "    print(f\"[RESULT] Fold {fold_id+1}: Test Loss={test_loss:.4f} Acc={test_acc:.4f} F1={test_f1:.4f}\")\n",
    "\n",
    "    return test_loss, test_acc, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07252df8-aafb-4243-90cd-9e17c07c4e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Starting Fold 1/36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Bentaleb\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss=1.0639 Acc=0.4303 F1=0.2486 || Val Loss=1.1736 Acc=0.2903 F1=0.1519\n",
      "Epoch 2/200 | Train Loss=1.0406 Acc=0.3964 F1=0.2573 || Val Loss=1.1232 Acc=0.3065 F1=0.2398\n",
      "Epoch 3/200 | Train Loss=1.0306 Acc=0.4502 F1=0.2996 || Val Loss=1.1127 Acc=0.4194 F1=0.2302\n",
      "Epoch 4/200 | Train Loss=1.0249 Acc=0.4363 F1=0.2513 || Val Loss=1.1129 Acc=0.4194 F1=0.2302\n",
      "Epoch 5/200 | Train Loss=1.0186 Acc=0.4442 F1=0.2936 || Val Loss=1.1083 Acc=0.4355 F1=0.2612\n",
      "Epoch 6/200 | Train Loss=1.0187 Acc=0.4582 F1=0.3248 || Val Loss=1.1090 Acc=0.4032 F1=0.2619\n",
      "Epoch 7/200 | Train Loss=1.0169 Acc=0.4721 F1=0.3416 || Val Loss=1.0984 Acc=0.4194 F1=0.2531\n",
      "Epoch 8/200 | Train Loss=1.0064 Acc=0.4821 F1=0.3481 || Val Loss=1.0838 Acc=0.4194 F1=0.2302\n",
      "Epoch 9/200 | Train Loss=1.0070 Acc=0.4761 F1=0.3408 || Val Loss=1.0818 Acc=0.4194 F1=0.2302\n",
      "Epoch 10/200 | Train Loss=1.0042 Acc=0.4801 F1=0.3555 || Val Loss=1.0808 Acc=0.4355 F1=0.2809\n",
      "Epoch 11/200 | Train Loss=1.0026 Acc=0.4980 F1=0.3856 || Val Loss=1.0786 Acc=0.4355 F1=0.2614\n",
      "Epoch 12/200 | Train Loss=1.0002 Acc=0.4980 F1=0.3775 || Val Loss=1.0730 Acc=0.4194 F1=0.2302\n",
      "Epoch 13/200 | Train Loss=0.9956 Acc=0.5100 F1=0.3791 || Val Loss=1.0692 Acc=0.4355 F1=0.2614\n",
      "Epoch 14/200 | Train Loss=0.9937 Acc=0.5219 F1=0.4142 || Val Loss=1.0651 Acc=0.4355 F1=0.2800\n",
      "Epoch 15/200 | Train Loss=0.9915 Acc=0.5120 F1=0.4132 || Val Loss=1.0627 Acc=0.4677 F1=0.3391\n",
      "Epoch 16/200 | Train Loss=0.9872 Acc=0.5259 F1=0.4062 || Val Loss=1.0613 Acc=0.4032 F1=0.2753\n",
      "Epoch 17/200 | Train Loss=0.9820 Acc=0.5339 F1=0.4311 || Val Loss=1.0562 Acc=0.4194 F1=0.3218\n",
      "Epoch 18/200 | Train Loss=0.9769 Acc=0.5139 F1=0.4259 || Val Loss=1.0531 Acc=0.4194 F1=0.3217\n",
      "Epoch 19/200 | Train Loss=0.9818 Acc=0.4920 F1=0.3839 || Val Loss=1.0586 Acc=0.4355 F1=0.3076\n",
      "Epoch 20/200 | Train Loss=0.9700 Acc=0.5219 F1=0.4213 || Val Loss=1.0661 Acc=0.4355 F1=0.3636\n",
      "Epoch 21/200 | Train Loss=0.9638 Acc=0.5438 F1=0.4486 || Val Loss=1.0569 Acc=0.4355 F1=0.3305\n",
      "Epoch 22/200 | Train Loss=0.9682 Acc=0.5199 F1=0.4232 || Val Loss=1.0527 Acc=0.4677 F1=0.3682\n",
      "Epoch 23/200 | Train Loss=0.9578 Acc=0.5319 F1=0.4386 || Val Loss=1.0610 Acc=0.4194 F1=0.3533\n",
      "Epoch 24/200 | Train Loss=0.9515 Acc=0.5219 F1=0.4226 || Val Loss=1.0546 Acc=0.4516 F1=0.3391\n",
      "Epoch 25/200 | Train Loss=0.9538 Acc=0.5259 F1=0.4040 || Val Loss=1.0543 Acc=0.4194 F1=0.3344\n",
      "Epoch 26/200 | Train Loss=0.9413 Acc=0.5398 F1=0.4299 || Val Loss=1.0630 Acc=0.4677 F1=0.4115\n",
      "Epoch 27/200 | Train Loss=0.9315 Acc=0.5697 F1=0.4929 || Val Loss=1.0425 Acc=0.4355 F1=0.3495\n",
      "Epoch 28/200 | Train Loss=0.9259 Acc=0.5478 F1=0.4591 || Val Loss=1.0494 Acc=0.4516 F1=0.3844\n",
      "Epoch 29/200 | Train Loss=0.9347 Acc=0.5378 F1=0.4479 || Val Loss=1.0583 Acc=0.4677 F1=0.4036\n",
      "Epoch 30/200 | Train Loss=0.9247 Acc=0.5518 F1=0.4480 || Val Loss=1.0658 Acc=0.4355 F1=0.3504\n",
      "Epoch 31/200 | Train Loss=0.9112 Acc=0.5657 F1=0.4708 || Val Loss=1.0628 Acc=0.4677 F1=0.4115\n",
      "Epoch 32/200 | Train Loss=0.9229 Acc=0.5737 F1=0.5084 || Val Loss=1.0695 Acc=0.4355 F1=0.3504\n",
      "Epoch 33/200 | Train Loss=0.9187 Acc=0.5598 F1=0.4533 || Val Loss=1.0743 Acc=0.4839 F1=0.4141\n",
      "Epoch 34/200 | Train Loss=0.9132 Acc=0.5657 F1=0.4832 || Val Loss=1.0656 Acc=0.4355 F1=0.3635\n",
      "Epoch 35/200 | Train Loss=0.9230 Acc=0.5458 F1=0.4650 || Val Loss=1.0640 Acc=0.4032 F1=0.3128\n",
      "Epoch 36/200 | Train Loss=0.9091 Acc=0.5558 F1=0.4775 || Val Loss=1.0907 Acc=0.4516 F1=0.3926\n",
      "Epoch 37/200 | Train Loss=0.8905 Acc=0.5896 F1=0.5079 || Val Loss=1.0694 Acc=0.4516 F1=0.3751\n",
      "Epoch 38/200 | Train Loss=0.8823 Acc=0.5817 F1=0.4996 || Val Loss=1.0721 Acc=0.4677 F1=0.3856\n",
      "Epoch 39/200 | Train Loss=0.9069 Acc=0.5618 F1=0.4857 || Val Loss=1.0853 Acc=0.4516 F1=0.3822\n",
      "Epoch 40/200 | Train Loss=0.8855 Acc=0.5717 F1=0.4904 || Val Loss=1.0965 Acc=0.4516 F1=0.3822\n",
      "Epoch 41/200 | Train Loss=0.8826 Acc=0.5697 F1=0.4847 || Val Loss=1.0836 Acc=0.4355 F1=0.3641\n",
      "Epoch 42/200 | Train Loss=0.8673 Acc=0.5956 F1=0.5287 || Val Loss=1.0940 Acc=0.4194 F1=0.3725\n",
      "Epoch 43/200 | Train Loss=0.8732 Acc=0.5876 F1=0.5253 || Val Loss=1.1129 Acc=0.4355 F1=0.3992\n",
      "Epoch 44/200 | Train Loss=0.8703 Acc=0.6016 F1=0.5196 || Val Loss=1.0920 Acc=0.4032 F1=0.3526\n",
      "Epoch 45/200 | Train Loss=0.8614 Acc=0.5956 F1=0.5104 || Val Loss=1.1108 Acc=0.4355 F1=0.3895\n",
      "Epoch 46/200 | Train Loss=0.8374 Acc=0.6315 F1=0.5707 || Val Loss=1.0782 Acc=0.4516 F1=0.4018\n",
      "Epoch 47/200 | Train Loss=0.8548 Acc=0.6076 F1=0.5137 || Val Loss=1.1123 Acc=0.4194 F1=0.3604\n",
      "Epoch 48/200 | Train Loss=0.8352 Acc=0.6056 F1=0.5336 || Val Loss=1.1461 Acc=0.4032 F1=0.3595\n",
      "Epoch 49/200 | Train Loss=0.8412 Acc=0.6016 F1=0.5378 || Val Loss=1.1411 Acc=0.4194 F1=0.3905\n",
      "Epoch 50/200 | Train Loss=0.8259 Acc=0.5956 F1=0.5162 || Val Loss=1.1492 Acc=0.4194 F1=0.3903\n",
      "Epoch 51/200 | Train Loss=0.8329 Acc=0.6275 F1=0.5438 || Val Loss=1.1135 Acc=0.3710 F1=0.3490\n",
      "Epoch 52/200 | Train Loss=0.8589 Acc=0.5956 F1=0.5442 || Val Loss=1.1142 Acc=0.3871 F1=0.3510\n",
      "Epoch 53/200 | Train Loss=0.8413 Acc=0.6275 F1=0.5442 || Val Loss=1.1587 Acc=0.3871 F1=0.3295\n",
      "[INFO] Early stopping triggered at epoch 53\n",
      "[RESULT] Fold 1: Test Loss=1.1005 Acc=0.4355 F1=0.3043\n",
      "\n",
      "[INFO] Starting Fold 2/36\n",
      "Epoch 1/200 | Train Loss=1.0521 Acc=0.4391 F1=0.3172 || Val Loss=0.9883 Acc=0.4355 F1=0.2022\n",
      "Epoch 2/200 | Train Loss=0.9831 Acc=0.4830 F1=0.2497 || Val Loss=0.9542 Acc=0.4355 F1=0.2022\n",
      "Epoch 3/200 | Train Loss=0.9851 Acc=0.4750 F1=0.2935 || Val Loss=0.9465 Acc=0.4677 F1=0.2770\n",
      "Epoch 4/200 | Train Loss=0.9767 Acc=0.4810 F1=0.3219 || Val Loss=0.9490 Acc=0.4355 F1=0.2022\n",
      "Epoch 5/200 | Train Loss=0.9718 Acc=0.4930 F1=0.2807 || Val Loss=0.9556 Acc=0.4355 F1=0.2022\n",
      "Epoch 6/200 | Train Loss=0.9750 Acc=0.4850 F1=0.2506 || Val Loss=0.9583 Acc=0.4355 F1=0.2022\n",
      "Epoch 7/200 | Train Loss=0.9704 Acc=0.4870 F1=0.2685 || Val Loss=0.9439 Acc=0.4516 F1=0.2572\n",
      "Epoch 8/200 | Train Loss=0.9701 Acc=0.4830 F1=0.2906 || Val Loss=0.9391 Acc=0.4677 F1=0.3043\n",
      "Epoch 9/200 | Train Loss=0.9658 Acc=0.4910 F1=0.2975 || Val Loss=0.9379 Acc=0.4516 F1=0.2692\n",
      "Epoch 10/200 | Train Loss=0.9612 Acc=0.4910 F1=0.2867 || Val Loss=0.9393 Acc=0.4677 F1=0.2770\n",
      "Epoch 11/200 | Train Loss=0.9582 Acc=0.4970 F1=0.2808 || Val Loss=0.9377 Acc=0.4516 F1=0.2692\n",
      "Epoch 12/200 | Train Loss=0.9591 Acc=0.5070 F1=0.3099 || Val Loss=0.9350 Acc=0.5161 F1=0.3446\n",
      "Epoch 13/200 | Train Loss=0.9585 Acc=0.5090 F1=0.3176 || Val Loss=0.9341 Acc=0.4839 F1=0.3132\n",
      "Epoch 14/200 | Train Loss=0.9502 Acc=0.5210 F1=0.3397 || Val Loss=0.9339 Acc=0.4677 F1=0.3202\n",
      "Epoch 15/200 | Train Loss=0.9483 Acc=0.5110 F1=0.3382 || Val Loss=0.9477 Acc=0.4516 F1=0.2696\n",
      "Epoch 16/200 | Train Loss=0.9491 Acc=0.5190 F1=0.3430 || Val Loss=0.9530 Acc=0.4677 F1=0.3103\n",
      "Epoch 17/200 | Train Loss=0.9403 Acc=0.5210 F1=0.3557 || Val Loss=0.9570 Acc=0.4677 F1=0.3041\n",
      "Epoch 18/200 | Train Loss=0.9365 Acc=0.5070 F1=0.3392 || Val Loss=0.9700 Acc=0.4839 F1=0.3251\n",
      "Epoch 19/200 | Train Loss=0.9304 Acc=0.5309 F1=0.3641 || Val Loss=0.9706 Acc=0.5000 F1=0.3429\n",
      "Epoch 20/200 | Train Loss=0.9202 Acc=0.5469 F1=0.3769 || Val Loss=0.9931 Acc=0.5323 F1=0.3626\n",
      "Epoch 21/200 | Train Loss=0.9220 Acc=0.5150 F1=0.3544 || Val Loss=1.0060 Acc=0.5161 F1=0.3586\n",
      "Epoch 22/200 | Train Loss=0.9118 Acc=0.5449 F1=0.3785 || Val Loss=1.0306 Acc=0.4516 F1=0.3011\n",
      "Epoch 23/200 | Train Loss=0.9017 Acc=0.5549 F1=0.3853 || Val Loss=1.0523 Acc=0.4516 F1=0.3127\n",
      "Epoch 24/200 | Train Loss=0.9116 Acc=0.5389 F1=0.3790 || Val Loss=1.0547 Acc=0.4516 F1=0.3127\n",
      "Epoch 25/200 | Train Loss=0.9107 Acc=0.5329 F1=0.3689 || Val Loss=1.0184 Acc=0.4355 F1=0.2964\n",
      "Epoch 26/200 | Train Loss=0.9002 Acc=0.5549 F1=0.3919 || Val Loss=1.0245 Acc=0.4355 F1=0.3000\n",
      "Epoch 27/200 | Train Loss=0.8818 Acc=0.5689 F1=0.4033 || Val Loss=1.1566 Acc=0.4516 F1=0.2880\n",
      "Epoch 28/200 | Train Loss=0.9054 Acc=0.5449 F1=0.3716 || Val Loss=1.0480 Acc=0.4516 F1=0.3165\n",
      "Epoch 29/200 | Train Loss=0.9168 Acc=0.5309 F1=0.3785 || Val Loss=0.9829 Acc=0.4355 F1=0.3043\n",
      "Epoch 30/200 | Train Loss=0.8912 Acc=0.5609 F1=0.3974 || Val Loss=1.0880 Acc=0.4677 F1=0.2499\n",
      "Epoch 31/200 | Train Loss=0.8887 Acc=0.5609 F1=0.3633 || Val Loss=1.1079 Acc=0.4839 F1=0.2718\n",
      "Epoch 32/200 | Train Loss=0.8652 Acc=0.5749 F1=0.4094 || Val Loss=1.0531 Acc=0.4194 F1=0.2898\n",
      "Epoch 33/200 | Train Loss=0.8650 Acc=0.5848 F1=0.4342 || Val Loss=1.0825 Acc=0.4516 F1=0.3127\n",
      "Epoch 34/200 | Train Loss=0.8642 Acc=0.5908 F1=0.4382 || Val Loss=1.1163 Acc=0.4516 F1=0.3061\n",
      "Epoch 35/200 | Train Loss=0.8484 Acc=0.5689 F1=0.4296 || Val Loss=1.1030 Acc=0.4516 F1=0.3100\n",
      "Epoch 36/200 | Train Loss=0.8542 Acc=0.5788 F1=0.4292 || Val Loss=1.0791 Acc=0.4516 F1=0.3150\n",
      "Epoch 37/200 | Train Loss=0.8356 Acc=0.5808 F1=0.4311 || Val Loss=1.1534 Acc=0.4032 F1=0.2681\n",
      "Epoch 38/200 | Train Loss=0.8323 Acc=0.5968 F1=0.4471 || Val Loss=1.1476 Acc=0.4194 F1=0.2867\n",
      "Epoch 39/200 | Train Loss=0.8239 Acc=0.6128 F1=0.4541 || Val Loss=1.1398 Acc=0.4677 F1=0.3256\n",
      "Epoch 40/200 | Train Loss=0.8208 Acc=0.5988 F1=0.4523 || Val Loss=1.2118 Acc=0.4194 F1=0.2825\n",
      "[INFO] Early stopping triggered at epoch 40\n",
      "[RESULT] Fold 2: Test Loss=0.9427 Acc=0.5312 F1=0.3203\n",
      "\n",
      "[INFO] Starting Fold 3/36\n",
      "Epoch 1/200 | Train Loss=1.0279 Acc=0.4187 F1=0.3140 || Val Loss=1.0390 Acc=0.4531 F1=0.2079\n",
      "Epoch 2/200 | Train Loss=0.9621 Acc=0.4841 F1=0.2372 || Val Loss=1.0720 Acc=0.4531 F1=0.2298\n",
      "Epoch 3/200 | Train Loss=0.9595 Acc=0.5060 F1=0.2746 || Val Loss=1.0455 Acc=0.4531 F1=0.2298\n",
      "Epoch 4/200 | Train Loss=0.9561 Acc=0.5020 F1=0.2860 || Val Loss=1.0211 Acc=0.4375 F1=0.2414\n",
      "Epoch 5/200 | Train Loss=0.9515 Acc=0.5278 F1=0.3277 || Val Loss=1.0217 Acc=0.4531 F1=0.2298\n",
      "Epoch 6/200 | Train Loss=0.9459 Acc=0.5079 F1=0.2887 || Val Loss=1.0377 Acc=0.4531 F1=0.2298\n",
      "Epoch 7/200 | Train Loss=0.9477 Acc=0.5119 F1=0.2793 || Val Loss=1.0526 Acc=0.4531 F1=0.2298\n",
      "Epoch 8/200 | Train Loss=0.9450 Acc=0.5099 F1=0.2842 || Val Loss=1.0463 Acc=0.4531 F1=0.2298\n",
      "Epoch 9/200 | Train Loss=0.9388 Acc=0.5139 F1=0.2897 || Val Loss=1.0377 Acc=0.4531 F1=0.2298\n",
      "Epoch 10/200 | Train Loss=0.9397 Acc=0.5139 F1=0.2932 || Val Loss=1.0300 Acc=0.4531 F1=0.2298\n",
      "Epoch 11/200 | Train Loss=0.9397 Acc=0.5139 F1=0.3044 || Val Loss=1.0302 Acc=0.4531 F1=0.2298\n",
      "Epoch 12/200 | Train Loss=0.9383 Acc=0.5179 F1=0.3200 || Val Loss=1.0324 Acc=0.4375 F1=0.2238\n",
      "Epoch 13/200 | Train Loss=0.9350 Acc=0.5079 F1=0.3098 || Val Loss=1.0408 Acc=0.4531 F1=0.2298\n",
      "Epoch 14/200 | Train Loss=0.9388 Acc=0.5198 F1=0.3185 || Val Loss=1.0451 Acc=0.4375 F1=0.2238\n",
      "Epoch 15/200 | Train Loss=0.9324 Acc=0.5556 F1=0.3570 || Val Loss=1.0393 Acc=0.4531 F1=0.2298\n",
      "Epoch 16/200 | Train Loss=0.9310 Acc=0.5476 F1=0.3564 || Val Loss=1.0294 Acc=0.4375 F1=0.2252\n",
      "Epoch 17/200 | Train Loss=0.9336 Acc=0.5139 F1=0.3330 || Val Loss=1.0366 Acc=0.4531 F1=0.2312\n",
      "Epoch 18/200 | Train Loss=0.9324 Acc=0.5456 F1=0.3473 || Val Loss=1.0419 Acc=0.4375 F1=0.2268\n",
      "Epoch 19/200 | Train Loss=0.9265 Acc=0.5595 F1=0.3789 || Val Loss=1.0388 Acc=0.4062 F1=0.2146\n",
      "Epoch 20/200 | Train Loss=0.9317 Acc=0.5437 F1=0.3583 || Val Loss=1.0420 Acc=0.4531 F1=0.2327\n",
      "Epoch 21/200 | Train Loss=0.9207 Acc=0.5357 F1=0.3460 || Val Loss=1.0492 Acc=0.4531 F1=0.2327\n",
      "Epoch 22/200 | Train Loss=0.9245 Acc=0.5437 F1=0.3633 || Val Loss=1.0448 Acc=0.4062 F1=0.2146\n",
      "Epoch 23/200 | Train Loss=0.9245 Acc=0.5357 F1=0.3609 || Val Loss=1.0452 Acc=0.4531 F1=0.2327\n",
      "Epoch 24/200 | Train Loss=0.9180 Acc=0.5496 F1=0.3591 || Val Loss=1.0438 Acc=0.4531 F1=0.2327\n",
      "[INFO] Early stopping triggered at epoch 24\n",
      "[RESULT] Fold 3: Test Loss=0.9427 Acc=0.4918 F1=0.3104\n",
      "\n",
      "[INFO] Starting Fold 4/36\n",
      "Epoch 1/200 | Train Loss=1.0075 Acc=0.5219 F1=0.2785 || Val Loss=0.9873 Acc=0.4516 F1=0.2556\n",
      "Epoch 2/200 | Train Loss=0.9624 Acc=0.5199 F1=0.2626 || Val Loss=0.9381 Acc=0.3871 F1=0.2360\n",
      "Epoch 3/200 | Train Loss=0.9601 Acc=0.5139 F1=0.2650 || Val Loss=0.9421 Acc=0.4355 F1=0.2022\n",
      "Epoch 4/200 | Train Loss=0.9558 Acc=0.5239 F1=0.2421 || Val Loss=0.9649 Acc=0.4355 F1=0.2022\n",
      "Epoch 5/200 | Train Loss=0.9530 Acc=0.5299 F1=0.2409 || Val Loss=0.9597 Acc=0.4516 F1=0.2074\n",
      "Epoch 6/200 | Train Loss=0.9509 Acc=0.5219 F1=0.2322 || Val Loss=0.9512 Acc=0.4839 F1=0.2174\n",
      "Epoch 7/200 | Train Loss=0.9475 Acc=0.5239 F1=0.2357 || Val Loss=0.9596 Acc=0.4839 F1=0.2174\n",
      "Epoch 8/200 | Train Loss=0.9457 Acc=0.5179 F1=0.2401 || Val Loss=0.9518 Acc=0.4839 F1=0.2174\n",
      "Epoch 9/200 | Train Loss=0.9424 Acc=0.5259 F1=0.2455 || Val Loss=0.9595 Acc=0.4839 F1=0.2174\n",
      "Epoch 10/200 | Train Loss=0.9394 Acc=0.5239 F1=0.2450 || Val Loss=0.9605 Acc=0.4839 F1=0.2174\n",
      "Epoch 11/200 | Train Loss=0.9413 Acc=0.5319 F1=0.2593 || Val Loss=0.9682 Acc=0.4839 F1=0.2174\n",
      "Epoch 12/200 | Train Loss=0.9341 Acc=0.5378 F1=0.2667 || Val Loss=0.9701 Acc=0.4839 F1=0.2174\n",
      "Epoch 13/200 | Train Loss=0.9374 Acc=0.5378 F1=0.2747 || Val Loss=0.9758 Acc=0.4839 F1=0.2174\n",
      "Epoch 14/200 | Train Loss=0.9355 Acc=0.5359 F1=0.2878 || Val Loss=0.9633 Acc=0.5000 F1=0.2436\n",
      "Epoch 15/200 | Train Loss=0.9315 Acc=0.5538 F1=0.3308 || Val Loss=0.9865 Acc=0.5000 F1=0.2436\n",
      "Epoch 16/200 | Train Loss=0.9337 Acc=0.5259 F1=0.2747 || Val Loss=1.0122 Acc=0.5000 F1=0.2436\n",
      "Epoch 17/200 | Train Loss=0.9277 Acc=0.5259 F1=0.3059 || Val Loss=0.9586 Acc=0.5323 F1=0.3176\n",
      "Epoch 18/200 | Train Loss=0.9295 Acc=0.5458 F1=0.3309 || Val Loss=1.0024 Acc=0.5161 F1=0.2682\n",
      "Epoch 19/200 | Train Loss=0.9236 Acc=0.5478 F1=0.3373 || Val Loss=0.9633 Acc=0.5323 F1=0.3368\n",
      "Epoch 20/200 | Train Loss=0.9165 Acc=0.5558 F1=0.3426 || Val Loss=1.0549 Acc=0.5161 F1=0.2682\n",
      "Epoch 21/200 | Train Loss=0.9227 Acc=0.5538 F1=0.3308 || Val Loss=0.9745 Acc=0.5323 F1=0.3368\n",
      "Epoch 22/200 | Train Loss=0.9175 Acc=0.5458 F1=0.3409 || Val Loss=1.0004 Acc=0.5484 F1=0.3365\n",
      "Epoch 23/200 | Train Loss=0.9108 Acc=0.5538 F1=0.3463 || Val Loss=1.0009 Acc=0.5645 F1=0.3626\n",
      "Epoch 24/200 | Train Loss=0.9056 Acc=0.5697 F1=0.3623 || Val Loss=1.0137 Acc=0.5645 F1=0.3626\n",
      "Epoch 25/200 | Train Loss=0.9086 Acc=0.5598 F1=0.3714 || Val Loss=1.0526 Acc=0.5806 F1=0.3708\n",
      "Epoch 26/200 | Train Loss=0.8981 Acc=0.5777 F1=0.3694 || Val Loss=1.0258 Acc=0.5323 F1=0.3438\n",
      "Epoch 27/200 | Train Loss=0.8989 Acc=0.5578 F1=0.3713 || Val Loss=1.0857 Acc=0.5806 F1=0.3708\n",
      "Epoch 28/200 | Train Loss=0.9040 Acc=0.5598 F1=0.3584 || Val Loss=0.9945 Acc=0.5161 F1=0.3405\n",
      "Epoch 29/200 | Train Loss=0.8946 Acc=0.5916 F1=0.4005 || Val Loss=1.0833 Acc=0.5806 F1=0.3708\n",
      "Epoch 30/200 | Train Loss=0.8906 Acc=0.5777 F1=0.3558 || Val Loss=1.0279 Acc=0.5484 F1=0.3593\n",
      "Epoch 31/200 | Train Loss=0.8864 Acc=0.5558 F1=0.3710 || Val Loss=1.1019 Acc=0.5968 F1=0.3872\n",
      "Epoch 32/200 | Train Loss=0.8892 Acc=0.5697 F1=0.3665 || Val Loss=1.0240 Acc=0.5484 F1=0.3644\n",
      "Epoch 33/200 | Train Loss=0.8810 Acc=0.5916 F1=0.4068 || Val Loss=1.1368 Acc=0.5806 F1=0.3708\n",
      "Epoch 34/200 | Train Loss=0.8873 Acc=0.5817 F1=0.3601 || Val Loss=1.0791 Acc=0.5806 F1=0.3778\n",
      "Epoch 35/200 | Train Loss=0.8771 Acc=0.5936 F1=0.4057 || Val Loss=1.0837 Acc=0.5806 F1=0.3778\n",
      "Epoch 36/200 | Train Loss=0.8704 Acc=0.5777 F1=0.3770 || Val Loss=1.1903 Acc=0.5968 F1=0.3872\n",
      "Epoch 37/200 | Train Loss=0.8681 Acc=0.5976 F1=0.3893 || Val Loss=1.1280 Acc=0.5645 F1=0.3686\n",
      "Epoch 38/200 | Train Loss=0.8601 Acc=0.5996 F1=0.4040 || Val Loss=1.1291 Acc=0.5484 F1=0.3593\n",
      "Epoch 39/200 | Train Loss=0.8587 Acc=0.6116 F1=0.4083 || Val Loss=1.1405 Acc=0.5645 F1=0.3686\n",
      "Epoch 40/200 | Train Loss=0.8550 Acc=0.6135 F1=0.4114 || Val Loss=1.1607 Acc=0.5806 F1=0.3778\n",
      "Epoch 41/200 | Train Loss=0.8454 Acc=0.5976 F1=0.3994 || Val Loss=1.2631 Acc=0.5645 F1=0.3686\n",
      "Epoch 42/200 | Train Loss=0.8482 Acc=0.5876 F1=0.3930 || Val Loss=1.1294 Acc=0.5000 F1=0.3315\n",
      "Epoch 43/200 | Train Loss=0.8510 Acc=0.6016 F1=0.4018 || Val Loss=1.1653 Acc=0.5806 F1=0.3778\n",
      "Epoch 44/200 | Train Loss=0.8388 Acc=0.6016 F1=0.4057 || Val Loss=1.1725 Acc=0.4839 F1=0.3265\n",
      "Epoch 45/200 | Train Loss=0.8072 Acc=0.6375 F1=0.4321 || Val Loss=1.3134 Acc=0.5806 F1=0.3708\n",
      "Epoch 46/200 | Train Loss=0.8400 Acc=0.6155 F1=0.4142 || Val Loss=1.1426 Acc=0.5484 F1=0.3648\n",
      "Epoch 47/200 | Train Loss=0.8467 Acc=0.6056 F1=0.3909 || Val Loss=1.0676 Acc=0.5484 F1=0.3527\n",
      "Epoch 48/200 | Train Loss=0.8266 Acc=0.6175 F1=0.4221 || Val Loss=1.0823 Acc=0.4677 F1=0.3201\n",
      "Epoch 49/200 | Train Loss=0.8193 Acc=0.6215 F1=0.4217 || Val Loss=1.2488 Acc=0.5484 F1=0.3527\n",
      "Epoch 50/200 | Train Loss=0.8238 Acc=0.6454 F1=0.4355 || Val Loss=1.0561 Acc=0.5323 F1=0.3501\n",
      "Epoch 51/200 | Train Loss=0.8087 Acc=0.6155 F1=0.4086 || Val Loss=1.1808 Acc=0.5484 F1=0.3593\n",
      "[INFO] Early stopping triggered at epoch 51\n",
      "[RESULT] Fold 4: Test Loss=0.9108 Acc=0.5645 F1=0.3646\n",
      "\n",
      "[INFO] Starting Fold 5/36\n",
      "Epoch 1/200 | Train Loss=1.0254 Acc=0.4731 F1=0.3340 || Val Loss=0.9837 Acc=0.4516 F1=0.2420\n",
      "Epoch 2/200 | Train Loss=0.9387 Acc=0.5110 F1=0.2646 || Val Loss=0.9511 Acc=0.4355 F1=0.2353\n",
      "Epoch 3/200 | Train Loss=0.9278 Acc=0.4691 F1=0.3234 || Val Loss=0.9483 Acc=0.4355 F1=0.3017\n",
      "Epoch 4/200 | Train Loss=0.9237 Acc=0.5070 F1=0.3052 || Val Loss=0.9537 Acc=0.4516 F1=0.2420\n",
      "Epoch 5/200 | Train Loss=0.9208 Acc=0.5110 F1=0.2645 || Val Loss=0.9528 Acc=0.4516 F1=0.2420\n",
      "Epoch 6/200 | Train Loss=0.9165 Acc=0.5130 F1=0.2717 || Val Loss=0.9441 Acc=0.4516 F1=0.2774\n",
      "Epoch 7/200 | Train Loss=0.9188 Acc=0.5090 F1=0.2816 || Val Loss=0.9426 Acc=0.4355 F1=0.2600\n",
      "Epoch 8/200 | Train Loss=0.9104 Acc=0.5210 F1=0.2887 || Val Loss=0.9483 Acc=0.4516 F1=0.2420\n",
      "Epoch 9/200 | Train Loss=0.9075 Acc=0.5150 F1=0.2726 || Val Loss=0.9484 Acc=0.4516 F1=0.2672\n",
      "Epoch 10/200 | Train Loss=0.9093 Acc=0.5210 F1=0.2939 || Val Loss=0.9459 Acc=0.3871 F1=0.2446\n",
      "Epoch 11/200 | Train Loss=0.9075 Acc=0.5110 F1=0.2844 || Val Loss=0.9446 Acc=0.4194 F1=0.2752\n",
      "Epoch 12/200 | Train Loss=0.9012 Acc=0.5210 F1=0.2974 || Val Loss=0.9471 Acc=0.4194 F1=0.2750\n",
      "Epoch 13/200 | Train Loss=0.9027 Acc=0.5230 F1=0.3183 || Val Loss=0.9426 Acc=0.4355 F1=0.2941\n",
      "Epoch 14/200 | Train Loss=0.8981 Acc=0.5329 F1=0.3271 || Val Loss=0.9516 Acc=0.4032 F1=0.2601\n",
      "Epoch 15/200 | Train Loss=0.8961 Acc=0.5309 F1=0.3129 || Val Loss=0.9506 Acc=0.4194 F1=0.2752\n",
      "Epoch 16/200 | Train Loss=0.8946 Acc=0.5329 F1=0.3409 || Val Loss=0.9443 Acc=0.4355 F1=0.3003\n",
      "Epoch 17/200 | Train Loss=0.8947 Acc=0.5309 F1=0.3338 || Val Loss=0.9590 Acc=0.4032 F1=0.2605\n",
      "Epoch 18/200 | Train Loss=0.8929 Acc=0.5230 F1=0.3264 || Val Loss=0.9508 Acc=0.4194 F1=0.2849\n",
      "Epoch 19/200 | Train Loss=0.8938 Acc=0.5469 F1=0.3412 || Val Loss=0.9565 Acc=0.4516 F1=0.3036\n",
      "Epoch 20/200 | Train Loss=0.8838 Acc=0.5429 F1=0.3692 || Val Loss=0.9467 Acc=0.4355 F1=0.2983\n",
      "Epoch 21/200 | Train Loss=0.8840 Acc=0.5429 F1=0.3585 || Val Loss=0.9610 Acc=0.4355 F1=0.2902\n",
      "Epoch 22/200 | Train Loss=0.8856 Acc=0.5309 F1=0.3368 || Val Loss=0.9614 Acc=0.4355 F1=0.2902\n",
      "Epoch 23/200 | Train Loss=0.8818 Acc=0.5269 F1=0.3510 || Val Loss=0.9423 Acc=0.4516 F1=0.3128\n",
      "Epoch 24/200 | Train Loss=0.8829 Acc=0.5250 F1=0.3829 || Val Loss=0.9503 Acc=0.4516 F1=0.3128\n",
      "Epoch 25/200 | Train Loss=0.8752 Acc=0.5309 F1=0.3947 || Val Loss=0.9526 Acc=0.4516 F1=0.3108\n",
      "Epoch 26/200 | Train Loss=0.8710 Acc=0.5529 F1=0.3746 || Val Loss=0.9542 Acc=0.4516 F1=0.3108\n",
      "Epoch 27/200 | Train Loss=0.8695 Acc=0.5509 F1=0.3934 || Val Loss=0.9616 Acc=0.4516 F1=0.3081\n",
      "Epoch 28/200 | Train Loss=0.8836 Acc=0.5250 F1=0.3740 || Val Loss=0.9493 Acc=0.4677 F1=0.3231\n",
      "Epoch 29/200 | Train Loss=0.8834 Acc=0.5549 F1=0.3880 || Val Loss=0.9915 Acc=0.4516 F1=0.2987\n",
      "Epoch 30/200 | Train Loss=0.8663 Acc=0.5509 F1=0.4256 || Val Loss=0.9381 Acc=0.4194 F1=0.2913\n",
      "Epoch 31/200 | Train Loss=0.8669 Acc=0.5429 F1=0.4129 || Val Loss=0.9864 Acc=0.4677 F1=0.3169\n",
      "Epoch 32/200 | Train Loss=0.8716 Acc=0.5409 F1=0.3704 || Val Loss=0.9800 Acc=0.4677 F1=0.3177\n",
      "Epoch 33/200 | Train Loss=0.8588 Acc=0.5429 F1=0.4090 || Val Loss=0.9600 Acc=0.4355 F1=0.3023\n",
      "Epoch 34/200 | Train Loss=0.8542 Acc=0.5429 F1=0.3913 || Val Loss=1.0008 Acc=0.4516 F1=0.3039\n",
      "Epoch 35/200 | Train Loss=0.8627 Acc=0.5489 F1=0.3679 || Val Loss=0.9772 Acc=0.4516 F1=0.3128\n",
      "Epoch 36/200 | Train Loss=0.8621 Acc=0.5549 F1=0.3975 || Val Loss=0.9844 Acc=0.4516 F1=0.3128\n",
      "Epoch 37/200 | Train Loss=0.8494 Acc=0.5868 F1=0.4364 || Val Loss=0.9949 Acc=0.4516 F1=0.3108\n",
      "Epoch 38/200 | Train Loss=0.8498 Acc=0.5649 F1=0.4309 || Val Loss=0.9690 Acc=0.4516 F1=0.3139\n",
      "Epoch 39/200 | Train Loss=0.8616 Acc=0.5549 F1=0.3895 || Val Loss=1.0002 Acc=0.4355 F1=0.2983\n",
      "Epoch 40/200 | Train Loss=0.8511 Acc=0.5469 F1=0.4076 || Val Loss=0.9447 Acc=0.5000 F1=0.3436\n",
      "Epoch 41/200 | Train Loss=0.8407 Acc=0.5569 F1=0.4262 || Val Loss=1.0437 Acc=0.4355 F1=0.2845\n",
      "Epoch 42/200 | Train Loss=0.8379 Acc=0.5669 F1=0.3907 || Val Loss=0.9886 Acc=0.4032 F1=0.2782\n",
      "Epoch 43/200 | Train Loss=0.8295 Acc=0.6068 F1=0.4825 || Val Loss=0.9799 Acc=0.5000 F1=0.3478\n",
      "Epoch 44/200 | Train Loss=0.8261 Acc=0.5848 F1=0.4558 || Val Loss=1.0454 Acc=0.4194 F1=0.2849\n",
      "Epoch 45/200 | Train Loss=0.8250 Acc=0.5649 F1=0.3876 || Val Loss=1.0049 Acc=0.4839 F1=0.3367\n",
      "Epoch 46/200 | Train Loss=0.8219 Acc=0.6028 F1=0.4689 || Val Loss=1.0614 Acc=0.4355 F1=0.2945\n",
      "Epoch 47/200 | Train Loss=0.8092 Acc=0.6088 F1=0.4531 || Val Loss=1.0046 Acc=0.5484 F1=0.3814\n",
      "Epoch 48/200 | Train Loss=0.8065 Acc=0.6088 F1=0.4505 || Val Loss=1.0168 Acc=0.4355 F1=0.3023\n",
      "Epoch 49/200 | Train Loss=0.8076 Acc=0.6108 F1=0.4654 || Val Loss=1.0339 Acc=0.4516 F1=0.3108\n",
      "Epoch 50/200 | Train Loss=0.7993 Acc=0.6108 F1=0.4833 || Val Loss=1.0256 Acc=0.4839 F1=0.3364\n",
      "Epoch 51/200 | Train Loss=0.8078 Acc=0.6168 F1=0.4402 || Val Loss=1.0982 Acc=0.5000 F1=0.3475\n",
      "Epoch 52/200 | Train Loss=0.7830 Acc=0.6567 F1=0.5387 || Val Loss=1.0767 Acc=0.4516 F1=0.3124\n",
      "Epoch 53/200 | Train Loss=0.7988 Acc=0.6168 F1=0.4546 || Val Loss=1.0681 Acc=0.4839 F1=0.3348\n",
      "Epoch 54/200 | Train Loss=0.7897 Acc=0.6148 F1=0.5206 || Val Loss=1.0685 Acc=0.4839 F1=0.3348\n",
      "Epoch 55/200 | Train Loss=0.7792 Acc=0.6088 F1=0.4541 || Val Loss=1.1157 Acc=0.4516 F1=0.3104\n",
      "Epoch 56/200 | Train Loss=0.7723 Acc=0.6287 F1=0.4894 || Val Loss=1.0727 Acc=0.5161 F1=0.3579\n",
      "Epoch 57/200 | Train Loss=0.7500 Acc=0.6587 F1=0.5166 || Val Loss=1.1943 Acc=0.4516 F1=0.3124\n",
      "Epoch 58/200 | Train Loss=0.7486 Acc=0.6747 F1=0.5185 || Val Loss=1.0754 Acc=0.4839 F1=0.3363\n",
      "Epoch 59/200 | Train Loss=0.7469 Acc=0.6647 F1=0.5333 || Val Loss=1.1833 Acc=0.5323 F1=0.3691\n",
      "Epoch 60/200 | Train Loss=0.7356 Acc=0.6766 F1=0.5521 || Val Loss=1.1441 Acc=0.5323 F1=0.3691\n",
      "Epoch 61/200 | Train Loss=0.7241 Acc=0.6707 F1=0.5475 || Val Loss=1.1539 Acc=0.5000 F1=0.3475\n",
      "Epoch 62/200 | Train Loss=0.7128 Acc=0.7006 F1=0.5688 || Val Loss=1.1600 Acc=0.5484 F1=0.3797\n",
      "Epoch 63/200 | Train Loss=0.7237 Acc=0.6906 F1=0.5806 || Val Loss=1.2111 Acc=0.5000 F1=0.3430\n",
      "Epoch 64/200 | Train Loss=0.7301 Acc=0.6766 F1=0.5513 || Val Loss=1.2049 Acc=0.5161 F1=0.3588\n",
      "Epoch 65/200 | Train Loss=0.6925 Acc=0.6906 F1=0.5817 || Val Loss=1.2486 Acc=0.4516 F1=0.3104\n",
      "Epoch 66/200 | Train Loss=0.7128 Acc=0.6766 F1=0.5678 || Val Loss=1.2198 Acc=0.5000 F1=0.3467\n",
      "Epoch 67/200 | Train Loss=0.7036 Acc=0.6886 F1=0.5853 || Val Loss=1.2717 Acc=0.4839 F1=0.3348\n",
      "[INFO] Early stopping triggered at epoch 67\n",
      "[RESULT] Fold 5: Test Loss=0.9311 Acc=0.5156 F1=0.3528\n",
      "\n",
      "[INFO] Starting Fold 6/36\n",
      "Epoch 1/200 | Train Loss=1.1249 Acc=0.2738 F1=0.2408 || Val Loss=0.9155 Acc=0.4844 F1=0.2777\n",
      "Epoch 2/200 | Train Loss=0.8877 Acc=0.5298 F1=0.2713 || Val Loss=0.8820 Acc=0.5312 F1=0.2852\n",
      "Epoch 3/200 | Train Loss=0.8882 Acc=0.5317 F1=0.2721 || Val Loss=0.8824 Acc=0.5312 F1=0.2852\n",
      "Epoch 4/200 | Train Loss=0.8849 Acc=0.5298 F1=0.2889 || Val Loss=0.8735 Acc=0.5469 F1=0.3055\n",
      "Epoch 5/200 | Train Loss=0.8716 Acc=0.5377 F1=0.2844 || Val Loss=0.8714 Acc=0.5312 F1=0.2852\n",
      "Epoch 6/200 | Train Loss=0.8748 Acc=0.5377 F1=0.2723 || Val Loss=0.8719 Acc=0.5312 F1=0.2852\n",
      "Epoch 7/200 | Train Loss=0.8691 Acc=0.5397 F1=0.2706 || Val Loss=0.8683 Acc=0.5312 F1=0.2852\n",
      "Epoch 8/200 | Train Loss=0.8670 Acc=0.5417 F1=0.2820 || Val Loss=0.8652 Acc=0.5312 F1=0.2852\n",
      "Epoch 9/200 | Train Loss=0.8639 Acc=0.5357 F1=0.2997 || Val Loss=0.8643 Acc=0.5312 F1=0.2984\n",
      "Epoch 10/200 | Train Loss=0.8674 Acc=0.5357 F1=0.2984 || Val Loss=0.8642 Acc=0.5312 F1=0.2984\n",
      "Epoch 11/200 | Train Loss=0.8616 Acc=0.5377 F1=0.3038 || Val Loss=0.8620 Acc=0.5312 F1=0.2984\n",
      "Epoch 12/200 | Train Loss=0.8553 Acc=0.5516 F1=0.3179 || Val Loss=0.8600 Acc=0.5625 F1=0.3354\n",
      "Epoch 13/200 | Train Loss=0.8556 Acc=0.5377 F1=0.3127 || Val Loss=0.8593 Acc=0.5625 F1=0.3354\n",
      "Epoch 14/200 | Train Loss=0.8566 Acc=0.5575 F1=0.3402 || Val Loss=0.8595 Acc=0.5938 F1=0.3693\n",
      "Epoch 15/200 | Train Loss=0.8514 Acc=0.5476 F1=0.3372 || Val Loss=0.8560 Acc=0.5781 F1=0.3527\n",
      "Epoch 16/200 | Train Loss=0.8531 Acc=0.5476 F1=0.3350 || Val Loss=0.8557 Acc=0.5625 F1=0.3523\n",
      "Epoch 17/200 | Train Loss=0.8505 Acc=0.5595 F1=0.3476 || Val Loss=0.8554 Acc=0.5469 F1=0.3363\n",
      "Epoch 18/200 | Train Loss=0.8425 Acc=0.5774 F1=0.3548 || Val Loss=0.8561 Acc=0.5625 F1=0.3697\n",
      "Epoch 19/200 | Train Loss=0.8400 Acc=0.5794 F1=0.3812 || Val Loss=0.8630 Acc=0.5312 F1=0.3624\n",
      "Epoch 20/200 | Train Loss=0.8335 Acc=0.5595 F1=0.3693 || Val Loss=0.8597 Acc=0.5312 F1=0.3514\n",
      "Epoch 21/200 | Train Loss=0.8380 Acc=0.5595 F1=0.3523 || Val Loss=0.8632 Acc=0.5312 F1=0.3514\n",
      "Epoch 22/200 | Train Loss=0.8391 Acc=0.5476 F1=0.3604 || Val Loss=0.8692 Acc=0.5000 F1=0.3410\n",
      "Epoch 23/200 | Train Loss=0.8391 Acc=0.5536 F1=0.3650 || Val Loss=0.8701 Acc=0.5312 F1=0.3514\n",
      "Epoch 24/200 | Train Loss=0.8223 Acc=0.5734 F1=0.3643 || Val Loss=0.8701 Acc=0.5625 F1=0.3767\n",
      "Epoch 25/200 | Train Loss=0.8323 Acc=0.5794 F1=0.3747 || Val Loss=0.8734 Acc=0.5469 F1=0.3672\n",
      "Epoch 26/200 | Train Loss=0.8236 Acc=0.5813 F1=0.3758 || Val Loss=0.8752 Acc=0.5000 F1=0.3400\n",
      "Epoch 27/200 | Train Loss=0.8247 Acc=0.5595 F1=0.3713 || Val Loss=0.8756 Acc=0.5000 F1=0.3385\n",
      "Epoch 28/200 | Train Loss=0.8276 Acc=0.5774 F1=0.3714 || Val Loss=0.8820 Acc=0.5781 F1=0.3867\n",
      "Epoch 29/200 | Train Loss=0.8247 Acc=0.5893 F1=0.3852 || Val Loss=0.8837 Acc=0.4688 F1=0.3197\n",
      "Epoch 30/200 | Train Loss=0.8113 Acc=0.6131 F1=0.4116 || Val Loss=0.8872 Acc=0.4844 F1=0.3306\n",
      "Epoch 31/200 | Train Loss=0.8134 Acc=0.6071 F1=0.4037 || Val Loss=0.8994 Acc=0.5625 F1=0.3794\n",
      "Epoch 32/200 | Train Loss=0.8083 Acc=0.5873 F1=0.3877 || Val Loss=0.8915 Acc=0.5000 F1=0.3412\n",
      "Epoch 33/200 | Train Loss=0.8051 Acc=0.6171 F1=0.4127 || Val Loss=0.9073 Acc=0.4844 F1=0.3306\n",
      "Epoch 34/200 | Train Loss=0.8105 Acc=0.5913 F1=0.3921 || Val Loss=0.9078 Acc=0.4844 F1=0.3306\n",
      "Epoch 35/200 | Train Loss=0.8058 Acc=0.5972 F1=0.3990 || Val Loss=0.9028 Acc=0.4844 F1=0.3306\n",
      "Epoch 36/200 | Train Loss=0.8031 Acc=0.6111 F1=0.4077 || Val Loss=0.9052 Acc=0.5156 F1=0.3519\n",
      "Epoch 37/200 | Train Loss=0.8076 Acc=0.6052 F1=0.4021 || Val Loss=0.9026 Acc=0.5000 F1=0.3410\n",
      "Epoch 38/200 | Train Loss=0.8028 Acc=0.6032 F1=0.4069 || Val Loss=0.9094 Acc=0.4844 F1=0.3308\n",
      "Epoch 39/200 | Train Loss=0.7949 Acc=0.6091 F1=0.4081 || Val Loss=0.9215 Acc=0.5156 F1=0.3519\n",
      "Epoch 40/200 | Train Loss=0.7944 Acc=0.6270 F1=0.4271 || Val Loss=0.9087 Acc=0.4688 F1=0.3169\n",
      "Epoch 41/200 | Train Loss=0.7984 Acc=0.5952 F1=0.4076 || Val Loss=0.9264 Acc=0.4844 F1=0.3299\n",
      "Epoch 42/200 | Train Loss=0.7937 Acc=0.5933 F1=0.3937 || Val Loss=0.9400 Acc=0.5156 F1=0.3515\n",
      "Epoch 43/200 | Train Loss=0.7799 Acc=0.6210 F1=0.4198 || Val Loss=0.9347 Acc=0.5000 F1=0.3410\n",
      "Epoch 44/200 | Train Loss=0.7821 Acc=0.6190 F1=0.4154 || Val Loss=0.9613 Acc=0.5469 F1=0.3718\n",
      "Epoch 45/200 | Train Loss=0.7829 Acc=0.5833 F1=0.3847 || Val Loss=0.9490 Acc=0.4531 F1=0.3093\n",
      "Epoch 46/200 | Train Loss=0.7743 Acc=0.6171 F1=0.4210 || Val Loss=0.9891 Acc=0.4219 F1=0.2871\n",
      "Epoch 47/200 | Train Loss=0.7779 Acc=0.6091 F1=0.4084 || Val Loss=0.9856 Acc=0.4219 F1=0.2879\n",
      "Epoch 48/200 | Train Loss=0.7707 Acc=0.6091 F1=0.4155 || Val Loss=1.0019 Acc=0.4531 F1=0.3093\n",
      "[INFO] Early stopping triggered at epoch 48\n",
      "[RESULT] Fold 6: Test Loss=0.9801 Acc=0.4754 F1=0.3128\n",
      "\n",
      "[INFO] Starting Fold 7/36\n",
      "Epoch 1/200 | Train Loss=1.0650 Acc=0.3964 F1=0.2330 || Val Loss=1.1044 Acc=0.3065 F1=0.1564\n",
      "Epoch 2/200 | Train Loss=1.0114 Acc=0.4582 F1=0.2400 || Val Loss=1.1087 Acc=0.3065 F1=0.1564\n",
      "Epoch 3/200 | Train Loss=1.0124 Acc=0.4562 F1=0.3279 || Val Loss=1.0939 Acc=0.3065 F1=0.1747\n",
      "Epoch 4/200 | Train Loss=1.0051 Acc=0.4622 F1=0.3341 || Val Loss=1.0975 Acc=0.3065 F1=0.1564\n",
      "Epoch 5/200 | Train Loss=1.0089 Acc=0.4602 F1=0.2611 || Val Loss=1.1095 Acc=0.3065 F1=0.1564\n",
      "Epoch 6/200 | Train Loss=1.0057 Acc=0.4661 F1=0.2511 || Val Loss=1.1146 Acc=0.3065 F1=0.1564\n",
      "Epoch 7/200 | Train Loss=0.9996 Acc=0.4980 F1=0.3120 || Val Loss=1.1085 Acc=0.3065 F1=0.1747\n",
      "Epoch 8/200 | Train Loss=1.0003 Acc=0.4940 F1=0.3552 || Val Loss=1.0945 Acc=0.3065 F1=0.1883\n",
      "Epoch 9/200 | Train Loss=0.9981 Acc=0.4821 F1=0.3331 || Val Loss=1.1108 Acc=0.3226 F1=0.1818\n",
      "Epoch 10/200 | Train Loss=0.9894 Acc=0.4960 F1=0.3237 || Val Loss=1.1077 Acc=0.3226 F1=0.2348\n",
      "Epoch 11/200 | Train Loss=0.9901 Acc=0.4801 F1=0.3578 || Val Loss=1.0917 Acc=0.3871 F1=0.3236\n",
      "Epoch 12/200 | Train Loss=0.9830 Acc=0.5139 F1=0.3596 || Val Loss=1.1275 Acc=0.3387 F1=0.2547\n",
      "Epoch 13/200 | Train Loss=0.9820 Acc=0.5199 F1=0.3598 || Val Loss=1.1125 Acc=0.3226 F1=0.2461\n",
      "Epoch 14/200 | Train Loss=0.9738 Acc=0.5219 F1=0.3794 || Val Loss=1.1164 Acc=0.3065 F1=0.2532\n",
      "Epoch 15/200 | Train Loss=0.9844 Acc=0.5000 F1=0.3748 || Val Loss=1.1293 Acc=0.3387 F1=0.2728\n",
      "Epoch 16/200 | Train Loss=0.9740 Acc=0.5040 F1=0.3529 || Val Loss=1.1223 Acc=0.3226 F1=0.2340\n",
      "Epoch 17/200 | Train Loss=0.9718 Acc=0.5199 F1=0.3521 || Val Loss=1.1057 Acc=0.3387 F1=0.2647\n",
      "Epoch 18/200 | Train Loss=0.9590 Acc=0.5219 F1=0.3818 || Val Loss=1.0982 Acc=0.3548 F1=0.3011\n",
      "Epoch 19/200 | Train Loss=0.9717 Acc=0.5159 F1=0.3857 || Val Loss=1.1131 Acc=0.3710 F1=0.3132\n",
      "Epoch 20/200 | Train Loss=0.9720 Acc=0.4900 F1=0.3343 || Val Loss=1.1083 Acc=0.3065 F1=0.2267\n",
      "Epoch 21/200 | Train Loss=0.9648 Acc=0.5080 F1=0.3580 || Val Loss=1.0808 Acc=0.3226 F1=0.2755\n",
      "Epoch 22/200 | Train Loss=0.9606 Acc=0.5080 F1=0.3747 || Val Loss=1.1111 Acc=0.3387 F1=0.2908\n",
      "Epoch 23/200 | Train Loss=0.9562 Acc=0.5159 F1=0.3773 || Val Loss=1.1206 Acc=0.3226 F1=0.2780\n",
      "Epoch 24/200 | Train Loss=0.9603 Acc=0.5100 F1=0.3823 || Val Loss=1.0921 Acc=0.3710 F1=0.3088\n",
      "Epoch 25/200 | Train Loss=0.9475 Acc=0.5339 F1=0.4155 || Val Loss=1.1094 Acc=0.3387 F1=0.2891\n",
      "Epoch 26/200 | Train Loss=0.9388 Acc=0.5418 F1=0.3910 || Val Loss=1.1263 Acc=0.3548 F1=0.3032\n",
      "Epoch 27/200 | Train Loss=0.9372 Acc=0.5398 F1=0.3954 || Val Loss=1.1133 Acc=0.4032 F1=0.3298\n",
      "Epoch 28/200 | Train Loss=0.9290 Acc=0.5558 F1=0.4438 || Val Loss=1.1483 Acc=0.3548 F1=0.2998\n",
      "Epoch 29/200 | Train Loss=0.9320 Acc=0.5339 F1=0.3960 || Val Loss=1.1031 Acc=0.3710 F1=0.2989\n",
      "Epoch 30/200 | Train Loss=0.9242 Acc=0.5697 F1=0.4462 || Val Loss=1.1192 Acc=0.3548 F1=0.2824\n",
      "Epoch 31/200 | Train Loss=0.9365 Acc=0.5518 F1=0.4044 || Val Loss=1.1107 Acc=0.3871 F1=0.3088\n",
      "Epoch 32/200 | Train Loss=0.9248 Acc=0.5637 F1=0.4578 || Val Loss=1.1081 Acc=0.3710 F1=0.2836\n",
      "Epoch 33/200 | Train Loss=0.9202 Acc=0.5657 F1=0.4147 || Val Loss=1.1972 Acc=0.3871 F1=0.3191\n",
      "Epoch 34/200 | Train Loss=0.9132 Acc=0.5558 F1=0.4211 || Val Loss=1.1014 Acc=0.4032 F1=0.3105\n",
      "Epoch 35/200 | Train Loss=0.9093 Acc=0.5737 F1=0.4355 || Val Loss=1.1829 Acc=0.3548 F1=0.2947\n",
      "Epoch 36/200 | Train Loss=0.8971 Acc=0.5717 F1=0.4519 || Val Loss=1.1382 Acc=0.3548 F1=0.2754\n",
      "Epoch 37/200 | Train Loss=0.9095 Acc=0.5438 F1=0.4276 || Val Loss=1.1435 Acc=0.3387 F1=0.2671\n",
      "Epoch 38/200 | Train Loss=0.8705 Acc=0.5797 F1=0.4437 || Val Loss=1.2010 Acc=0.3710 F1=0.3108\n",
      "Epoch 39/200 | Train Loss=0.8910 Acc=0.5677 F1=0.4461 || Val Loss=1.1715 Acc=0.3548 F1=0.2843\n",
      "Epoch 40/200 | Train Loss=0.8717 Acc=0.6195 F1=0.4830 || Val Loss=1.2135 Acc=0.3871 F1=0.3158\n",
      "Epoch 41/200 | Train Loss=0.8876 Acc=0.5538 F1=0.4262 || Val Loss=1.1618 Acc=0.3871 F1=0.3253\n",
      "Epoch 42/200 | Train Loss=0.8833 Acc=0.5936 F1=0.4978 || Val Loss=1.2524 Acc=0.3548 F1=0.3208\n",
      "Epoch 43/200 | Train Loss=0.8620 Acc=0.6016 F1=0.4810 || Val Loss=1.1679 Acc=0.4032 F1=0.3445\n",
      "Epoch 44/200 | Train Loss=0.8462 Acc=0.6135 F1=0.4825 || Val Loss=1.1887 Acc=0.4194 F1=0.3356\n",
      "Epoch 45/200 | Train Loss=0.8513 Acc=0.6116 F1=0.4934 || Val Loss=1.2524 Acc=0.3548 F1=0.2901\n",
      "Epoch 46/200 | Train Loss=0.8442 Acc=0.6096 F1=0.4893 || Val Loss=1.2065 Acc=0.3871 F1=0.3330\n",
      "Epoch 47/200 | Train Loss=0.8352 Acc=0.6295 F1=0.5301 || Val Loss=1.2467 Acc=0.4032 F1=0.3485\n",
      "Epoch 48/200 | Train Loss=0.8309 Acc=0.6135 F1=0.5060 || Val Loss=1.2175 Acc=0.4194 F1=0.3529\n",
      "Epoch 49/200 | Train Loss=0.8265 Acc=0.6076 F1=0.5049 || Val Loss=1.3169 Acc=0.3710 F1=0.3394\n",
      "Epoch 50/200 | Train Loss=0.8137 Acc=0.6355 F1=0.5176 || Val Loss=1.3806 Acc=0.3548 F1=0.2843\n",
      "Epoch 51/200 | Train Loss=0.7916 Acc=0.6534 F1=0.5392 || Val Loss=1.4380 Acc=0.3710 F1=0.3099\n",
      "Epoch 52/200 | Train Loss=0.8072 Acc=0.6255 F1=0.4988 || Val Loss=1.2979 Acc=0.4032 F1=0.3258\n",
      "Epoch 53/200 | Train Loss=0.8205 Acc=0.6335 F1=0.5248 || Val Loss=1.2991 Acc=0.3548 F1=0.2958\n",
      "Epoch 54/200 | Train Loss=0.7982 Acc=0.6434 F1=0.5401 || Val Loss=1.4673 Acc=0.3226 F1=0.2647\n",
      "Epoch 55/200 | Train Loss=0.7774 Acc=0.6355 F1=0.5320 || Val Loss=1.3504 Acc=0.3871 F1=0.3167\n",
      "Epoch 56/200 | Train Loss=0.7889 Acc=0.6434 F1=0.5307 || Val Loss=1.4319 Acc=0.3871 F1=0.3237\n",
      "Epoch 57/200 | Train Loss=0.7796 Acc=0.6614 F1=0.5386 || Val Loss=1.3746 Acc=0.4032 F1=0.3636\n",
      "Epoch 58/200 | Train Loss=0.7718 Acc=0.6474 F1=0.5305 || Val Loss=1.4814 Acc=0.4194 F1=0.3691\n",
      "Epoch 59/200 | Train Loss=0.7787 Acc=0.6534 F1=0.5664 || Val Loss=1.3159 Acc=0.3871 F1=0.3352\n",
      "Epoch 60/200 | Train Loss=0.7610 Acc=0.6534 F1=0.5473 || Val Loss=1.6264 Acc=0.4355 F1=0.3632\n",
      "Epoch 61/200 | Train Loss=0.7704 Acc=0.6514 F1=0.5694 || Val Loss=1.3512 Acc=0.4194 F1=0.3851\n",
      "Epoch 62/200 | Train Loss=0.7430 Acc=0.6653 F1=0.5842 || Val Loss=1.6678 Acc=0.4032 F1=0.3313\n",
      "Epoch 63/200 | Train Loss=0.7571 Acc=0.6653 F1=0.5453 || Val Loss=1.3025 Acc=0.4194 F1=0.3470\n",
      "Epoch 64/200 | Train Loss=0.7465 Acc=0.6574 F1=0.5527 || Val Loss=1.5493 Acc=0.4355 F1=0.3618\n",
      "Epoch 65/200 | Train Loss=0.7110 Acc=0.6873 F1=0.5897 || Val Loss=1.6590 Acc=0.4355 F1=0.3842\n",
      "Epoch 66/200 | Train Loss=0.7199 Acc=0.6972 F1=0.6052 || Val Loss=1.4955 Acc=0.3871 F1=0.3770\n",
      "Epoch 67/200 | Train Loss=0.7171 Acc=0.6873 F1=0.6168 || Val Loss=1.6738 Acc=0.3710 F1=0.3058\n",
      "Epoch 68/200 | Train Loss=0.7328 Acc=0.6693 F1=0.5675 || Val Loss=1.6764 Acc=0.4194 F1=0.3466\n",
      "Epoch 69/200 | Train Loss=0.7420 Acc=0.6773 F1=0.5814 || Val Loss=1.7654 Acc=0.3710 F1=0.3197\n",
      "Epoch 70/200 | Train Loss=0.7375 Acc=0.6733 F1=0.5834 || Val Loss=1.7840 Acc=0.3710 F1=0.3235\n",
      "Epoch 71/200 | Train Loss=0.6967 Acc=0.7052 F1=0.6485 || Val Loss=1.7460 Acc=0.3065 F1=0.2941\n",
      "Epoch 72/200 | Train Loss=0.6976 Acc=0.6972 F1=0.6223 || Val Loss=1.7259 Acc=0.3710 F1=0.2949\n",
      "Epoch 73/200 | Train Loss=0.7104 Acc=0.6713 F1=0.5544 || Val Loss=1.7035 Acc=0.4355 F1=0.3604\n",
      "Epoch 74/200 | Train Loss=0.6888 Acc=0.6992 F1=0.6170 || Val Loss=1.5834 Acc=0.4194 F1=0.3450\n",
      "Epoch 75/200 | Train Loss=0.6969 Acc=0.6833 F1=0.5946 || Val Loss=1.9008 Acc=0.4194 F1=0.3739\n",
      "Epoch 76/200 | Train Loss=0.6780 Acc=0.7052 F1=0.6124 || Val Loss=1.8728 Acc=0.4516 F1=0.3911\n",
      "Epoch 77/200 | Train Loss=0.6491 Acc=0.7430 F1=0.6656 || Val Loss=1.7974 Acc=0.4677 F1=0.4126\n",
      "Epoch 78/200 | Train Loss=0.6694 Acc=0.6892 F1=0.6168 || Val Loss=1.8403 Acc=0.4355 F1=0.3457\n",
      "Epoch 79/200 | Train Loss=0.6873 Acc=0.7171 F1=0.6290 || Val Loss=1.8759 Acc=0.4194 F1=0.3701\n",
      "Epoch 80/200 | Train Loss=0.6531 Acc=0.7211 F1=0.6578 || Val Loss=1.8733 Acc=0.4355 F1=0.3730\n",
      "Epoch 81/200 | Train Loss=0.6456 Acc=0.7052 F1=0.6112 || Val Loss=1.7545 Acc=0.4516 F1=0.3887\n",
      "Epoch 82/200 | Train Loss=0.6520 Acc=0.7032 F1=0.6379 || Val Loss=1.7759 Acc=0.3871 F1=0.3443\n",
      "Epoch 83/200 | Train Loss=0.6243 Acc=0.7191 F1=0.6549 || Val Loss=2.0926 Acc=0.3871 F1=0.3107\n",
      "Epoch 84/200 | Train Loss=0.6413 Acc=0.7151 F1=0.6285 || Val Loss=1.9955 Acc=0.4032 F1=0.3588\n",
      "Epoch 85/200 | Train Loss=0.6609 Acc=0.6992 F1=0.6244 || Val Loss=1.8273 Acc=0.3710 F1=0.3485\n",
      "Epoch 86/200 | Train Loss=0.6405 Acc=0.7112 F1=0.6374 || Val Loss=2.0123 Acc=0.4516 F1=0.3776\n",
      "Epoch 87/200 | Train Loss=0.6305 Acc=0.7251 F1=0.6629 || Val Loss=1.9645 Acc=0.3226 F1=0.2674\n",
      "Epoch 88/200 | Train Loss=0.6260 Acc=0.7311 F1=0.6786 || Val Loss=1.9621 Acc=0.3065 F1=0.2573\n",
      "Epoch 89/200 | Train Loss=0.6117 Acc=0.7311 F1=0.6783 || Val Loss=2.0907 Acc=0.3871 F1=0.3294\n",
      "Epoch 90/200 | Train Loss=0.6334 Acc=0.7231 F1=0.6398 || Val Loss=2.2512 Acc=0.3871 F1=0.3405\n",
      "Epoch 91/200 | Train Loss=0.6541 Acc=0.7151 F1=0.6493 || Val Loss=1.8414 Acc=0.3710 F1=0.3441\n",
      "Epoch 92/200 | Train Loss=0.6265 Acc=0.7291 F1=0.6786 || Val Loss=2.0741 Acc=0.4194 F1=0.3728\n",
      "Epoch 93/200 | Train Loss=0.6644 Acc=0.7112 F1=0.6448 || Val Loss=2.1422 Acc=0.3548 F1=0.2870\n",
      "Epoch 94/200 | Train Loss=0.6592 Acc=0.7351 F1=0.6599 || Val Loss=2.0490 Acc=0.3871 F1=0.3072\n",
      "Epoch 95/200 | Train Loss=0.5936 Acc=0.7470 F1=0.6801 || Val Loss=2.1651 Acc=0.4032 F1=0.3582\n",
      "Epoch 96/200 | Train Loss=0.5786 Acc=0.7530 F1=0.6778 || Val Loss=2.0600 Acc=0.4194 F1=0.3602\n",
      "Epoch 97/200 | Train Loss=0.6016 Acc=0.7590 F1=0.6925 || Val Loss=2.2497 Acc=0.4355 F1=0.3811\n",
      "[INFO] Early stopping triggered at epoch 97\n",
      "[RESULT] Fold 7: Test Loss=1.6947 Acc=0.4032 F1=0.3176\n",
      "\n",
      "[INFO] Starting Fold 8/36\n",
      "Epoch 1/200 | Train Loss=1.0400 Acc=0.4291 F1=0.2927 || Val Loss=0.9069 Acc=0.4839 F1=0.2174\n",
      "Epoch 2/200 | Train Loss=1.0381 Acc=0.4291 F1=0.2199 || Val Loss=0.9316 Acc=0.5000 F1=0.2601\n",
      "Epoch 3/200 | Train Loss=1.0315 Acc=0.4291 F1=0.3100 || Val Loss=0.9426 Acc=0.5000 F1=0.2966\n",
      "Epoch 4/200 | Train Loss=1.0261 Acc=0.4651 F1=0.3330 || Val Loss=0.9197 Acc=0.5161 F1=0.3044\n",
      "Epoch 5/200 | Train Loss=1.0304 Acc=0.4311 F1=0.2898 || Val Loss=0.9044 Acc=0.5161 F1=0.3044\n",
      "Epoch 6/200 | Train Loss=1.0243 Acc=0.4611 F1=0.3361 || Val Loss=0.9040 Acc=0.5323 F1=0.3489\n",
      "Epoch 7/200 | Train Loss=1.0200 Acc=0.4810 F1=0.3338 || Val Loss=0.9080 Acc=0.5323 F1=0.3305\n",
      "Epoch 8/200 | Train Loss=1.0239 Acc=0.4611 F1=0.3174 || Val Loss=0.9106 Acc=0.5323 F1=0.3615\n",
      "Epoch 9/200 | Train Loss=1.0184 Acc=0.4691 F1=0.3416 || Val Loss=0.9038 Acc=0.5000 F1=0.2966\n",
      "Epoch 10/200 | Train Loss=1.0198 Acc=0.4691 F1=0.3342 || Val Loss=0.9006 Acc=0.5323 F1=0.3305\n",
      "Epoch 11/200 | Train Loss=1.0166 Acc=0.4471 F1=0.3237 || Val Loss=0.9075 Acc=0.4516 F1=0.2881\n",
      "Epoch 12/200 | Train Loss=1.0167 Acc=0.4671 F1=0.3392 || Val Loss=0.8936 Acc=0.4839 F1=0.3246\n",
      "Epoch 13/200 | Train Loss=1.0095 Acc=0.4611 F1=0.3265 || Val Loss=0.8987 Acc=0.4839 F1=0.3306\n",
      "Epoch 14/200 | Train Loss=1.0108 Acc=0.4910 F1=0.3570 || Val Loss=0.9028 Acc=0.4839 F1=0.2978\n",
      "Epoch 15/200 | Train Loss=1.0143 Acc=0.4631 F1=0.3324 || Val Loss=0.8980 Acc=0.5000 F1=0.2969\n",
      "Epoch 16/200 | Train Loss=1.0109 Acc=0.4790 F1=0.3468 || Val Loss=0.8871 Acc=0.5161 F1=0.3495\n",
      "Epoch 17/200 | Train Loss=1.0115 Acc=0.4970 F1=0.3596 || Val Loss=0.8879 Acc=0.5161 F1=0.3495\n",
      "Epoch 18/200 | Train Loss=1.0040 Acc=0.4671 F1=0.3350 || Val Loss=0.8952 Acc=0.5000 F1=0.3394\n",
      "Epoch 19/200 | Train Loss=1.0039 Acc=0.4890 F1=0.3562 || Val Loss=0.9023 Acc=0.5000 F1=0.3059\n",
      "Epoch 20/200 | Train Loss=1.0001 Acc=0.4830 F1=0.3509 || Val Loss=0.8868 Acc=0.5000 F1=0.3394\n",
      "Epoch 21/200 | Train Loss=1.0009 Acc=0.5110 F1=0.3726 || Val Loss=0.8818 Acc=0.4839 F1=0.3291\n",
      "Epoch 22/200 | Train Loss=0.9966 Acc=0.5010 F1=0.3654 || Val Loss=0.8950 Acc=0.4839 F1=0.3291\n",
      "Epoch 23/200 | Train Loss=0.9948 Acc=0.5050 F1=0.3746 || Val Loss=0.8971 Acc=0.4839 F1=0.3291\n",
      "Epoch 24/200 | Train Loss=0.9950 Acc=0.4990 F1=0.3639 || Val Loss=0.8868 Acc=0.5161 F1=0.3495\n",
      "Epoch 25/200 | Train Loss=0.9874 Acc=0.5190 F1=0.3789 || Val Loss=0.8884 Acc=0.5161 F1=0.3495\n",
      "Epoch 26/200 | Train Loss=0.9835 Acc=0.5170 F1=0.3819 || Val Loss=0.8826 Acc=0.4677 F1=0.3195\n",
      "Epoch 27/200 | Train Loss=0.9864 Acc=0.4990 F1=0.3638 || Val Loss=0.8791 Acc=0.4839 F1=0.3211\n",
      "Epoch 28/200 | Train Loss=0.9842 Acc=0.5150 F1=0.3755 || Val Loss=0.9070 Acc=0.5000 F1=0.3372\n",
      "[INFO] Early stopping triggered at epoch 28\n",
      "[RESULT] Fold 8: Test Loss=0.9650 Acc=0.5000 F1=0.3225\n",
      "\n",
      "[INFO] Starting Fold 9/36\n",
      "Epoch 1/200 | Train Loss=1.0784 Acc=0.3829 F1=0.2573 || Val Loss=1.0156 Acc=0.4531 F1=0.2538\n",
      "Epoch 2/200 | Train Loss=0.9713 Acc=0.4683 F1=0.2333 || Val Loss=1.0263 Acc=0.4688 F1=0.2151\n",
      "Epoch 3/200 | Train Loss=0.9747 Acc=0.4484 F1=0.2993 || Val Loss=1.0416 Acc=0.3750 F1=0.2354\n",
      "Epoch 4/200 | Train Loss=0.9747 Acc=0.4484 F1=0.2923 || Val Loss=1.0147 Acc=0.5156 F1=0.2735\n",
      "Epoch 5/200 | Train Loss=0.9629 Acc=0.4782 F1=0.2455 || Val Loss=1.0058 Acc=0.4844 F1=0.2608\n",
      "Epoch 6/200 | Train Loss=0.9607 Acc=0.4802 F1=0.2599 || Val Loss=1.0084 Acc=0.4688 F1=0.3105\n",
      "Epoch 7/200 | Train Loss=0.9583 Acc=0.5020 F1=0.3125 || Val Loss=1.0058 Acc=0.5000 F1=0.2979\n",
      "Epoch 8/200 | Train Loss=0.9536 Acc=0.4821 F1=0.2628 || Val Loss=1.0089 Acc=0.4844 F1=0.2766\n",
      "Epoch 9/200 | Train Loss=0.9498 Acc=0.4841 F1=0.2583 || Val Loss=1.0092 Acc=0.5312 F1=0.3365\n",
      "Epoch 10/200 | Train Loss=0.9454 Acc=0.4861 F1=0.2740 || Val Loss=1.0088 Acc=0.5000 F1=0.3555\n",
      "Epoch 11/200 | Train Loss=0.9390 Acc=0.5139 F1=0.3478 || Val Loss=1.0103 Acc=0.5156 F1=0.3660\n",
      "Epoch 12/200 | Train Loss=0.9407 Acc=0.5040 F1=0.3231 || Val Loss=1.0056 Acc=0.4844 F1=0.3258\n",
      "Epoch 13/200 | Train Loss=0.9341 Acc=0.4841 F1=0.3087 || Val Loss=1.0109 Acc=0.5312 F1=0.3780\n",
      "Epoch 14/200 | Train Loss=0.9305 Acc=0.4980 F1=0.3222 || Val Loss=1.0228 Acc=0.4531 F1=0.3128\n",
      "Epoch 15/200 | Train Loss=0.9269 Acc=0.4782 F1=0.3170 || Val Loss=1.0200 Acc=0.5625 F1=0.4036\n",
      "Epoch 16/200 | Train Loss=0.9245 Acc=0.4643 F1=0.3150 || Val Loss=1.0275 Acc=0.4531 F1=0.3131\n",
      "Epoch 17/200 | Train Loss=0.9140 Acc=0.4980 F1=0.3448 || Val Loss=1.0269 Acc=0.5469 F1=0.3916\n",
      "Epoch 18/200 | Train Loss=0.9139 Acc=0.5139 F1=0.3308 || Val Loss=1.0265 Acc=0.5625 F1=0.3959\n",
      "Epoch 19/200 | Train Loss=0.9058 Acc=0.5040 F1=0.3482 || Val Loss=1.0288 Acc=0.5469 F1=0.3923\n",
      "Epoch 20/200 | Train Loss=0.9144 Acc=0.4881 F1=0.3743 || Val Loss=1.0246 Acc=0.5625 F1=0.4006\n",
      "Epoch 21/200 | Train Loss=0.8985 Acc=0.4960 F1=0.3485 || Val Loss=1.0364 Acc=0.5625 F1=0.3956\n",
      "Epoch 22/200 | Train Loss=0.8951 Acc=0.5357 F1=0.3930 || Val Loss=1.0284 Acc=0.5469 F1=0.3856\n",
      "Epoch 23/200 | Train Loss=0.8930 Acc=0.5159 F1=0.4187 || Val Loss=1.0316 Acc=0.5625 F1=0.3956\n",
      "Epoch 24/200 | Train Loss=0.8888 Acc=0.5179 F1=0.4010 || Val Loss=1.0274 Acc=0.5781 F1=0.4059\n",
      "Epoch 25/200 | Train Loss=0.8806 Acc=0.5655 F1=0.4683 || Val Loss=1.0411 Acc=0.5625 F1=0.3956\n",
      "Epoch 26/200 | Train Loss=0.8763 Acc=0.5337 F1=0.4239 || Val Loss=1.0452 Acc=0.5625 F1=0.3959\n",
      "Epoch 27/200 | Train Loss=0.8725 Acc=0.5417 F1=0.4256 || Val Loss=1.0503 Acc=0.5469 F1=0.3826\n",
      "Epoch 28/200 | Train Loss=0.8655 Acc=0.5456 F1=0.4481 || Val Loss=1.0567 Acc=0.5781 F1=0.4054\n",
      "Epoch 29/200 | Train Loss=0.8662 Acc=0.5377 F1=0.4309 || Val Loss=1.0435 Acc=0.5781 F1=0.4085\n",
      "Epoch 30/200 | Train Loss=0.8578 Acc=0.5456 F1=0.4431 || Val Loss=1.0644 Acc=0.5469 F1=0.3882\n",
      "Epoch 31/200 | Train Loss=0.8580 Acc=0.5536 F1=0.4587 || Val Loss=1.0672 Acc=0.5781 F1=0.4658\n",
      "Epoch 32/200 | Train Loss=0.8502 Acc=0.5575 F1=0.4831 || Val Loss=1.0746 Acc=0.5625 F1=0.4532\n",
      "Epoch 33/200 | Train Loss=0.8489 Acc=0.5556 F1=0.4688 || Val Loss=1.1159 Acc=0.5156 F1=0.4070\n",
      "Epoch 34/200 | Train Loss=0.8434 Acc=0.5536 F1=0.4841 || Val Loss=1.1076 Acc=0.5156 F1=0.4238\n",
      "Epoch 35/200 | Train Loss=0.8263 Acc=0.5675 F1=0.5022 || Val Loss=1.1130 Acc=0.5000 F1=0.3897\n",
      "Epoch 36/200 | Train Loss=0.8376 Acc=0.5774 F1=0.4856 || Val Loss=1.1387 Acc=0.5000 F1=0.4024\n",
      "Epoch 37/200 | Train Loss=0.8196 Acc=0.5536 F1=0.4909 || Val Loss=1.1270 Acc=0.4531 F1=0.3658\n",
      "Epoch 38/200 | Train Loss=0.8060 Acc=0.5833 F1=0.5157 || Val Loss=1.1396 Acc=0.4844 F1=0.3631\n",
      "Epoch 39/200 | Train Loss=0.8136 Acc=0.5575 F1=0.4773 || Val Loss=1.1413 Acc=0.4219 F1=0.3538\n",
      "Epoch 40/200 | Train Loss=0.8175 Acc=0.5873 F1=0.5166 || Val Loss=1.1367 Acc=0.4375 F1=0.3442\n",
      "Epoch 41/200 | Train Loss=0.7971 Acc=0.5774 F1=0.5006 || Val Loss=1.1392 Acc=0.3906 F1=0.3110\n",
      "Epoch 42/200 | Train Loss=0.7995 Acc=0.5913 F1=0.5166 || Val Loss=1.1603 Acc=0.4219 F1=0.3415\n",
      "Epoch 43/200 | Train Loss=0.7880 Acc=0.5873 F1=0.5340 || Val Loss=1.1840 Acc=0.3438 F1=0.3285\n",
      "Epoch 44/200 | Train Loss=0.7752 Acc=0.5952 F1=0.5279 || Val Loss=1.2367 Acc=0.2969 F1=0.2493\n",
      "Epoch 45/200 | Train Loss=0.7831 Acc=0.5992 F1=0.5500 || Val Loss=1.2203 Acc=0.3438 F1=0.3231\n",
      "Epoch 46/200 | Train Loss=0.7684 Acc=0.5794 F1=0.5241 || Val Loss=1.2121 Acc=0.3750 F1=0.2877\n",
      "Epoch 47/200 | Train Loss=0.7564 Acc=0.6210 F1=0.5522 || Val Loss=1.2193 Acc=0.3906 F1=0.3236\n",
      "Epoch 48/200 | Train Loss=0.7590 Acc=0.6111 F1=0.5648 || Val Loss=1.2407 Acc=0.3594 F1=0.2996\n",
      "Epoch 49/200 | Train Loss=0.7665 Acc=0.5933 F1=0.5336 || Val Loss=1.2559 Acc=0.3750 F1=0.3237\n",
      "Epoch 50/200 | Train Loss=0.7493 Acc=0.6429 F1=0.5808 || Val Loss=1.2997 Acc=0.2969 F1=0.2767\n",
      "Epoch 51/200 | Train Loss=0.7516 Acc=0.6171 F1=0.5647 || Val Loss=1.2713 Acc=0.3594 F1=0.3160\n",
      "[INFO] Early stopping triggered at epoch 51\n",
      "[RESULT] Fold 9: Test Loss=1.3236 Acc=0.3934 F1=0.2287\n",
      "\n",
      "[INFO] Starting Fold 10/36\n",
      "Epoch 1/200 | Train Loss=1.0589 Acc=0.3645 F1=0.3250 || Val Loss=1.0384 Acc=0.3548 F1=0.2263\n",
      "Epoch 2/200 | Train Loss=0.9782 Acc=0.4622 F1=0.2335 || Val Loss=1.0850 Acc=0.4194 F1=0.3535\n",
      "Epoch 3/200 | Train Loss=0.9736 Acc=0.4223 F1=0.2963 || Val Loss=1.0603 Acc=0.5000 F1=0.2735\n",
      "Epoch 4/200 | Train Loss=0.9595 Acc=0.4681 F1=0.3059 || Val Loss=1.0505 Acc=0.3387 F1=0.1707\n",
      "Epoch 5/200 | Train Loss=0.9567 Acc=0.4821 F1=0.2433 || Val Loss=1.0296 Acc=0.3387 F1=0.2408\n",
      "Epoch 6/200 | Train Loss=0.9529 Acc=0.4920 F1=0.2700 || Val Loss=1.0296 Acc=0.4516 F1=0.2922\n",
      "Epoch 7/200 | Train Loss=0.9502 Acc=0.5020 F1=0.3259 || Val Loss=1.0451 Acc=0.4677 F1=0.3098\n",
      "Epoch 8/200 | Train Loss=0.9502 Acc=0.5080 F1=0.3420 || Val Loss=1.0495 Acc=0.4677 F1=0.2899\n",
      "Epoch 9/200 | Train Loss=0.9488 Acc=0.5159 F1=0.3447 || Val Loss=1.0386 Acc=0.4677 F1=0.3294\n",
      "Epoch 10/200 | Train Loss=0.9475 Acc=0.5199 F1=0.3294 || Val Loss=1.0236 Acc=0.4839 F1=0.3267\n",
      "Epoch 11/200 | Train Loss=0.9385 Acc=0.5339 F1=0.3605 || Val Loss=1.0207 Acc=0.5323 F1=0.3218\n",
      "Epoch 12/200 | Train Loss=0.9350 Acc=0.5179 F1=0.3576 || Val Loss=1.0291 Acc=0.5000 F1=0.3058\n",
      "Epoch 13/200 | Train Loss=0.9418 Acc=0.4900 F1=0.3211 || Val Loss=1.0361 Acc=0.5000 F1=0.3175\n",
      "Epoch 14/200 | Train Loss=0.9314 Acc=0.5259 F1=0.3640 || Val Loss=1.0261 Acc=0.5161 F1=0.3138\n",
      "Epoch 15/200 | Train Loss=0.9304 Acc=0.5259 F1=0.3660 || Val Loss=1.0203 Acc=0.5323 F1=0.3221\n",
      "Epoch 16/200 | Train Loss=0.9357 Acc=0.5060 F1=0.3535 || Val Loss=1.0238 Acc=0.5323 F1=0.3221\n",
      "Epoch 17/200 | Train Loss=0.9241 Acc=0.5359 F1=0.3717 || Val Loss=1.0239 Acc=0.5000 F1=0.3056\n",
      "Epoch 18/200 | Train Loss=0.9279 Acc=0.5378 F1=0.3692 || Val Loss=1.0304 Acc=0.5000 F1=0.3056\n",
      "Epoch 19/200 | Train Loss=0.9263 Acc=0.5120 F1=0.3581 || Val Loss=1.0320 Acc=0.5161 F1=0.3137\n",
      "Epoch 20/200 | Train Loss=0.9207 Acc=0.5259 F1=0.3616 || Val Loss=1.0318 Acc=0.5161 F1=0.3523\n",
      "Epoch 21/200 | Train Loss=0.9193 Acc=0.5438 F1=0.3715 || Val Loss=1.0384 Acc=0.5161 F1=0.3264\n",
      "Epoch 22/200 | Train Loss=0.9302 Acc=0.5080 F1=0.3571 || Val Loss=1.0444 Acc=0.4677 F1=0.2388\n",
      "[INFO] Early stopping triggered at epoch 22\n",
      "[RESULT] Fold 10: Test Loss=0.9925 Acc=0.5161 F1=0.3615\n",
      "\n",
      "[INFO] Starting Fold 11/36\n",
      "Epoch 1/200 | Train Loss=1.0449 Acc=0.3673 F1=0.3410 || Val Loss=0.9425 Acc=0.4355 F1=0.3069\n",
      "Epoch 2/200 | Train Loss=1.0289 Acc=0.4291 F1=0.3311 || Val Loss=0.9675 Acc=0.3871 F1=0.1905\n",
      "Epoch 3/200 | Train Loss=1.0121 Acc=0.4451 F1=0.2513 || Val Loss=0.9900 Acc=0.3710 F1=0.1847\n",
      "Epoch 4/200 | Train Loss=1.0136 Acc=0.4431 F1=0.3201 || Val Loss=0.9919 Acc=0.4839 F1=0.3415\n",
      "Epoch 5/200 | Train Loss=1.0096 Acc=0.4651 F1=0.3599 || Val Loss=0.9836 Acc=0.4194 F1=0.2540\n",
      "Epoch 6/200 | Train Loss=1.0040 Acc=0.4431 F1=0.2768 || Val Loss=0.9833 Acc=0.3871 F1=0.2171\n",
      "Epoch 7/200 | Train Loss=1.0033 Acc=0.4571 F1=0.2948 || Val Loss=0.9844 Acc=0.4194 F1=0.2631\n",
      "Epoch 8/200 | Train Loss=0.9924 Acc=0.4731 F1=0.3368 || Val Loss=1.0013 Acc=0.5000 F1=0.3394\n",
      "Epoch 9/200 | Train Loss=0.9960 Acc=0.4830 F1=0.3439 || Val Loss=1.0210 Acc=0.4516 F1=0.2880\n",
      "Epoch 10/200 | Train Loss=0.9849 Acc=0.4671 F1=0.3403 || Val Loss=1.0123 Acc=0.5161 F1=0.3532\n",
      "Epoch 11/200 | Train Loss=0.9858 Acc=0.4830 F1=0.3685 || Val Loss=1.0128 Acc=0.5323 F1=0.3694\n",
      "Epoch 12/200 | Train Loss=0.9807 Acc=0.4890 F1=0.3597 || Val Loss=1.0296 Acc=0.5484 F1=0.3798\n",
      "Epoch 13/200 | Train Loss=0.9815 Acc=0.4711 F1=0.3451 || Val Loss=1.0676 Acc=0.5000 F1=0.3433\n",
      "Epoch 14/200 | Train Loss=0.9745 Acc=0.4830 F1=0.3676 || Val Loss=1.0509 Acc=0.5161 F1=0.3607\n",
      "Epoch 15/200 | Train Loss=0.9722 Acc=0.4790 F1=0.3616 || Val Loss=1.0508 Acc=0.5000 F1=0.3499\n",
      "Epoch 16/200 | Train Loss=0.9713 Acc=0.5030 F1=0.3981 || Val Loss=1.0680 Acc=0.4839 F1=0.3328\n",
      "Epoch 17/200 | Train Loss=0.9633 Acc=0.4671 F1=0.3652 || Val Loss=1.0826 Acc=0.5161 F1=0.3619\n",
      "Epoch 18/200 | Train Loss=0.9577 Acc=0.4910 F1=0.3783 || Val Loss=1.0834 Acc=0.4839 F1=0.3359\n",
      "Epoch 19/200 | Train Loss=0.9541 Acc=0.4830 F1=0.3766 || Val Loss=1.0900 Acc=0.4839 F1=0.3414\n",
      "Epoch 20/200 | Train Loss=0.9535 Acc=0.5110 F1=0.4040 || Val Loss=1.1172 Acc=0.5000 F1=0.3560\n",
      "Epoch 21/200 | Train Loss=0.9587 Acc=0.4731 F1=0.3545 || Val Loss=1.1127 Acc=0.4839 F1=0.3449\n",
      "Epoch 22/200 | Train Loss=0.9483 Acc=0.4930 F1=0.3902 || Val Loss=1.1404 Acc=0.4355 F1=0.3196\n",
      "Epoch 23/200 | Train Loss=0.9442 Acc=0.5070 F1=0.4211 || Val Loss=1.1312 Acc=0.5323 F1=0.3792\n",
      "Epoch 24/200 | Train Loss=0.9433 Acc=0.5050 F1=0.4010 || Val Loss=1.1094 Acc=0.5484 F1=0.3875\n",
      "Epoch 25/200 | Train Loss=0.9409 Acc=0.4790 F1=0.3917 || Val Loss=1.1361 Acc=0.5000 F1=0.3630\n",
      "Epoch 26/200 | Train Loss=0.9306 Acc=0.5050 F1=0.4389 || Val Loss=1.1651 Acc=0.4516 F1=0.3237\n",
      "Epoch 27/200 | Train Loss=0.9284 Acc=0.5250 F1=0.4346 || Val Loss=1.1351 Acc=0.5000 F1=0.3546\n",
      "Epoch 28/200 | Train Loss=0.9224 Acc=0.5170 F1=0.4189 || Val Loss=1.1867 Acc=0.4194 F1=0.3170\n",
      "Epoch 29/200 | Train Loss=0.9290 Acc=0.5210 F1=0.4631 || Val Loss=1.1896 Acc=0.5000 F1=0.3620\n",
      "Epoch 30/200 | Train Loss=0.9095 Acc=0.5190 F1=0.4359 || Val Loss=1.1605 Acc=0.4516 F1=0.3306\n",
      "Epoch 31/200 | Train Loss=0.9242 Acc=0.5070 F1=0.4269 || Val Loss=1.1714 Acc=0.5000 F1=0.3521\n",
      "Epoch 32/200 | Train Loss=0.9034 Acc=0.5130 F1=0.4308 || Val Loss=1.2106 Acc=0.4516 F1=0.3331\n",
      "Epoch 33/200 | Train Loss=0.8998 Acc=0.5529 F1=0.4938 || Val Loss=1.2337 Acc=0.4355 F1=0.3239\n",
      "Epoch 34/200 | Train Loss=0.8927 Acc=0.5130 F1=0.4384 || Val Loss=1.1968 Acc=0.5161 F1=0.3700\n",
      "Epoch 35/200 | Train Loss=0.8911 Acc=0.5489 F1=0.4771 || Val Loss=1.2290 Acc=0.4194 F1=0.3167\n",
      "Epoch 36/200 | Train Loss=0.8815 Acc=0.5529 F1=0.4904 || Val Loss=1.2960 Acc=0.4516 F1=0.3202\n",
      "Epoch 37/200 | Train Loss=0.8799 Acc=0.5429 F1=0.4704 || Val Loss=1.2078 Acc=0.4355 F1=0.3188\n",
      "Epoch 38/200 | Train Loss=0.8836 Acc=0.5609 F1=0.5061 || Val Loss=1.3263 Acc=0.4194 F1=0.3077\n",
      "Epoch 39/200 | Train Loss=0.8644 Acc=0.5669 F1=0.5084 || Val Loss=1.2867 Acc=0.3871 F1=0.2932\n",
      "Epoch 40/200 | Train Loss=0.8599 Acc=0.5649 F1=0.4990 || Val Loss=1.3451 Acc=0.4194 F1=0.3109\n",
      "Epoch 41/200 | Train Loss=0.8621 Acc=0.5389 F1=0.4752 || Val Loss=1.2681 Acc=0.4032 F1=0.3059\n",
      "Epoch 42/200 | Train Loss=0.8463 Acc=0.5729 F1=0.5018 || Val Loss=1.3811 Acc=0.4032 F1=0.2957\n",
      "Epoch 43/200 | Train Loss=0.8467 Acc=0.5868 F1=0.5318 || Val Loss=1.3957 Acc=0.4032 F1=0.3063\n",
      "Epoch 44/200 | Train Loss=0.8460 Acc=0.5729 F1=0.5056 || Val Loss=1.3191 Acc=0.4355 F1=0.3315\n",
      "[INFO] Early stopping triggered at epoch 44\n",
      "[RESULT] Fold 11: Test Loss=1.0326 Acc=0.5156 F1=0.2731\n",
      "\n",
      "[INFO] Starting Fold 12/36\n",
      "Epoch 1/200 | Train Loss=1.0181 Acc=0.4306 F1=0.3139 || Val Loss=0.9652 Acc=0.4844 F1=0.2175\n",
      "Epoch 2/200 | Train Loss=0.9567 Acc=0.4802 F1=0.2407 || Val Loss=0.9893 Acc=0.4844 F1=0.2175\n",
      "Epoch 3/200 | Train Loss=0.9541 Acc=0.5020 F1=0.3281 || Val Loss=0.9708 Acc=0.5312 F1=0.3718\n",
      "Epoch 4/200 | Train Loss=0.9474 Acc=0.4762 F1=0.2968 || Val Loss=0.9611 Acc=0.5000 F1=0.2642\n",
      "Epoch 5/200 | Train Loss=0.9485 Acc=0.4921 F1=0.2800 || Val Loss=0.9592 Acc=0.5000 F1=0.2642\n",
      "Epoch 6/200 | Train Loss=0.9364 Acc=0.5000 F1=0.2901 || Val Loss=0.9620 Acc=0.5156 F1=0.2867\n",
      "Epoch 7/200 | Train Loss=0.9380 Acc=0.4940 F1=0.2827 || Val Loss=0.9621 Acc=0.5156 F1=0.2872\n",
      "Epoch 8/200 | Train Loss=0.9358 Acc=0.5000 F1=0.3197 || Val Loss=0.9610 Acc=0.5469 F1=0.3795\n",
      "Epoch 9/200 | Train Loss=0.9337 Acc=0.5179 F1=0.3543 || Val Loss=0.9579 Acc=0.5469 F1=0.3795\n",
      "Epoch 10/200 | Train Loss=0.9287 Acc=0.4940 F1=0.3226 || Val Loss=0.9528 Acc=0.5625 F1=0.3638\n",
      "Epoch 11/200 | Train Loss=0.9257 Acc=0.5218 F1=0.3380 || Val Loss=0.9572 Acc=0.5312 F1=0.3718\n",
      "Epoch 12/200 | Train Loss=0.9266 Acc=0.5258 F1=0.3622 || Val Loss=0.9586 Acc=0.5156 F1=0.3616\n",
      "Epoch 13/200 | Train Loss=0.9187 Acc=0.5437 F1=0.3639 || Val Loss=0.9570 Acc=0.5312 F1=0.3666\n",
      "Epoch 14/200 | Train Loss=0.9217 Acc=0.5139 F1=0.3337 || Val Loss=0.9603 Acc=0.5312 F1=0.3718\n",
      "Epoch 15/200 | Train Loss=0.9162 Acc=0.5357 F1=0.3647 || Val Loss=0.9672 Acc=0.5000 F1=0.3523\n",
      "Epoch 16/200 | Train Loss=0.9171 Acc=0.5437 F1=0.3696 || Val Loss=0.9636 Acc=0.5469 F1=0.3795\n",
      "Epoch 17/200 | Train Loss=0.9158 Acc=0.5139 F1=0.3458 || Val Loss=0.9766 Acc=0.4844 F1=0.3415\n",
      "Epoch 18/200 | Train Loss=0.9098 Acc=0.5377 F1=0.3663 || Val Loss=0.9751 Acc=0.4844 F1=0.3408\n",
      "Epoch 19/200 | Train Loss=0.9076 Acc=0.5317 F1=0.3550 || Val Loss=0.9841 Acc=0.5156 F1=0.3616\n",
      "Epoch 20/200 | Train Loss=0.9041 Acc=0.5357 F1=0.3682 || Val Loss=0.9968 Acc=0.4688 F1=0.3273\n",
      "Epoch 21/200 | Train Loss=0.9082 Acc=0.5437 F1=0.3784 || Val Loss=1.0050 Acc=0.4531 F1=0.3195\n",
      "Epoch 22/200 | Train Loss=0.9034 Acc=0.5476 F1=0.3645 || Val Loss=0.9977 Acc=0.5625 F1=0.3893\n",
      "Epoch 23/200 | Train Loss=0.9031 Acc=0.5337 F1=0.3514 || Val Loss=1.0029 Acc=0.4688 F1=0.3246\n",
      "Epoch 24/200 | Train Loss=0.8956 Acc=0.5516 F1=0.3794 || Val Loss=1.0262 Acc=0.4531 F1=0.3172\n",
      "Epoch 25/200 | Train Loss=0.8966 Acc=0.5556 F1=0.3858 || Val Loss=1.0286 Acc=0.5000 F1=0.3523\n",
      "Epoch 26/200 | Train Loss=0.8924 Acc=0.5417 F1=0.3623 || Val Loss=1.0117 Acc=0.5625 F1=0.3816\n",
      "Epoch 27/200 | Train Loss=0.8971 Acc=0.5377 F1=0.3559 || Val Loss=1.0203 Acc=0.5156 F1=0.3636\n",
      "Epoch 28/200 | Train Loss=0.8911 Acc=0.5317 F1=0.3668 || Val Loss=1.0454 Acc=0.4375 F1=0.3051\n",
      "Epoch 29/200 | Train Loss=0.8865 Acc=0.5595 F1=0.3855 || Val Loss=1.0430 Acc=0.5156 F1=0.3596\n",
      "Epoch 30/200 | Train Loss=0.8858 Acc=0.5476 F1=0.3685 || Val Loss=1.0335 Acc=0.5156 F1=0.3616\n",
      "Epoch 31/200 | Train Loss=0.8809 Acc=0.5456 F1=0.3670 || Val Loss=1.0274 Acc=0.4844 F1=0.3415\n",
      "Epoch 32/200 | Train Loss=0.8798 Acc=0.5536 F1=0.3802 || Val Loss=1.0564 Acc=0.5000 F1=0.3513\n",
      "Epoch 33/200 | Train Loss=0.8848 Acc=0.5397 F1=0.3681 || Val Loss=1.0514 Acc=0.5000 F1=0.3438\n",
      "Epoch 34/200 | Train Loss=0.8796 Acc=0.5476 F1=0.3691 || Val Loss=1.0524 Acc=0.4688 F1=0.3302\n",
      "Epoch 35/200 | Train Loss=0.8704 Acc=0.5575 F1=0.3789 || Val Loss=1.0717 Acc=0.4531 F1=0.3148\n",
      "Epoch 36/200 | Train Loss=0.8774 Acc=0.5734 F1=0.4043 || Val Loss=1.0627 Acc=0.4688 F1=0.3291\n",
      "Epoch 37/200 | Train Loss=0.8749 Acc=0.5575 F1=0.3750 || Val Loss=1.1044 Acc=0.4375 F1=0.2938\n",
      "Epoch 38/200 | Train Loss=0.8641 Acc=0.5575 F1=0.3823 || Val Loss=1.0807 Acc=0.4062 F1=0.2848\n",
      "Epoch 39/200 | Train Loss=0.8630 Acc=0.5933 F1=0.4056 || Val Loss=1.0774 Acc=0.4844 F1=0.3259\n",
      "Epoch 40/200 | Train Loss=0.8628 Acc=0.5694 F1=0.3845 || Val Loss=1.1605 Acc=0.4375 F1=0.3021\n",
      "Epoch 41/200 | Train Loss=0.8565 Acc=0.5893 F1=0.4082 || Val Loss=1.0733 Acc=0.4219 F1=0.2974\n",
      "Epoch 42/200 | Train Loss=0.8529 Acc=0.5833 F1=0.4021 || Val Loss=1.0943 Acc=0.4531 F1=0.3027\n",
      "[INFO] Early stopping triggered at epoch 42\n",
      "[RESULT] Fold 12: Test Loss=0.9910 Acc=0.4262 F1=0.2973\n",
      "\n",
      "[INFO] Starting Fold 13/36\n",
      "Epoch 1/200 | Train Loss=1.0871 Acc=0.3386 F1=0.2767 || Val Loss=1.2004 Acc=0.2419 F1=0.1967\n",
      "Epoch 2/200 | Train Loss=1.0712 Acc=0.4084 F1=0.2454 || Val Loss=1.1975 Acc=0.2581 F1=0.2104\n",
      "Epoch 3/200 | Train Loss=1.0587 Acc=0.4163 F1=0.3061 || Val Loss=1.1850 Acc=0.2419 F1=0.1790\n",
      "Epoch 4/200 | Train Loss=1.0566 Acc=0.4602 F1=0.3506 || Val Loss=1.1753 Acc=0.2903 F1=0.2288\n",
      "Epoch 5/200 | Train Loss=1.0555 Acc=0.4343 F1=0.3225 || Val Loss=1.1841 Acc=0.2903 F1=0.2313\n",
      "Epoch 6/200 | Train Loss=1.0434 Acc=0.4462 F1=0.3308 || Val Loss=1.2000 Acc=0.2903 F1=0.2313\n",
      "Epoch 7/200 | Train Loss=1.0398 Acc=0.4821 F1=0.3684 || Val Loss=1.2210 Acc=0.2581 F1=0.1984\n",
      "Epoch 8/200 | Train Loss=1.0396 Acc=0.4522 F1=0.3708 || Val Loss=1.2316 Acc=0.2903 F1=0.2857\n",
      "Epoch 9/200 | Train Loss=1.0326 Acc=0.4681 F1=0.3988 || Val Loss=1.2409 Acc=0.3065 F1=0.3078\n",
      "Epoch 10/200 | Train Loss=1.0282 Acc=0.4960 F1=0.4251 || Val Loss=1.2520 Acc=0.3387 F1=0.3483\n",
      "Epoch 11/200 | Train Loss=1.0253 Acc=0.4721 F1=0.3959 || Val Loss=1.2783 Acc=0.3226 F1=0.3321\n",
      "Epoch 12/200 | Train Loss=1.0181 Acc=0.4960 F1=0.4282 || Val Loss=1.3145 Acc=0.3065 F1=0.3087\n",
      "Epoch 13/200 | Train Loss=1.0199 Acc=0.5060 F1=0.4543 || Val Loss=1.3427 Acc=0.2903 F1=0.2889\n",
      "Epoch 14/200 | Train Loss=1.0169 Acc=0.4741 F1=0.4327 || Val Loss=1.3607 Acc=0.2742 F1=0.2719\n",
      "Epoch 15/200 | Train Loss=1.0188 Acc=0.4940 F1=0.4532 || Val Loss=1.3643 Acc=0.2742 F1=0.2719\n",
      "Epoch 16/200 | Train Loss=1.0136 Acc=0.4920 F1=0.4437 || Val Loss=1.3736 Acc=0.2419 F1=0.2128\n",
      "Epoch 17/200 | Train Loss=1.0127 Acc=0.4900 F1=0.4353 || Val Loss=1.3751 Acc=0.2742 F1=0.2692\n",
      "Epoch 18/200 | Train Loss=1.0106 Acc=0.4841 F1=0.4366 || Val Loss=1.3989 Acc=0.2742 F1=0.2564\n",
      "Epoch 19/200 | Train Loss=1.0077 Acc=0.4920 F1=0.4496 || Val Loss=1.3920 Acc=0.2742 F1=0.2313\n",
      "Epoch 20/200 | Train Loss=1.0006 Acc=0.4880 F1=0.4442 || Val Loss=1.3695 Acc=0.3065 F1=0.2772\n",
      "Epoch 21/200 | Train Loss=0.9937 Acc=0.4801 F1=0.4232 || Val Loss=1.3767 Acc=0.2742 F1=0.2443\n",
      "Epoch 22/200 | Train Loss=0.9966 Acc=0.4960 F1=0.4417 || Val Loss=1.3947 Acc=0.2742 F1=0.2456\n",
      "Epoch 23/200 | Train Loss=0.9768 Acc=0.5100 F1=0.4760 || Val Loss=1.3963 Acc=0.2581 F1=0.2148\n",
      "Epoch 24/200 | Train Loss=0.9881 Acc=0.5159 F1=0.4618 || Val Loss=1.3937 Acc=0.2903 F1=0.2414\n",
      "Epoch 25/200 | Train Loss=0.9843 Acc=0.5000 F1=0.4504 || Val Loss=1.4201 Acc=0.3065 F1=0.2186\n",
      "Epoch 26/200 | Train Loss=0.9744 Acc=0.5120 F1=0.4863 || Val Loss=1.4705 Acc=0.2742 F1=0.2478\n",
      "Epoch 27/200 | Train Loss=0.9712 Acc=0.5219 F1=0.4809 || Val Loss=1.4751 Acc=0.3065 F1=0.2519\n",
      "Epoch 28/200 | Train Loss=0.9675 Acc=0.5339 F1=0.4821 || Val Loss=1.4720 Acc=0.3065 F1=0.2519\n",
      "Epoch 29/200 | Train Loss=0.9514 Acc=0.5359 F1=0.4964 || Val Loss=1.4727 Acc=0.3065 F1=0.2445\n",
      "Epoch 30/200 | Train Loss=0.9539 Acc=0.5139 F1=0.4754 || Val Loss=1.4634 Acc=0.3387 F1=0.3161\n",
      "[INFO] Early stopping triggered at epoch 30\n",
      "[RESULT] Fold 13: Test Loss=1.2145 Acc=0.3548 F1=0.3086\n",
      "\n",
      "[INFO] Starting Fold 14/36\n",
      "Epoch 1/200 | Train Loss=1.0784 Acc=0.3992 F1=0.2483 || Val Loss=1.0223 Acc=0.4677 F1=0.2730\n",
      "Epoch 2/200 | Train Loss=1.0527 Acc=0.4371 F1=0.2434 || Val Loss=1.0308 Acc=0.4516 F1=0.2074\n",
      "Epoch 3/200 | Train Loss=1.0540 Acc=0.4371 F1=0.2028 || Val Loss=1.0291 Acc=0.4516 F1=0.2074\n",
      "Epoch 4/200 | Train Loss=1.0516 Acc=0.4391 F1=0.2136 || Val Loss=1.0243 Acc=0.4355 F1=0.2239\n",
      "Epoch 5/200 | Train Loss=1.0468 Acc=0.4631 F1=0.2746 || Val Loss=1.0243 Acc=0.4677 F1=0.2729\n",
      "Epoch 6/200 | Train Loss=1.0495 Acc=0.4531 F1=0.2716 || Val Loss=1.0261 Acc=0.4516 F1=0.2651\n",
      "Epoch 7/200 | Train Loss=1.0449 Acc=0.4631 F1=0.2937 || Val Loss=1.0282 Acc=0.4194 F1=0.2719\n",
      "Epoch 8/200 | Train Loss=1.0395 Acc=0.4611 F1=0.3155 || Val Loss=1.0306 Acc=0.4032 F1=0.2632\n",
      "Epoch 9/200 | Train Loss=1.0376 Acc=0.4671 F1=0.3070 || Val Loss=1.0355 Acc=0.4355 F1=0.2809\n",
      "Epoch 10/200 | Train Loss=1.0361 Acc=0.4691 F1=0.3120 || Val Loss=1.0471 Acc=0.4032 F1=0.2775\n",
      "Epoch 11/200 | Train Loss=1.0429 Acc=0.4691 F1=0.3297 || Val Loss=1.0572 Acc=0.3871 F1=0.2790\n",
      "Epoch 12/200 | Train Loss=1.0362 Acc=0.4571 F1=0.3176 || Val Loss=1.0438 Acc=0.4194 F1=0.2719\n",
      "Epoch 13/200 | Train Loss=1.0346 Acc=0.4451 F1=0.2947 || Val Loss=1.0436 Acc=0.4032 F1=0.2826\n",
      "Epoch 14/200 | Train Loss=1.0294 Acc=0.4531 F1=0.3133 || Val Loss=1.0488 Acc=0.3871 F1=0.2767\n",
      "Epoch 15/200 | Train Loss=1.0227 Acc=0.4731 F1=0.3345 || Val Loss=1.0545 Acc=0.4194 F1=0.2967\n",
      "Epoch 16/200 | Train Loss=1.0247 Acc=0.4471 F1=0.3033 || Val Loss=1.0580 Acc=0.4355 F1=0.3068\n",
      "Epoch 17/200 | Train Loss=1.0237 Acc=0.4671 F1=0.3314 || Val Loss=1.0705 Acc=0.4032 F1=0.2915\n",
      "Epoch 18/200 | Train Loss=1.0218 Acc=0.4731 F1=0.3464 || Val Loss=1.0826 Acc=0.4194 F1=0.2968\n",
      "Epoch 19/200 | Train Loss=1.0261 Acc=0.4691 F1=0.3096 || Val Loss=1.0843 Acc=0.4355 F1=0.2421\n",
      "Epoch 20/200 | Train Loss=1.0197 Acc=0.4731 F1=0.3135 || Val Loss=1.0359 Acc=0.4355 F1=0.3148\n",
      "Epoch 21/200 | Train Loss=1.0111 Acc=0.4910 F1=0.3591 || Val Loss=1.0721 Acc=0.3548 F1=0.2576\n",
      "Epoch 22/200 | Train Loss=1.0116 Acc=0.4930 F1=0.3573 || Val Loss=1.0913 Acc=0.4032 F1=0.2915\n",
      "Epoch 23/200 | Train Loss=1.0005 Acc=0.4950 F1=0.3604 || Val Loss=1.0641 Acc=0.3871 F1=0.2807\n",
      "Epoch 24/200 | Train Loss=1.0056 Acc=0.4950 F1=0.3650 || Val Loss=1.1335 Acc=0.3710 F1=0.2690\n",
      "Epoch 25/200 | Train Loss=0.9945 Acc=0.4910 F1=0.3582 || Val Loss=1.0908 Acc=0.4355 F1=0.3136\n",
      "Epoch 26/200 | Train Loss=0.9946 Acc=0.4910 F1=0.3588 || Val Loss=1.1134 Acc=0.4032 F1=0.2915\n",
      "Epoch 27/200 | Train Loss=0.9775 Acc=0.5130 F1=0.3818 || Val Loss=1.1386 Acc=0.3871 F1=0.2762\n",
      "Epoch 28/200 | Train Loss=0.9779 Acc=0.5130 F1=0.3785 || Val Loss=1.1430 Acc=0.3710 F1=0.2687\n",
      "Epoch 29/200 | Train Loss=0.9747 Acc=0.5090 F1=0.3776 || Val Loss=1.1546 Acc=0.4032 F1=0.2828\n",
      "Epoch 30/200 | Train Loss=0.9771 Acc=0.5030 F1=0.3764 || Val Loss=1.1176 Acc=0.4032 F1=0.2916\n",
      "Epoch 31/200 | Train Loss=0.9665 Acc=0.5070 F1=0.3770 || Val Loss=1.1619 Acc=0.4194 F1=0.2808\n",
      "Epoch 32/200 | Train Loss=0.9532 Acc=0.5250 F1=0.3857 || Val Loss=1.1670 Acc=0.4032 F1=0.2926\n",
      "Epoch 33/200 | Train Loss=0.9504 Acc=0.5190 F1=0.3886 || Val Loss=1.1913 Acc=0.3871 F1=0.2794\n",
      "Epoch 34/200 | Train Loss=0.9392 Acc=0.5469 F1=0.4389 || Val Loss=1.2159 Acc=0.4032 F1=0.2897\n",
      "Epoch 35/200 | Train Loss=0.9526 Acc=0.5170 F1=0.3977 || Val Loss=1.1775 Acc=0.4194 F1=0.3045\n",
      "Epoch 36/200 | Train Loss=0.9404 Acc=0.5210 F1=0.4067 || Val Loss=1.2225 Acc=0.4516 F1=0.3656\n",
      "Epoch 37/200 | Train Loss=0.9178 Acc=0.5409 F1=0.4448 || Val Loss=1.2722 Acc=0.4194 F1=0.3546\n",
      "Epoch 38/200 | Train Loss=0.9016 Acc=0.5409 F1=0.4432 || Val Loss=1.2982 Acc=0.4677 F1=0.3805\n",
      "Epoch 39/200 | Train Loss=0.9086 Acc=0.5429 F1=0.4427 || Val Loss=1.3213 Acc=0.4355 F1=0.3637\n",
      "Epoch 40/200 | Train Loss=0.9140 Acc=0.5230 F1=0.4373 || Val Loss=1.3599 Acc=0.4194 F1=0.3557\n",
      "Epoch 41/200 | Train Loss=0.9071 Acc=0.5509 F1=0.4628 || Val Loss=1.2695 Acc=0.4355 F1=0.3608\n",
      "Epoch 42/200 | Train Loss=0.9092 Acc=0.5609 F1=0.4792 || Val Loss=1.3696 Acc=0.4032 F1=0.2901\n",
      "Epoch 43/200 | Train Loss=0.8781 Acc=0.5609 F1=0.4762 || Val Loss=1.3689 Acc=0.3871 F1=0.3078\n",
      "Epoch 44/200 | Train Loss=0.8954 Acc=0.5709 F1=0.5112 || Val Loss=1.3225 Acc=0.3710 F1=0.2866\n",
      "Epoch 45/200 | Train Loss=0.8965 Acc=0.5629 F1=0.4832 || Val Loss=1.4605 Acc=0.3710 F1=0.2667\n",
      "Epoch 46/200 | Train Loss=0.8634 Acc=0.5808 F1=0.4961 || Val Loss=1.3404 Acc=0.3710 F1=0.2915\n",
      "Epoch 47/200 | Train Loss=0.8698 Acc=0.5589 F1=0.4981 || Val Loss=1.4710 Acc=0.4194 F1=0.3449\n",
      "Epoch 48/200 | Train Loss=0.8659 Acc=0.5569 F1=0.4669 || Val Loss=1.4003 Acc=0.4194 F1=0.3358\n",
      "Epoch 49/200 | Train Loss=0.8518 Acc=0.5808 F1=0.4958 || Val Loss=1.5390 Acc=0.3710 F1=0.2951\n",
      "Epoch 50/200 | Train Loss=0.8300 Acc=0.5788 F1=0.5247 || Val Loss=1.4903 Acc=0.3226 F1=0.2362\n",
      "Epoch 51/200 | Train Loss=0.8364 Acc=0.5788 F1=0.5294 || Val Loss=1.5175 Acc=0.3871 F1=0.3102\n",
      "Epoch 52/200 | Train Loss=0.8325 Acc=0.5848 F1=0.5294 || Val Loss=1.5229 Acc=0.4032 F1=0.2833\n",
      "Epoch 53/200 | Train Loss=0.8200 Acc=0.5948 F1=0.5488 || Val Loss=1.6289 Acc=0.4032 F1=0.3124\n",
      "Epoch 54/200 | Train Loss=0.8156 Acc=0.5968 F1=0.5547 || Val Loss=1.6468 Acc=0.3710 F1=0.3012\n",
      "Epoch 55/200 | Train Loss=0.8234 Acc=0.5948 F1=0.5298 || Val Loss=1.5106 Acc=0.4032 F1=0.2793\n",
      "Epoch 56/200 | Train Loss=0.8140 Acc=0.5888 F1=0.5313 || Val Loss=1.7716 Acc=0.3710 F1=0.3045\n",
      "Epoch 57/200 | Train Loss=0.8023 Acc=0.5948 F1=0.5585 || Val Loss=1.4589 Acc=0.4032 F1=0.2910\n",
      "Epoch 58/200 | Train Loss=0.8161 Acc=0.6108 F1=0.5601 || Val Loss=1.5047 Acc=0.3871 F1=0.2688\n",
      "[INFO] Early stopping triggered at epoch 58\n",
      "[RESULT] Fold 14: Test Loss=1.3176 Acc=0.3594 F1=0.2483\n",
      "\n",
      "[INFO] Starting Fold 15/36\n",
      "Epoch 1/200 | Train Loss=1.0626 Acc=0.4345 F1=0.2274 || Val Loss=1.0108 Acc=0.3906 F1=0.2047\n",
      "Epoch 2/200 | Train Loss=1.0563 Acc=0.4266 F1=0.2145 || Val Loss=1.0095 Acc=0.5312 F1=0.3814\n",
      "Epoch 3/200 | Train Loss=1.0447 Acc=0.4464 F1=0.2723 || Val Loss=1.0145 Acc=0.5312 F1=0.3826\n",
      "Epoch 4/200 | Train Loss=1.0431 Acc=0.4544 F1=0.2763 || Val Loss=1.0155 Acc=0.4219 F1=0.3007\n",
      "Epoch 5/200 | Train Loss=1.0415 Acc=0.4623 F1=0.2748 || Val Loss=1.0148 Acc=0.4062 F1=0.2403\n",
      "Epoch 6/200 | Train Loss=1.0420 Acc=0.4385 F1=0.2403 || Val Loss=1.0187 Acc=0.4219 F1=0.2591\n",
      "Epoch 7/200 | Train Loss=1.0339 Acc=0.4683 F1=0.2835 || Val Loss=1.0193 Acc=0.3906 F1=0.2597\n",
      "Epoch 8/200 | Train Loss=1.0315 Acc=0.4603 F1=0.2738 || Val Loss=1.0256 Acc=0.3906 F1=0.2597\n",
      "Epoch 9/200 | Train Loss=1.0329 Acc=0.4643 F1=0.2813 || Val Loss=1.0225 Acc=0.3750 F1=0.2513\n",
      "Epoch 10/200 | Train Loss=1.0284 Acc=0.4742 F1=0.3083 || Val Loss=1.0200 Acc=0.3750 F1=0.2647\n",
      "Epoch 11/200 | Train Loss=1.0266 Acc=0.4742 F1=0.3204 || Val Loss=1.0236 Acc=0.3750 F1=0.2614\n",
      "Epoch 12/200 | Train Loss=1.0277 Acc=0.4702 F1=0.3035 || Val Loss=1.0284 Acc=0.3594 F1=0.2425\n",
      "Epoch 13/200 | Train Loss=1.0227 Acc=0.4464 F1=0.2832 || Val Loss=1.0308 Acc=0.3750 F1=0.2614\n",
      "Epoch 14/200 | Train Loss=1.0250 Acc=0.4524 F1=0.3085 || Val Loss=1.0386 Acc=0.3750 F1=0.2614\n",
      "Epoch 15/200 | Train Loss=1.0207 Acc=0.4583 F1=0.3141 || Val Loss=1.0387 Acc=0.3594 F1=0.2518\n",
      "Epoch 16/200 | Train Loss=1.0184 Acc=0.4683 F1=0.3198 || Val Loss=1.0374 Acc=0.3281 F1=0.2290\n",
      "Epoch 17/200 | Train Loss=1.0162 Acc=0.4702 F1=0.3255 || Val Loss=1.0518 Acc=0.3438 F1=0.2272\n",
      "Epoch 18/200 | Train Loss=1.0080 Acc=0.4802 F1=0.3175 || Val Loss=1.0613 Acc=0.3438 F1=0.2340\n",
      "Epoch 19/200 | Train Loss=1.0094 Acc=0.4782 F1=0.3507 || Val Loss=1.0590 Acc=0.3594 F1=0.2547\n",
      "Epoch 20/200 | Train Loss=1.0029 Acc=0.4881 F1=0.3632 || Val Loss=1.0552 Acc=0.3594 F1=0.2476\n",
      "Epoch 21/200 | Train Loss=1.0004 Acc=0.4881 F1=0.3589 || Val Loss=1.0587 Acc=0.3750 F1=0.2647\n",
      "Epoch 22/200 | Train Loss=0.9979 Acc=0.4821 F1=0.3491 || Val Loss=1.0517 Acc=0.3594 F1=0.2518\n",
      "Epoch 23/200 | Train Loss=0.9951 Acc=0.5000 F1=0.3762 || Val Loss=1.0506 Acc=0.3281 F1=0.2321\n",
      "[INFO] Early stopping triggered at epoch 23\n",
      "[RESULT] Fold 15: Test Loss=0.9534 Acc=0.5410 F1=0.3629\n",
      "\n",
      "[INFO] Starting Fold 16/36\n",
      "Epoch 1/200 | Train Loss=1.0585 Acc=0.3625 F1=0.3036 || Val Loss=1.2355 Acc=0.2742 F1=0.2493\n",
      "Epoch 2/200 | Train Loss=1.0350 Acc=0.4183 F1=0.2573 || Val Loss=1.2235 Acc=0.3065 F1=0.2182\n",
      "Epoch 3/200 | Train Loss=1.0278 Acc=0.4402 F1=0.3260 || Val Loss=1.1503 Acc=0.3065 F1=0.2905\n",
      "Epoch 4/200 | Train Loss=1.0217 Acc=0.4462 F1=0.3451 || Val Loss=1.1629 Acc=0.3226 F1=0.2755\n",
      "Epoch 5/200 | Train Loss=1.0150 Acc=0.4661 F1=0.3374 || Val Loss=1.2012 Acc=0.3065 F1=0.2566\n",
      "Epoch 6/200 | Train Loss=1.0103 Acc=0.4602 F1=0.3449 || Val Loss=1.2002 Acc=0.2903 F1=0.2459\n",
      "Epoch 7/200 | Train Loss=1.0111 Acc=0.4602 F1=0.3301 || Val Loss=1.1806 Acc=0.3065 F1=0.2401\n",
      "Epoch 8/200 | Train Loss=1.0066 Acc=0.4681 F1=0.3684 || Val Loss=1.1610 Acc=0.3548 F1=0.3069\n",
      "Epoch 9/200 | Train Loss=1.0027 Acc=0.4622 F1=0.3689 || Val Loss=1.1690 Acc=0.3226 F1=0.2824\n",
      "Epoch 10/200 | Train Loss=0.9981 Acc=0.4900 F1=0.3778 || Val Loss=1.1791 Acc=0.3548 F1=0.3014\n",
      "Epoch 11/200 | Train Loss=0.9973 Acc=0.4880 F1=0.3819 || Val Loss=1.1770 Acc=0.3548 F1=0.3055\n",
      "Epoch 12/200 | Train Loss=0.9904 Acc=0.5040 F1=0.3992 || Val Loss=1.1664 Acc=0.3710 F1=0.3206\n",
      "Epoch 13/200 | Train Loss=0.9876 Acc=0.5100 F1=0.4147 || Val Loss=1.1682 Acc=0.3387 F1=0.2851\n",
      "Epoch 14/200 | Train Loss=0.9801 Acc=0.5159 F1=0.4140 || Val Loss=1.1645 Acc=0.3387 F1=0.2963\n",
      "Epoch 15/200 | Train Loss=0.9795 Acc=0.5020 F1=0.4068 || Val Loss=1.1738 Acc=0.2903 F1=0.2555\n",
      "Epoch 16/200 | Train Loss=0.9791 Acc=0.5139 F1=0.4206 || Val Loss=1.1755 Acc=0.3065 F1=0.2684\n",
      "Epoch 17/200 | Train Loss=0.9771 Acc=0.5219 F1=0.4324 || Val Loss=1.1648 Acc=0.3548 F1=0.3134\n",
      "Epoch 18/200 | Train Loss=0.9718 Acc=0.5100 F1=0.4206 || Val Loss=1.1606 Acc=0.3387 F1=0.3042\n",
      "Epoch 19/200 | Train Loss=0.9798 Acc=0.5219 F1=0.4165 || Val Loss=1.1834 Acc=0.3065 F1=0.2669\n",
      "Epoch 20/200 | Train Loss=0.9645 Acc=0.5299 F1=0.4340 || Val Loss=1.1542 Acc=0.2903 F1=0.2565\n",
      "Epoch 21/200 | Train Loss=0.9737 Acc=0.5219 F1=0.4404 || Val Loss=1.1535 Acc=0.3710 F1=0.3442\n",
      "Epoch 22/200 | Train Loss=0.9633 Acc=0.5299 F1=0.4480 || Val Loss=1.1680 Acc=0.3387 F1=0.2799\n",
      "Epoch 23/200 | Train Loss=0.9534 Acc=0.5458 F1=0.4505 || Val Loss=1.1789 Acc=0.3065 F1=0.2622\n",
      "Epoch 24/200 | Train Loss=0.9570 Acc=0.5339 F1=0.4527 || Val Loss=1.1750 Acc=0.3226 F1=0.2843\n",
      "Epoch 25/200 | Train Loss=0.9533 Acc=0.5478 F1=0.4485 || Val Loss=1.1626 Acc=0.3387 F1=0.3009\n",
      "Epoch 26/200 | Train Loss=0.9571 Acc=0.5418 F1=0.4531 || Val Loss=1.1648 Acc=0.3387 F1=0.3093\n",
      "Epoch 27/200 | Train Loss=0.9463 Acc=0.5418 F1=0.4448 || Val Loss=1.1705 Acc=0.3387 F1=0.2831\n",
      "Epoch 28/200 | Train Loss=0.9491 Acc=0.5598 F1=0.4743 || Val Loss=1.1827 Acc=0.3226 F1=0.2884\n",
      "Epoch 29/200 | Train Loss=0.9491 Acc=0.5618 F1=0.4780 || Val Loss=1.1983 Acc=0.3065 F1=0.2703\n",
      "Epoch 30/200 | Train Loss=0.9434 Acc=0.5637 F1=0.4789 || Val Loss=1.1678 Acc=0.3710 F1=0.3517\n",
      "Epoch 31/200 | Train Loss=0.9272 Acc=0.5717 F1=0.4896 || Val Loss=1.1732 Acc=0.3387 F1=0.2970\n",
      "Epoch 32/200 | Train Loss=0.9359 Acc=0.5737 F1=0.4803 || Val Loss=1.2045 Acc=0.3387 F1=0.2988\n",
      "Epoch 33/200 | Train Loss=0.9344 Acc=0.5538 F1=0.4500 || Val Loss=1.1867 Acc=0.3871 F1=0.3515\n",
      "Epoch 34/200 | Train Loss=0.9189 Acc=0.5657 F1=0.4924 || Val Loss=1.1616 Acc=0.4194 F1=0.3891\n",
      "Epoch 35/200 | Train Loss=0.9251 Acc=0.5717 F1=0.4806 || Val Loss=1.2087 Acc=0.3710 F1=0.3403\n",
      "Epoch 36/200 | Train Loss=0.9217 Acc=0.5757 F1=0.4794 || Val Loss=1.1973 Acc=0.3548 F1=0.3094\n",
      "Epoch 37/200 | Train Loss=0.9143 Acc=0.5657 F1=0.4691 || Val Loss=1.2195 Acc=0.3710 F1=0.3375\n",
      "Epoch 38/200 | Train Loss=0.8991 Acc=0.5677 F1=0.4695 || Val Loss=1.2050 Acc=0.3548 F1=0.3211\n",
      "Epoch 39/200 | Train Loss=0.8978 Acc=0.5817 F1=0.5069 || Val Loss=1.1864 Acc=0.3710 F1=0.3579\n",
      "Epoch 40/200 | Train Loss=0.8869 Acc=0.5777 F1=0.5027 || Val Loss=1.2608 Acc=0.3548 F1=0.3074\n",
      "Epoch 41/200 | Train Loss=0.8862 Acc=0.5717 F1=0.4873 || Val Loss=1.2052 Acc=0.4032 F1=0.3854\n",
      "Epoch 42/200 | Train Loss=0.8811 Acc=0.5976 F1=0.5257 || Val Loss=1.2221 Acc=0.3710 F1=0.3499\n",
      "Epoch 43/200 | Train Loss=0.8788 Acc=0.5697 F1=0.5103 || Val Loss=1.2598 Acc=0.3871 F1=0.3606\n",
      "Epoch 44/200 | Train Loss=0.8998 Acc=0.5697 F1=0.5030 || Val Loss=1.2089 Acc=0.4194 F1=0.4171\n",
      "Epoch 45/200 | Train Loss=0.8903 Acc=0.5757 F1=0.4883 || Val Loss=1.2641 Acc=0.3871 F1=0.3548\n",
      "Epoch 46/200 | Train Loss=0.8725 Acc=0.5857 F1=0.5180 || Val Loss=1.1955 Acc=0.4032 F1=0.3893\n",
      "Epoch 47/200 | Train Loss=0.8582 Acc=0.6215 F1=0.5557 || Val Loss=1.2905 Acc=0.4032 F1=0.3637\n",
      "Epoch 48/200 | Train Loss=0.8415 Acc=0.6155 F1=0.5360 || Val Loss=1.2305 Acc=0.4516 F1=0.4395\n",
      "Epoch 49/200 | Train Loss=0.8613 Acc=0.6036 F1=0.5327 || Val Loss=1.2085 Acc=0.4032 F1=0.3673\n",
      "Epoch 50/200 | Train Loss=0.8482 Acc=0.6135 F1=0.5475 || Val Loss=1.3348 Acc=0.3710 F1=0.3288\n",
      "Epoch 51/200 | Train Loss=0.8416 Acc=0.6036 F1=0.5255 || Val Loss=1.1818 Acc=0.4355 F1=0.4036\n",
      "Epoch 52/200 | Train Loss=0.8290 Acc=0.6295 F1=0.5729 || Val Loss=1.2801 Acc=0.3871 F1=0.3510\n",
      "Epoch 53/200 | Train Loss=0.8258 Acc=0.6036 F1=0.5239 || Val Loss=1.3110 Acc=0.4032 F1=0.3678\n",
      "Epoch 54/200 | Train Loss=0.8230 Acc=0.6375 F1=0.5762 || Val Loss=1.2130 Acc=0.4516 F1=0.4341\n",
      "Epoch 55/200 | Train Loss=0.8321 Acc=0.6155 F1=0.5667 || Val Loss=1.2504 Acc=0.4355 F1=0.4171\n",
      "Epoch 56/200 | Train Loss=0.8130 Acc=0.6175 F1=0.5529 || Val Loss=1.2942 Acc=0.3871 F1=0.3726\n",
      "Epoch 57/200 | Train Loss=0.8101 Acc=0.6315 F1=0.5736 || Val Loss=1.2760 Acc=0.4516 F1=0.4298\n",
      "Epoch 58/200 | Train Loss=0.7696 Acc=0.6653 F1=0.6143 || Val Loss=1.3357 Acc=0.4355 F1=0.4157\n",
      "Epoch 59/200 | Train Loss=0.7789 Acc=0.6355 F1=0.5788 || Val Loss=1.3640 Acc=0.4194 F1=0.3967\n",
      "Epoch 60/200 | Train Loss=0.7543 Acc=0.6633 F1=0.5972 || Val Loss=1.3298 Acc=0.4355 F1=0.4169\n",
      "Epoch 61/200 | Train Loss=0.7815 Acc=0.6454 F1=0.5877 || Val Loss=1.3303 Acc=0.4516 F1=0.4408\n",
      "Epoch 62/200 | Train Loss=0.7808 Acc=0.6255 F1=0.5688 || Val Loss=1.4008 Acc=0.4355 F1=0.4185\n",
      "Epoch 63/200 | Train Loss=0.7841 Acc=0.6315 F1=0.5715 || Val Loss=1.3197 Acc=0.4516 F1=0.4362\n",
      "Epoch 64/200 | Train Loss=0.7639 Acc=0.6614 F1=0.6052 || Val Loss=1.3518 Acc=0.4355 F1=0.4157\n",
      "Epoch 65/200 | Train Loss=0.7806 Acc=0.6494 F1=0.5748 || Val Loss=1.3891 Acc=0.4194 F1=0.3947\n",
      "Epoch 66/200 | Train Loss=0.7472 Acc=0.6594 F1=0.6043 || Val Loss=1.3886 Acc=0.3710 F1=0.3309\n",
      "Epoch 67/200 | Train Loss=0.7299 Acc=0.6673 F1=0.6067 || Val Loss=1.4124 Acc=0.4032 F1=0.3891\n",
      "Epoch 68/200 | Train Loss=0.7253 Acc=0.6673 F1=0.6056 || Val Loss=1.5285 Acc=0.3710 F1=0.3326\n",
      "Epoch 69/200 | Train Loss=0.7277 Acc=0.6574 F1=0.6030 || Val Loss=1.3792 Acc=0.4194 F1=0.3946\n",
      "Epoch 70/200 | Train Loss=0.7286 Acc=0.6773 F1=0.6205 || Val Loss=1.5002 Acc=0.4032 F1=0.3758\n",
      "Epoch 71/200 | Train Loss=0.6864 Acc=0.6813 F1=0.6250 || Val Loss=1.4692 Acc=0.4194 F1=0.3880\n",
      "Epoch 72/200 | Train Loss=0.7111 Acc=0.6514 F1=0.6002 || Val Loss=1.3812 Acc=0.3871 F1=0.3769\n",
      "Epoch 73/200 | Train Loss=0.6989 Acc=0.6693 F1=0.6067 || Val Loss=1.4872 Acc=0.4032 F1=0.3657\n",
      "Epoch 74/200 | Train Loss=0.7128 Acc=0.6793 F1=0.6244 || Val Loss=1.3687 Acc=0.4032 F1=0.3879\n",
      "Epoch 75/200 | Train Loss=0.7092 Acc=0.6813 F1=0.6372 || Val Loss=1.4171 Acc=0.4032 F1=0.3886\n",
      "Epoch 76/200 | Train Loss=0.7039 Acc=0.6713 F1=0.6080 || Val Loss=1.4367 Acc=0.4194 F1=0.3946\n",
      "Epoch 77/200 | Train Loss=0.7006 Acc=0.6912 F1=0.6425 || Val Loss=1.5299 Acc=0.4194 F1=0.3947\n",
      "Epoch 78/200 | Train Loss=0.6859 Acc=0.6912 F1=0.6238 || Val Loss=1.4286 Acc=0.4032 F1=0.3758\n",
      "Epoch 79/200 | Train Loss=0.6747 Acc=0.6773 F1=0.6159 || Val Loss=1.4982 Acc=0.4194 F1=0.3955\n",
      "Epoch 80/200 | Train Loss=0.6672 Acc=0.6892 F1=0.6408 || Val Loss=1.4879 Acc=0.4194 F1=0.3946\n",
      "Epoch 81/200 | Train Loss=0.6954 Acc=0.6733 F1=0.6345 || Val Loss=1.4952 Acc=0.4194 F1=0.4016\n",
      "[INFO] Early stopping triggered at epoch 81\n",
      "[RESULT] Fold 16: Test Loss=1.3587 Acc=0.3548 F1=0.3224\n",
      "\n",
      "[INFO] Starting Fold 17/36\n",
      "Epoch 1/200 | Train Loss=1.0952 Acc=0.3832 F1=0.2805 || Val Loss=1.0175 Acc=0.5161 F1=0.2270\n",
      "Epoch 2/200 | Train Loss=1.0522 Acc=0.4331 F1=0.2202 || Val Loss=1.0021 Acc=0.5645 F1=0.2719\n",
      "Epoch 3/200 | Train Loss=1.0467 Acc=0.4331 F1=0.2256 || Val Loss=1.0057 Acc=0.5484 F1=0.2658\n",
      "Epoch 4/200 | Train Loss=1.0394 Acc=0.4611 F1=0.2971 || Val Loss=1.0246 Acc=0.4194 F1=0.2902\n",
      "Epoch 5/200 | Train Loss=1.0378 Acc=0.4611 F1=0.3354 || Val Loss=1.0288 Acc=0.3710 F1=0.2661\n",
      "Epoch 6/200 | Train Loss=1.0241 Acc=0.4571 F1=0.3372 || Val Loss=1.0184 Acc=0.3548 F1=0.2540\n",
      "Epoch 7/200 | Train Loss=1.0219 Acc=0.4591 F1=0.3260 || Val Loss=1.0137 Acc=0.3871 F1=0.2660\n",
      "Epoch 8/200 | Train Loss=1.0147 Acc=0.4591 F1=0.3334 || Val Loss=1.0190 Acc=0.3710 F1=0.2672\n",
      "Epoch 9/200 | Train Loss=1.0098 Acc=0.4511 F1=0.3346 || Val Loss=1.0183 Acc=0.3548 F1=0.2546\n",
      "Epoch 10/200 | Train Loss=1.0159 Acc=0.4571 F1=0.3282 || Val Loss=1.0046 Acc=0.4516 F1=0.3029\n",
      "Epoch 11/200 | Train Loss=1.0027 Acc=0.4471 F1=0.3222 || Val Loss=1.0236 Acc=0.3871 F1=0.2785\n",
      "Epoch 12/200 | Train Loss=1.0036 Acc=0.4591 F1=0.3627 || Val Loss=1.0427 Acc=0.3871 F1=0.2769\n",
      "Epoch 13/200 | Train Loss=1.0051 Acc=0.4511 F1=0.3549 || Val Loss=1.0392 Acc=0.4032 F1=0.2889\n",
      "Epoch 14/200 | Train Loss=0.9997 Acc=0.4291 F1=0.3381 || Val Loss=1.0235 Acc=0.3710 F1=0.2637\n",
      "Epoch 15/200 | Train Loss=1.0019 Acc=0.4591 F1=0.3755 || Val Loss=1.0613 Acc=0.3871 F1=0.2705\n",
      "Epoch 16/200 | Train Loss=1.0025 Acc=0.4611 F1=0.3930 || Val Loss=1.0681 Acc=0.3871 F1=0.2705\n",
      "Epoch 17/200 | Train Loss=0.9931 Acc=0.4671 F1=0.4002 || Val Loss=1.0210 Acc=0.4194 F1=0.2975\n",
      "Epoch 18/200 | Train Loss=0.9878 Acc=0.4591 F1=0.3875 || Val Loss=1.0546 Acc=0.3871 F1=0.2703\n",
      "Epoch 19/200 | Train Loss=0.9908 Acc=0.4591 F1=0.3946 || Val Loss=1.0892 Acc=0.3710 F1=0.2504\n",
      "Epoch 20/200 | Train Loss=0.9875 Acc=0.4750 F1=0.4211 || Val Loss=1.0518 Acc=0.4032 F1=0.2899\n",
      "Epoch 21/200 | Train Loss=0.9746 Acc=0.4631 F1=0.3959 || Val Loss=1.0699 Acc=0.4032 F1=0.2870\n",
      "Epoch 22/200 | Train Loss=0.9724 Acc=0.4671 F1=0.4101 || Val Loss=1.1053 Acc=0.3710 F1=0.2504\n",
      "Epoch 23/200 | Train Loss=0.9738 Acc=0.4750 F1=0.4212 || Val Loss=1.0783 Acc=0.4194 F1=0.3012\n",
      "Epoch 24/200 | Train Loss=0.9656 Acc=0.4750 F1=0.4101 || Val Loss=1.1039 Acc=0.3548 F1=0.2346\n",
      "Epoch 25/200 | Train Loss=0.9600 Acc=0.4671 F1=0.4038 || Val Loss=1.1137 Acc=0.3710 F1=0.2505\n",
      "Epoch 26/200 | Train Loss=0.9621 Acc=0.5050 F1=0.4427 || Val Loss=1.1082 Acc=0.4032 F1=0.2926\n",
      "Epoch 27/200 | Train Loss=0.9554 Acc=0.4790 F1=0.4376 || Val Loss=1.1124 Acc=0.3710 F1=0.2505\n",
      "Epoch 28/200 | Train Loss=0.9644 Acc=0.4790 F1=0.4242 || Val Loss=1.1329 Acc=0.3871 F1=0.2657\n",
      "Epoch 29/200 | Train Loss=0.9536 Acc=0.5030 F1=0.4459 || Val Loss=1.0672 Acc=0.4194 F1=0.3012\n",
      "Epoch 30/200 | Train Loss=0.9450 Acc=0.4910 F1=0.4381 || Val Loss=1.1088 Acc=0.4194 F1=0.2943\n",
      "[INFO] Early stopping triggered at epoch 30\n",
      "[RESULT] Fold 17: Test Loss=1.0318 Acc=0.4688 F1=0.2956\n",
      "\n",
      "[INFO] Starting Fold 18/36\n",
      "Epoch 1/200 | Train Loss=1.0637 Acc=0.4048 F1=0.3403 || Val Loss=1.0503 Acc=0.3438 F1=0.2006\n",
      "Epoch 2/200 | Train Loss=1.0148 Acc=0.4722 F1=0.3262 || Val Loss=1.0352 Acc=0.4844 F1=0.2796\n",
      "Epoch 3/200 | Train Loss=1.0016 Acc=0.4821 F1=0.3160 || Val Loss=1.0177 Acc=0.4688 F1=0.2174\n",
      "Epoch 4/200 | Train Loss=1.0025 Acc=0.4762 F1=0.3063 || Val Loss=1.0221 Acc=0.5156 F1=0.3306\n",
      "Epoch 5/200 | Train Loss=1.0000 Acc=0.4663 F1=0.3360 || Val Loss=1.0282 Acc=0.4219 F1=0.3051\n",
      "Epoch 6/200 | Train Loss=0.9946 Acc=0.4841 F1=0.3427 || Val Loss=1.0393 Acc=0.4219 F1=0.3025\n",
      "Epoch 7/200 | Train Loss=0.9964 Acc=0.4722 F1=0.3364 || Val Loss=1.0304 Acc=0.4062 F1=0.2540\n",
      "Epoch 8/200 | Train Loss=0.9885 Acc=0.4762 F1=0.3399 || Val Loss=1.0457 Acc=0.4062 F1=0.2825\n",
      "Epoch 9/200 | Train Loss=0.9831 Acc=0.4821 F1=0.3467 || Val Loss=1.0381 Acc=0.4062 F1=0.2938\n",
      "Epoch 10/200 | Train Loss=0.9773 Acc=0.4901 F1=0.3533 || Val Loss=1.0403 Acc=0.3906 F1=0.2815\n",
      "Epoch 11/200 | Train Loss=0.9782 Acc=0.4881 F1=0.3524 || Val Loss=1.0473 Acc=0.4062 F1=0.2938\n",
      "Epoch 12/200 | Train Loss=0.9818 Acc=0.4921 F1=0.3547 || Val Loss=1.0536 Acc=0.4062 F1=0.2938\n",
      "Epoch 13/200 | Train Loss=0.9685 Acc=0.4881 F1=0.3521 || Val Loss=1.0414 Acc=0.4375 F1=0.3098\n",
      "Epoch 14/200 | Train Loss=0.9695 Acc=0.4960 F1=0.3509 || Val Loss=1.0410 Acc=0.4844 F1=0.3481\n",
      "Epoch 15/200 | Train Loss=0.9660 Acc=0.5119 F1=0.3774 || Val Loss=1.0603 Acc=0.3906 F1=0.2825\n",
      "Epoch 16/200 | Train Loss=0.9595 Acc=0.5238 F1=0.3854 || Val Loss=1.0618 Acc=0.4688 F1=0.3376\n",
      "Epoch 17/200 | Train Loss=0.9551 Acc=0.5218 F1=0.3821 || Val Loss=1.0509 Acc=0.5000 F1=0.3586\n",
      "Epoch 18/200 | Train Loss=0.9507 Acc=0.5099 F1=0.3759 || Val Loss=1.0532 Acc=0.4375 F1=0.3160\n",
      "Epoch 19/200 | Train Loss=0.9475 Acc=0.5198 F1=0.3832 || Val Loss=1.0765 Acc=0.5000 F1=0.3440\n",
      "Epoch 20/200 | Train Loss=0.9420 Acc=0.5496 F1=0.3990 || Val Loss=1.1012 Acc=0.4219 F1=0.3050\n",
      "Epoch 21/200 | Train Loss=0.9348 Acc=0.5258 F1=0.3945 || Val Loss=1.0727 Acc=0.4844 F1=0.3394\n",
      "Epoch 22/200 | Train Loss=0.9402 Acc=0.5099 F1=0.3824 || Val Loss=1.1183 Acc=0.4844 F1=0.3433\n",
      "Epoch 23/200 | Train Loss=0.9236 Acc=0.5536 F1=0.4137 || Val Loss=1.0943 Acc=0.4688 F1=0.3196\n",
      "Epoch 24/200 | Train Loss=0.9188 Acc=0.5278 F1=0.3938 || Val Loss=1.0994 Acc=0.4844 F1=0.3394\n",
      "Epoch 25/200 | Train Loss=0.9201 Acc=0.5337 F1=0.3925 || Val Loss=1.1251 Acc=0.4688 F1=0.3196\n",
      "Epoch 26/200 | Train Loss=0.9130 Acc=0.5556 F1=0.4222 || Val Loss=1.1336 Acc=0.4688 F1=0.3251\n",
      "Epoch 27/200 | Train Loss=0.9012 Acc=0.5714 F1=0.4527 || Val Loss=1.1410 Acc=0.4688 F1=0.3251\n",
      "Epoch 28/200 | Train Loss=0.8957 Acc=0.5694 F1=0.4441 || Val Loss=1.1847 Acc=0.4688 F1=0.3251\n",
      "Epoch 29/200 | Train Loss=0.9002 Acc=0.5754 F1=0.4565 || Val Loss=1.1703 Acc=0.4375 F1=0.2880\n",
      "Epoch 30/200 | Train Loss=0.8980 Acc=0.5893 F1=0.4758 || Val Loss=1.1328 Acc=0.4688 F1=0.3296\n",
      "Epoch 31/200 | Train Loss=0.8759 Acc=0.5774 F1=0.4635 || Val Loss=1.2063 Acc=0.4688 F1=0.3051\n",
      "Epoch 32/200 | Train Loss=0.8863 Acc=0.5754 F1=0.4272 || Val Loss=1.1717 Acc=0.4531 F1=0.3229\n",
      "Epoch 33/200 | Train Loss=0.8760 Acc=0.5754 F1=0.4510 || Val Loss=1.1511 Acc=0.5156 F1=0.3534\n",
      "Epoch 34/200 | Train Loss=0.8808 Acc=0.5913 F1=0.4525 || Val Loss=1.1868 Acc=0.4688 F1=0.3357\n",
      "Epoch 35/200 | Train Loss=0.8721 Acc=0.5734 F1=0.4497 || Val Loss=1.1474 Acc=0.5000 F1=0.3379\n",
      "Epoch 36/200 | Train Loss=0.8576 Acc=0.5992 F1=0.5021 || Val Loss=1.2224 Acc=0.4531 F1=0.3231\n",
      "Epoch 37/200 | Train Loss=0.8607 Acc=0.5893 F1=0.4670 || Val Loss=1.2601 Acc=0.4844 F1=0.3394\n",
      "[INFO] Early stopping triggered at epoch 37\n",
      "[RESULT] Fold 18: Test Loss=0.9223 Acc=0.4590 F1=0.3124\n",
      "\n",
      "[INFO] Starting Fold 19/36\n",
      "Epoch 1/200 | Train Loss=1.0640 Acc=0.3884 F1=0.3158 || Val Loss=1.0929 Acc=0.4194 F1=0.1970\n",
      "Epoch 2/200 | Train Loss=1.0487 Acc=0.4143 F1=0.2755 || Val Loss=1.1159 Acc=0.4194 F1=0.2227\n",
      "Epoch 3/200 | Train Loss=1.0345 Acc=0.4323 F1=0.3030 || Val Loss=1.0816 Acc=0.4194 F1=0.2227\n",
      "Epoch 4/200 | Train Loss=1.0364 Acc=0.4562 F1=0.3337 || Val Loss=1.0728 Acc=0.4194 F1=0.2227\n",
      "Epoch 5/200 | Train Loss=1.0330 Acc=0.4382 F1=0.3262 || Val Loss=1.0727 Acc=0.4194 F1=0.2227\n",
      "Epoch 6/200 | Train Loss=1.0244 Acc=0.4562 F1=0.3328 || Val Loss=1.0770 Acc=0.4355 F1=0.2497\n",
      "Epoch 7/200 | Train Loss=1.0194 Acc=0.4442 F1=0.3241 || Val Loss=1.0810 Acc=0.4355 F1=0.2497\n",
      "Epoch 8/200 | Train Loss=1.0149 Acc=0.4861 F1=0.3532 || Val Loss=1.0816 Acc=0.4516 F1=0.2895\n",
      "Epoch 9/200 | Train Loss=1.0110 Acc=0.4841 F1=0.3530 || Val Loss=1.0799 Acc=0.4355 F1=0.3019\n",
      "Epoch 10/200 | Train Loss=1.0118 Acc=0.4582 F1=0.3343 || Val Loss=1.0771 Acc=0.4516 F1=0.3117\n",
      "Epoch 11/200 | Train Loss=1.0074 Acc=0.4821 F1=0.3511 || Val Loss=1.0787 Acc=0.4516 F1=0.3117\n",
      "Epoch 12/200 | Train Loss=0.9994 Acc=0.4821 F1=0.3538 || Val Loss=1.0792 Acc=0.4194 F1=0.2996\n",
      "Epoch 13/200 | Train Loss=0.9990 Acc=0.5000 F1=0.3715 || Val Loss=1.0880 Acc=0.3871 F1=0.2847\n",
      "Epoch 14/200 | Train Loss=0.9976 Acc=0.4821 F1=0.3516 || Val Loss=1.0857 Acc=0.4194 F1=0.3058\n",
      "Epoch 15/200 | Train Loss=0.9977 Acc=0.4960 F1=0.3592 || Val Loss=1.0846 Acc=0.4516 F1=0.3016\n",
      "Epoch 16/200 | Train Loss=0.9903 Acc=0.5020 F1=0.3634 || Val Loss=1.0834 Acc=0.4194 F1=0.2996\n",
      "Epoch 17/200 | Train Loss=0.9840 Acc=0.5279 F1=0.3983 || Val Loss=1.0881 Acc=0.4032 F1=0.2952\n",
      "Epoch 18/200 | Train Loss=0.9841 Acc=0.5000 F1=0.3718 || Val Loss=1.0924 Acc=0.4194 F1=0.2996\n",
      "Epoch 19/200 | Train Loss=0.9811 Acc=0.5120 F1=0.3778 || Val Loss=1.0953 Acc=0.4194 F1=0.2828\n",
      "Epoch 20/200 | Train Loss=0.9864 Acc=0.5060 F1=0.3730 || Val Loss=1.1025 Acc=0.4194 F1=0.3095\n",
      "Epoch 21/200 | Train Loss=0.9782 Acc=0.5060 F1=0.3698 || Val Loss=1.0935 Acc=0.4355 F1=0.3163\n",
      "Epoch 22/200 | Train Loss=0.9728 Acc=0.5139 F1=0.3883 || Val Loss=1.0935 Acc=0.4355 F1=0.3163\n",
      "Epoch 23/200 | Train Loss=0.9654 Acc=0.5080 F1=0.3823 || Val Loss=1.0932 Acc=0.4032 F1=0.2823\n",
      "Epoch 24/200 | Train Loss=0.9662 Acc=0.5000 F1=0.3631 || Val Loss=1.1132 Acc=0.4032 F1=0.2952\n",
      "Epoch 25/200 | Train Loss=0.9530 Acc=0.5279 F1=0.3976 || Val Loss=1.1120 Acc=0.4032 F1=0.2952\n",
      "Epoch 26/200 | Train Loss=0.9563 Acc=0.5239 F1=0.3867 || Val Loss=1.1083 Acc=0.3871 F1=0.2793\n",
      "Epoch 27/200 | Train Loss=0.9483 Acc=0.5259 F1=0.3982 || Val Loss=1.1159 Acc=0.4032 F1=0.2943\n",
      "Epoch 28/200 | Train Loss=0.9501 Acc=0.5279 F1=0.3896 || Val Loss=1.1186 Acc=0.3871 F1=0.2725\n",
      "Epoch 29/200 | Train Loss=0.9535 Acc=0.5159 F1=0.3969 || Val Loss=1.1072 Acc=0.4194 F1=0.3123\n",
      "Epoch 30/200 | Train Loss=0.9347 Acc=0.5378 F1=0.4387 || Val Loss=1.1099 Acc=0.4355 F1=0.2845\n",
      "Epoch 31/200 | Train Loss=0.9354 Acc=0.5299 F1=0.4280 || Val Loss=1.1511 Acc=0.4194 F1=0.3130\n",
      "Epoch 32/200 | Train Loss=0.9322 Acc=0.5359 F1=0.4278 || Val Loss=1.1648 Acc=0.4032 F1=0.3021\n",
      "Epoch 33/200 | Train Loss=0.9213 Acc=0.5339 F1=0.4360 || Val Loss=1.1558 Acc=0.4032 F1=0.2952\n",
      "Epoch 34/200 | Train Loss=0.9097 Acc=0.5518 F1=0.4630 || Val Loss=1.1645 Acc=0.4032 F1=0.2895\n",
      "Epoch 35/200 | Train Loss=0.9142 Acc=0.5359 F1=0.4328 || Val Loss=1.1792 Acc=0.4355 F1=0.3293\n",
      "Epoch 36/200 | Train Loss=0.8979 Acc=0.5538 F1=0.4697 || Val Loss=1.2043 Acc=0.4355 F1=0.3199\n",
      "Epoch 37/200 | Train Loss=0.8936 Acc=0.5438 F1=0.4486 || Val Loss=1.1793 Acc=0.4516 F1=0.3373\n",
      "Epoch 38/200 | Train Loss=0.9089 Acc=0.5498 F1=0.4722 || Val Loss=1.1797 Acc=0.4355 F1=0.3253\n",
      "Epoch 39/200 | Train Loss=0.8954 Acc=0.5637 F1=0.4982 || Val Loss=1.2641 Acc=0.4355 F1=0.3201\n",
      "Epoch 40/200 | Train Loss=0.8787 Acc=0.5677 F1=0.4992 || Val Loss=1.2001 Acc=0.4516 F1=0.3322\n",
      "Epoch 41/200 | Train Loss=0.8668 Acc=0.5936 F1=0.5236 || Val Loss=1.2071 Acc=0.4355 F1=0.3098\n",
      "Epoch 42/200 | Train Loss=0.8559 Acc=0.5916 F1=0.5186 || Val Loss=1.2793 Acc=0.4516 F1=0.3431\n",
      "Epoch 43/200 | Train Loss=0.8454 Acc=0.5936 F1=0.5298 || Val Loss=1.2546 Acc=0.4516 F1=0.3308\n",
      "Epoch 44/200 | Train Loss=0.8419 Acc=0.6195 F1=0.5361 || Val Loss=1.2187 Acc=0.4032 F1=0.2987\n",
      "Epoch 45/200 | Train Loss=0.8752 Acc=0.5976 F1=0.5467 || Val Loss=1.2298 Acc=0.4355 F1=0.3217\n",
      "Epoch 46/200 | Train Loss=0.8560 Acc=0.5777 F1=0.5259 || Val Loss=1.2024 Acc=0.4355 F1=0.3068\n",
      "Epoch 47/200 | Train Loss=0.8453 Acc=0.6016 F1=0.5221 || Val Loss=1.2802 Acc=0.4194 F1=0.3132\n",
      "Epoch 48/200 | Train Loss=0.8130 Acc=0.6255 F1=0.5557 || Val Loss=1.3817 Acc=0.3871 F1=0.2532\n",
      "Epoch 49/200 | Train Loss=0.8158 Acc=0.5837 F1=0.5259 || Val Loss=1.1772 Acc=0.4355 F1=0.3545\n",
      "Epoch 50/200 | Train Loss=0.8058 Acc=0.6295 F1=0.5862 || Val Loss=1.3125 Acc=0.4194 F1=0.3322\n",
      "Epoch 51/200 | Train Loss=0.8081 Acc=0.6116 F1=0.5316 || Val Loss=1.3076 Acc=0.4355 F1=0.3812\n",
      "Epoch 52/200 | Train Loss=0.8169 Acc=0.6016 F1=0.5459 || Val Loss=1.2752 Acc=0.4355 F1=0.3312\n",
      "Epoch 53/200 | Train Loss=0.8098 Acc=0.6096 F1=0.5377 || Val Loss=1.2768 Acc=0.4194 F1=0.3214\n",
      "Epoch 54/200 | Train Loss=0.8081 Acc=0.6235 F1=0.5521 || Val Loss=1.3065 Acc=0.3387 F1=0.2868\n",
      "Epoch 55/200 | Train Loss=0.8092 Acc=0.6036 F1=0.5732 || Val Loss=1.3333 Acc=0.4032 F1=0.3024\n",
      "Epoch 56/200 | Train Loss=0.7900 Acc=0.6315 F1=0.5663 || Val Loss=1.3221 Acc=0.4355 F1=0.3483\n",
      "Epoch 57/200 | Train Loss=0.7741 Acc=0.6454 F1=0.6046 || Val Loss=1.3737 Acc=0.4194 F1=0.3228\n",
      "Epoch 58/200 | Train Loss=0.7692 Acc=0.6614 F1=0.6162 || Val Loss=1.3297 Acc=0.4194 F1=0.3214\n",
      "Epoch 59/200 | Train Loss=0.7580 Acc=0.6315 F1=0.5846 || Val Loss=1.4089 Acc=0.3871 F1=0.3111\n",
      "Epoch 60/200 | Train Loss=0.7324 Acc=0.6713 F1=0.6295 || Val Loss=1.3854 Acc=0.4032 F1=0.3217\n",
      "Epoch 61/200 | Train Loss=0.7564 Acc=0.6335 F1=0.5698 || Val Loss=1.4348 Acc=0.4032 F1=0.3128\n",
      "Epoch 62/200 | Train Loss=0.7512 Acc=0.6534 F1=0.6325 || Val Loss=1.3552 Acc=0.3548 F1=0.2892\n",
      "Epoch 63/200 | Train Loss=0.7598 Acc=0.6375 F1=0.6016 || Val Loss=1.4567 Acc=0.4032 F1=0.3188\n",
      "Epoch 64/200 | Train Loss=0.7326 Acc=0.6693 F1=0.6057 || Val Loss=1.4633 Acc=0.3871 F1=0.3095\n",
      "Epoch 65/200 | Train Loss=0.7525 Acc=0.6554 F1=0.6254 || Val Loss=1.4188 Acc=0.4516 F1=0.3661\n",
      "Epoch 66/200 | Train Loss=0.7679 Acc=0.6474 F1=0.5823 || Val Loss=1.4287 Acc=0.4194 F1=0.3368\n",
      "Epoch 67/200 | Train Loss=0.6984 Acc=0.6673 F1=0.6192 || Val Loss=1.5069 Acc=0.3871 F1=0.3500\n",
      "Epoch 68/200 | Train Loss=0.7135 Acc=0.6713 F1=0.6494 || Val Loss=1.4003 Acc=0.4355 F1=0.3472\n",
      "Epoch 69/200 | Train Loss=0.7198 Acc=0.6713 F1=0.6118 || Val Loss=1.5085 Acc=0.4355 F1=0.3415\n",
      "Epoch 70/200 | Train Loss=0.7151 Acc=0.6793 F1=0.6384 || Val Loss=1.5061 Acc=0.3710 F1=0.3289\n",
      "Epoch 71/200 | Train Loss=0.7062 Acc=0.6952 F1=0.6678 || Val Loss=1.5917 Acc=0.3871 F1=0.3034\n",
      "[INFO] Early stopping triggered at epoch 71\n",
      "[RESULT] Fold 19: Test Loss=1.3628 Acc=0.4032 F1=0.3022\n",
      "\n",
      "[INFO] Starting Fold 20/36\n",
      "Epoch 1/200 | Train Loss=1.0599 Acc=0.4012 F1=0.2397 || Val Loss=1.0813 Acc=0.3871 F1=0.2115\n",
      "Epoch 2/200 | Train Loss=1.0313 Acc=0.4251 F1=0.3069 || Val Loss=1.0857 Acc=0.3387 F1=0.2487\n",
      "Epoch 3/200 | Train Loss=1.0225 Acc=0.4491 F1=0.2920 || Val Loss=1.0664 Acc=0.3548 F1=0.2238\n",
      "Epoch 4/200 | Train Loss=1.0210 Acc=0.4511 F1=0.2687 || Val Loss=1.0701 Acc=0.3387 F1=0.1882\n",
      "Epoch 5/200 | Train Loss=1.0137 Acc=0.4431 F1=0.2982 || Val Loss=1.0836 Acc=0.3871 F1=0.2883\n",
      "Epoch 6/200 | Train Loss=1.0133 Acc=0.4471 F1=0.3194 || Val Loss=1.0990 Acc=0.3065 F1=0.2002\n",
      "Epoch 7/200 | Train Loss=1.0091 Acc=0.4591 F1=0.2914 || Val Loss=1.0982 Acc=0.3387 F1=0.1707\n",
      "Epoch 8/200 | Train Loss=1.0100 Acc=0.4551 F1=0.2657 || Val Loss=1.0994 Acc=0.3548 F1=0.2113\n",
      "Epoch 9/200 | Train Loss=1.0041 Acc=0.4770 F1=0.3070 || Val Loss=1.1031 Acc=0.3871 F1=0.2754\n",
      "Epoch 10/200 | Train Loss=1.0024 Acc=0.4830 F1=0.3487 || Val Loss=1.1088 Acc=0.3871 F1=0.2878\n",
      "Epoch 11/200 | Train Loss=0.9996 Acc=0.4770 F1=0.3444 || Val Loss=1.1117 Acc=0.3710 F1=0.2521\n",
      "Epoch 12/200 | Train Loss=0.9974 Acc=0.4890 F1=0.3284 || Val Loss=1.1261 Acc=0.3710 F1=0.2320\n",
      "Epoch 13/200 | Train Loss=0.9977 Acc=0.4591 F1=0.3203 || Val Loss=1.1234 Acc=0.3710 F1=0.2596\n",
      "Epoch 14/200 | Train Loss=0.9984 Acc=0.4611 F1=0.3318 || Val Loss=1.1398 Acc=0.3871 F1=0.2754\n",
      "Epoch 15/200 | Train Loss=0.9912 Acc=0.4711 F1=0.3231 || Val Loss=1.1433 Acc=0.3871 F1=0.2516\n",
      "Epoch 16/200 | Train Loss=0.9898 Acc=0.4750 F1=0.3406 || Val Loss=1.1455 Acc=0.3710 F1=0.2654\n",
      "Epoch 17/200 | Train Loss=0.9894 Acc=0.4830 F1=0.3510 || Val Loss=1.1587 Acc=0.3548 F1=0.2553\n",
      "Epoch 18/200 | Train Loss=0.9841 Acc=0.4671 F1=0.3393 || Val Loss=1.1708 Acc=0.3387 F1=0.2336\n",
      "Epoch 19/200 | Train Loss=0.9833 Acc=0.5030 F1=0.3610 || Val Loss=1.1744 Acc=0.3548 F1=0.2553\n",
      "Epoch 20/200 | Train Loss=0.9796 Acc=0.4870 F1=0.3799 || Val Loss=1.2191 Acc=0.3387 F1=0.2401\n",
      "Epoch 21/200 | Train Loss=0.9749 Acc=0.4990 F1=0.3660 || Val Loss=1.2020 Acc=0.3387 F1=0.2336\n",
      "Epoch 22/200 | Train Loss=0.9680 Acc=0.4970 F1=0.3745 || Val Loss=1.1929 Acc=0.3387 F1=0.2401\n",
      "Epoch 23/200 | Train Loss=0.9703 Acc=0.4930 F1=0.3781 || Val Loss=1.1938 Acc=0.3387 F1=0.2451\n",
      "Epoch 24/200 | Train Loss=0.9588 Acc=0.5070 F1=0.3867 || Val Loss=1.2861 Acc=0.3548 F1=0.2342\n",
      "Epoch 25/200 | Train Loss=0.9614 Acc=0.4890 F1=0.3458 || Val Loss=1.2421 Acc=0.3548 F1=0.2429\n",
      "[INFO] Early stopping triggered at epoch 25\n",
      "[RESULT] Fold 20: Test Loss=0.9382 Acc=0.4688 F1=0.3279\n",
      "\n",
      "[INFO] Starting Fold 21/36\n",
      "Epoch 1/200 | Train Loss=1.0114 Acc=0.4603 F1=0.3160 || Val Loss=1.0055 Acc=0.4688 F1=0.2910\n",
      "Epoch 2/200 | Train Loss=0.8927 Acc=0.5060 F1=0.2816 || Val Loss=1.0385 Acc=0.4688 F1=0.2910\n",
      "Epoch 3/200 | Train Loss=0.8778 Acc=0.5000 F1=0.2757 || Val Loss=1.0884 Acc=0.4844 F1=0.3089\n",
      "Epoch 4/200 | Train Loss=0.8820 Acc=0.5079 F1=0.3253 || Val Loss=1.0817 Acc=0.5000 F1=0.3254\n",
      "Epoch 5/200 | Train Loss=0.8795 Acc=0.5040 F1=0.2725 || Val Loss=1.0436 Acc=0.4844 F1=0.3085\n",
      "Epoch 6/200 | Train Loss=0.8712 Acc=0.5079 F1=0.2825 || Val Loss=1.0107 Acc=0.5938 F1=0.4178\n",
      "Epoch 7/200 | Train Loss=0.8716 Acc=0.5317 F1=0.3357 || Val Loss=1.0023 Acc=0.5938 F1=0.4148\n",
      "Epoch 8/200 | Train Loss=0.8680 Acc=0.5298 F1=0.3346 || Val Loss=1.0047 Acc=0.5938 F1=0.4148\n",
      "Epoch 9/200 | Train Loss=0.8686 Acc=0.5159 F1=0.3226 || Val Loss=1.0168 Acc=0.5312 F1=0.3573\n",
      "Epoch 10/200 | Train Loss=0.8673 Acc=0.4921 F1=0.3055 || Val Loss=1.0102 Acc=0.5938 F1=0.4196\n",
      "Epoch 11/200 | Train Loss=0.8610 Acc=0.5377 F1=0.3672 || Val Loss=1.0037 Acc=0.5938 F1=0.4200\n",
      "Epoch 12/200 | Train Loss=0.8610 Acc=0.5139 F1=0.3052 || Val Loss=0.9974 Acc=0.5938 F1=0.4148\n",
      "Epoch 13/200 | Train Loss=0.8635 Acc=0.5099 F1=0.3451 || Val Loss=0.9805 Acc=0.5469 F1=0.3599\n",
      "Epoch 14/200 | Train Loss=0.8537 Acc=0.5516 F1=0.3786 || Val Loss=0.9903 Acc=0.6250 F1=0.4449\n",
      "Epoch 15/200 | Train Loss=0.8541 Acc=0.5456 F1=0.3402 || Val Loss=0.9995 Acc=0.5938 F1=0.4148\n",
      "Epoch 16/200 | Train Loss=0.8514 Acc=0.5377 F1=0.3464 || Val Loss=0.9815 Acc=0.5938 F1=0.4043\n",
      "Epoch 17/200 | Train Loss=0.8481 Acc=0.5536 F1=0.3797 || Val Loss=0.9872 Acc=0.5938 F1=0.4149\n",
      "Epoch 18/200 | Train Loss=0.8394 Acc=0.5575 F1=0.3732 || Val Loss=1.0003 Acc=0.5938 F1=0.4190\n",
      "Epoch 19/200 | Train Loss=0.8445 Acc=0.5615 F1=0.3563 || Val Loss=0.9934 Acc=0.6094 F1=0.4336\n",
      "Epoch 20/200 | Train Loss=0.8350 Acc=0.5913 F1=0.4000 || Val Loss=0.9837 Acc=0.5781 F1=0.4052\n",
      "Epoch 21/200 | Train Loss=0.8335 Acc=0.5635 F1=0.3865 || Val Loss=0.9812 Acc=0.5625 F1=0.3972\n",
      "Epoch 22/200 | Train Loss=0.8370 Acc=0.5615 F1=0.3759 || Val Loss=0.9902 Acc=0.6094 F1=0.4327\n",
      "Epoch 23/200 | Train Loss=0.8332 Acc=0.5873 F1=0.3891 || Val Loss=1.0016 Acc=0.5625 F1=0.3972\n",
      "Epoch 24/200 | Train Loss=0.8280 Acc=0.5833 F1=0.3971 || Val Loss=0.9987 Acc=0.6094 F1=0.4280\n",
      "Epoch 25/200 | Train Loss=0.8226 Acc=0.5655 F1=0.3850 || Val Loss=0.9956 Acc=0.5781 F1=0.4111\n",
      "Epoch 26/200 | Train Loss=0.8218 Acc=0.5833 F1=0.3939 || Val Loss=0.9819 Acc=0.5938 F1=0.4220\n",
      "Epoch 27/200 | Train Loss=0.8189 Acc=0.5933 F1=0.4009 || Val Loss=0.9848 Acc=0.5938 F1=0.4211\n",
      "Epoch 28/200 | Train Loss=0.8144 Acc=0.5615 F1=0.3838 || Val Loss=0.9985 Acc=0.6094 F1=0.4280\n",
      "Epoch 29/200 | Train Loss=0.8122 Acc=0.5813 F1=0.3946 || Val Loss=1.0148 Acc=0.5938 F1=0.4226\n",
      "Epoch 30/200 | Train Loss=0.8048 Acc=0.5893 F1=0.3955 || Val Loss=0.9990 Acc=0.6406 F1=0.4526\n",
      "Epoch 31/200 | Train Loss=0.8058 Acc=0.5853 F1=0.4210 || Val Loss=0.9929 Acc=0.6094 F1=0.4280\n",
      "Epoch 32/200 | Train Loss=0.7976 Acc=0.5893 F1=0.4422 || Val Loss=0.9827 Acc=0.6094 F1=0.4334\n",
      "Epoch 33/200 | Train Loss=0.7972 Acc=0.5972 F1=0.4231 || Val Loss=1.0116 Acc=0.6094 F1=0.4317\n",
      "Epoch 34/200 | Train Loss=0.7872 Acc=0.5913 F1=0.4564 || Val Loss=1.0091 Acc=0.5781 F1=0.4105\n",
      "Epoch 35/200 | Train Loss=0.7780 Acc=0.6091 F1=0.4359 || Val Loss=1.0228 Acc=0.5781 F1=0.4093\n",
      "Epoch 36/200 | Train Loss=0.7690 Acc=0.6032 F1=0.4495 || Val Loss=1.0391 Acc=0.5938 F1=0.4198\n",
      "Epoch 37/200 | Train Loss=0.7721 Acc=0.6131 F1=0.4542 || Val Loss=0.9990 Acc=0.6094 F1=0.4301\n",
      "Epoch 38/200 | Train Loss=0.7705 Acc=0.6111 F1=0.4559 || Val Loss=1.0252 Acc=0.5938 F1=0.4211\n",
      "Epoch 39/200 | Train Loss=0.7606 Acc=0.6190 F1=0.4744 || Val Loss=1.0555 Acc=0.6094 F1=0.4333\n",
      "Epoch 40/200 | Train Loss=0.7502 Acc=0.6151 F1=0.4921 || Val Loss=0.9981 Acc=0.5938 F1=0.4202\n",
      "Epoch 41/200 | Train Loss=0.7484 Acc=0.6151 F1=0.4886 || Val Loss=1.0631 Acc=0.6094 F1=0.4333\n",
      "Epoch 42/200 | Train Loss=0.7621 Acc=0.6111 F1=0.4543 || Val Loss=1.0742 Acc=0.5938 F1=0.4167\n",
      "Epoch 43/200 | Train Loss=0.7492 Acc=0.6052 F1=0.4701 || Val Loss=1.0880 Acc=0.6094 F1=0.4336\n",
      "Epoch 44/200 | Train Loss=0.7503 Acc=0.6151 F1=0.4689 || Val Loss=1.1104 Acc=0.5312 F1=0.3615\n",
      "Epoch 45/200 | Train Loss=0.7511 Acc=0.6012 F1=0.4125 || Val Loss=1.0870 Acc=0.5938 F1=0.4183\n",
      "Epoch 46/200 | Train Loss=0.7444 Acc=0.6171 F1=0.4541 || Val Loss=1.1094 Acc=0.6562 F1=0.5335\n",
      "Epoch 47/200 | Train Loss=0.7574 Acc=0.6210 F1=0.4788 || Val Loss=1.0524 Acc=0.5781 F1=0.4064\n",
      "Epoch 48/200 | Train Loss=0.7210 Acc=0.6290 F1=0.5197 || Val Loss=1.1221 Acc=0.6250 F1=0.4444\n",
      "Epoch 49/200 | Train Loss=0.7132 Acc=0.6369 F1=0.5190 || Val Loss=1.1249 Acc=0.6250 F1=0.4422\n",
      "Epoch 50/200 | Train Loss=0.7225 Acc=0.6210 F1=0.4998 || Val Loss=1.1108 Acc=0.6094 F1=0.4327\n",
      "Epoch 51/200 | Train Loss=0.7019 Acc=0.6528 F1=0.5593 || Val Loss=1.1121 Acc=0.5625 F1=0.3999\n",
      "Epoch 52/200 | Train Loss=0.7024 Acc=0.6567 F1=0.5549 || Val Loss=1.1412 Acc=0.5469 F1=0.3854\n",
      "Epoch 53/200 | Train Loss=0.7141 Acc=0.6389 F1=0.5311 || Val Loss=1.2175 Acc=0.5781 F1=0.4111\n",
      "Epoch 54/200 | Train Loss=0.6882 Acc=0.6587 F1=0.5407 || Val Loss=1.1409 Acc=0.5312 F1=0.3782\n",
      "Epoch 55/200 | Train Loss=0.6837 Acc=0.6647 F1=0.5290 || Val Loss=1.2486 Acc=0.5625 F1=0.3997\n",
      "Epoch 56/200 | Train Loss=0.6797 Acc=0.6627 F1=0.5594 || Val Loss=1.3415 Acc=0.5625 F1=0.3976\n",
      "Epoch 57/200 | Train Loss=0.6922 Acc=0.6389 F1=0.5206 || Val Loss=1.2088 Acc=0.5312 F1=0.3776\n",
      "Epoch 58/200 | Train Loss=0.6953 Acc=0.6627 F1=0.5706 || Val Loss=1.2601 Acc=0.5312 F1=0.3776\n",
      "Epoch 59/200 | Train Loss=0.6735 Acc=0.6607 F1=0.5358 || Val Loss=1.3310 Acc=0.5625 F1=0.3999\n",
      "Epoch 60/200 | Train Loss=0.6809 Acc=0.6528 F1=0.5233 || Val Loss=1.2479 Acc=0.5469 F1=0.3877\n",
      "Epoch 61/200 | Train Loss=0.6549 Acc=0.6865 F1=0.5504 || Val Loss=1.3726 Acc=0.5469 F1=0.3825\n",
      "Epoch 62/200 | Train Loss=0.6695 Acc=0.6825 F1=0.5579 || Val Loss=1.3008 Acc=0.5469 F1=0.3859\n",
      "Epoch 63/200 | Train Loss=0.6608 Acc=0.7004 F1=0.5844 || Val Loss=1.3444 Acc=0.5938 F1=0.4211\n",
      "Epoch 64/200 | Train Loss=0.6460 Acc=0.6905 F1=0.5481 || Val Loss=1.3482 Acc=0.5156 F1=0.3569\n",
      "Epoch 65/200 | Train Loss=0.6551 Acc=0.6865 F1=0.5648 || Val Loss=1.3739 Acc=0.5938 F1=0.4198\n",
      "Epoch 66/200 | Train Loss=0.6561 Acc=0.6667 F1=0.5536 || Val Loss=1.3568 Acc=0.5000 F1=0.3529\n",
      "[INFO] Early stopping triggered at epoch 66\n",
      "[RESULT] Fold 21: Test Loss=1.3870 Acc=0.4098 F1=0.2891\n",
      "\n",
      "[INFO] Starting Fold 22/36\n",
      "Epoch 1/200 | Train Loss=1.0212 Acc=0.4442 F1=0.3072 || Val Loss=1.0331 Acc=0.4516 F1=0.2991\n",
      "Epoch 2/200 | Train Loss=0.9897 Acc=0.4064 F1=0.2876 || Val Loss=1.0485 Acc=0.4839 F1=0.2952\n",
      "Epoch 3/200 | Train Loss=0.9865 Acc=0.4701 F1=0.2891 || Val Loss=1.0380 Acc=0.4839 F1=0.2952\n",
      "Epoch 4/200 | Train Loss=0.9774 Acc=0.5159 F1=0.3413 || Val Loss=1.0284 Acc=0.4839 F1=0.2952\n",
      "Epoch 5/200 | Train Loss=0.9773 Acc=0.4960 F1=0.3356 || Val Loss=1.0264 Acc=0.4839 F1=0.2952\n",
      "Epoch 6/200 | Train Loss=0.9790 Acc=0.4801 F1=0.3086 || Val Loss=1.0287 Acc=0.4839 F1=0.2952\n",
      "Epoch 7/200 | Train Loss=0.9747 Acc=0.4861 F1=0.3132 || Val Loss=1.0296 Acc=0.4839 F1=0.2952\n",
      "Epoch 8/200 | Train Loss=0.9738 Acc=0.4821 F1=0.3098 || Val Loss=1.0332 Acc=0.4839 F1=0.2952\n",
      "Epoch 9/200 | Train Loss=0.9739 Acc=0.4781 F1=0.3173 || Val Loss=1.0318 Acc=0.4839 F1=0.2949\n",
      "Epoch 10/200 | Train Loss=0.9692 Acc=0.5199 F1=0.3623 || Val Loss=1.0293 Acc=0.4839 F1=0.2949\n",
      "Epoch 11/200 | Train Loss=0.9665 Acc=0.5239 F1=0.3705 || Val Loss=1.0279 Acc=0.4839 F1=0.2949\n",
      "Epoch 12/200 | Train Loss=0.9592 Acc=0.5478 F1=0.3796 || Val Loss=1.0323 Acc=0.4839 F1=0.2949\n",
      "Epoch 13/200 | Train Loss=0.9561 Acc=0.5259 F1=0.3545 || Val Loss=1.0311 Acc=0.4677 F1=0.2866\n",
      "Epoch 14/200 | Train Loss=0.9567 Acc=0.5179 F1=0.3662 || Val Loss=1.0275 Acc=0.5161 F1=0.3443\n",
      "Epoch 15/200 | Train Loss=0.9495 Acc=0.5239 F1=0.3656 || Val Loss=1.0345 Acc=0.4677 F1=0.2866\n",
      "Epoch 16/200 | Train Loss=0.9489 Acc=0.5299 F1=0.3644 || Val Loss=1.0306 Acc=0.4839 F1=0.3068\n",
      "Epoch 17/200 | Train Loss=0.9436 Acc=0.5319 F1=0.3730 || Val Loss=1.0374 Acc=0.5323 F1=0.3618\n",
      "Epoch 18/200 | Train Loss=0.9411 Acc=0.5438 F1=0.3839 || Val Loss=1.0285 Acc=0.4839 F1=0.3068\n",
      "Epoch 19/200 | Train Loss=0.9412 Acc=0.5458 F1=0.3838 || Val Loss=1.0396 Acc=0.4839 F1=0.3068\n",
      "Epoch 20/200 | Train Loss=0.9368 Acc=0.5418 F1=0.3784 || Val Loss=1.0456 Acc=0.5000 F1=0.3260\n",
      "Epoch 21/200 | Train Loss=0.9213 Acc=0.5697 F1=0.4016 || Val Loss=1.0351 Acc=0.4839 F1=0.2949\n",
      "Epoch 22/200 | Train Loss=0.9286 Acc=0.5558 F1=0.4025 || Val Loss=1.0520 Acc=0.4677 F1=0.2866\n",
      "Epoch 23/200 | Train Loss=0.9182 Acc=0.5657 F1=0.4047 || Val Loss=1.0516 Acc=0.5323 F1=0.3672\n",
      "Epoch 24/200 | Train Loss=0.9158 Acc=0.5398 F1=0.3824 || Val Loss=1.0387 Acc=0.4677 F1=0.2980\n",
      "Epoch 25/200 | Train Loss=0.9144 Acc=0.5657 F1=0.4170 || Val Loss=1.0684 Acc=0.4677 F1=0.2866\n",
      "Epoch 26/200 | Train Loss=0.9100 Acc=0.5757 F1=0.4164 || Val Loss=1.0665 Acc=0.5161 F1=0.3509\n",
      "Epoch 27/200 | Train Loss=0.9082 Acc=0.5299 F1=0.3728 || Val Loss=1.1083 Acc=0.4677 F1=0.2866\n",
      "Epoch 28/200 | Train Loss=0.9132 Acc=0.5558 F1=0.4087 || Val Loss=1.0700 Acc=0.4839 F1=0.3162\n",
      "Epoch 29/200 | Train Loss=0.8974 Acc=0.5637 F1=0.4282 || Val Loss=1.0824 Acc=0.4839 F1=0.3068\n",
      "Epoch 30/200 | Train Loss=0.8871 Acc=0.5896 F1=0.4272 || Val Loss=1.1359 Acc=0.4839 F1=0.3162\n",
      "Epoch 31/200 | Train Loss=0.9046 Acc=0.5618 F1=0.4046 || Val Loss=1.0827 Acc=0.4839 F1=0.3168\n",
      "Epoch 32/200 | Train Loss=0.8957 Acc=0.5657 F1=0.4368 || Val Loss=1.0683 Acc=0.4677 F1=0.2980\n",
      "Epoch 33/200 | Train Loss=0.8867 Acc=0.5896 F1=0.4517 || Val Loss=1.1335 Acc=0.5323 F1=0.3672\n",
      "Epoch 34/200 | Train Loss=0.8891 Acc=0.5697 F1=0.4079 || Val Loss=1.1054 Acc=0.4839 F1=0.3068\n",
      "Epoch 35/200 | Train Loss=0.8976 Acc=0.5697 F1=0.4468 || Val Loss=1.0851 Acc=0.4839 F1=0.3162\n",
      "Epoch 36/200 | Train Loss=0.8738 Acc=0.5876 F1=0.4608 || Val Loss=1.1326 Acc=0.5323 F1=0.3672\n",
      "Epoch 37/200 | Train Loss=0.8554 Acc=0.5857 F1=0.4473 || Val Loss=1.1287 Acc=0.5161 F1=0.3343\n",
      "Epoch 38/200 | Train Loss=0.8646 Acc=0.5757 F1=0.4575 || Val Loss=1.1372 Acc=0.5484 F1=0.3704\n",
      "Epoch 39/200 | Train Loss=0.8570 Acc=0.5797 F1=0.4551 || Val Loss=1.1968 Acc=0.5323 F1=0.3527\n",
      "Epoch 40/200 | Train Loss=0.8732 Acc=0.5598 F1=0.4580 || Val Loss=1.2047 Acc=0.5000 F1=0.3155\n",
      "Epoch 41/200 | Train Loss=0.8519 Acc=0.5697 F1=0.4692 || Val Loss=1.1965 Acc=0.5645 F1=0.3873\n",
      "Epoch 42/200 | Train Loss=0.8499 Acc=0.5857 F1=0.4691 || Val Loss=1.1950 Acc=0.4839 F1=0.2949\n",
      "Epoch 43/200 | Train Loss=0.8343 Acc=0.6096 F1=0.4834 || Val Loss=1.1942 Acc=0.5161 F1=0.3433\n",
      "Epoch 44/200 | Train Loss=0.8436 Acc=0.6076 F1=0.5032 || Val Loss=1.2498 Acc=0.4839 F1=0.3064\n",
      "Epoch 45/200 | Train Loss=0.8368 Acc=0.6016 F1=0.4997 || Val Loss=1.1768 Acc=0.4839 F1=0.3068\n",
      "Epoch 46/200 | Train Loss=0.8258 Acc=0.6175 F1=0.5087 || Val Loss=1.2175 Acc=0.5323 F1=0.3726\n",
      "Epoch 47/200 | Train Loss=0.8311 Acc=0.6135 F1=0.4933 || Val Loss=1.2292 Acc=0.4839 F1=0.3064\n",
      "Epoch 48/200 | Train Loss=0.8464 Acc=0.5976 F1=0.4837 || Val Loss=1.3007 Acc=0.4677 F1=0.2866\n",
      "Epoch 49/200 | Train Loss=0.8131 Acc=0.6036 F1=0.4996 || Val Loss=1.2741 Acc=0.5323 F1=0.3771\n",
      "Epoch 50/200 | Train Loss=0.8174 Acc=0.6195 F1=0.5108 || Val Loss=1.2748 Acc=0.4839 F1=0.3064\n",
      "Epoch 51/200 | Train Loss=0.8075 Acc=0.6155 F1=0.5118 || Val Loss=1.2009 Acc=0.4839 F1=0.3064\n",
      "Epoch 52/200 | Train Loss=0.7840 Acc=0.6315 F1=0.5254 || Val Loss=1.2722 Acc=0.5323 F1=0.3672\n",
      "Epoch 53/200 | Train Loss=0.7990 Acc=0.6076 F1=0.4866 || Val Loss=1.2193 Acc=0.5484 F1=0.3761\n",
      "Epoch 54/200 | Train Loss=0.8123 Acc=0.5976 F1=0.4942 || Val Loss=1.3309 Acc=0.4677 F1=0.2978\n",
      "Epoch 55/200 | Train Loss=0.7778 Acc=0.6295 F1=0.5028 || Val Loss=1.3903 Acc=0.5000 F1=0.3339\n",
      "Epoch 56/200 | Train Loss=0.7596 Acc=0.6414 F1=0.5372 || Val Loss=1.3044 Acc=0.5000 F1=0.3339\n",
      "Epoch 57/200 | Train Loss=0.7832 Acc=0.6315 F1=0.5297 || Val Loss=1.3489 Acc=0.5000 F1=0.3412\n",
      "Epoch 58/200 | Train Loss=0.7528 Acc=0.6574 F1=0.5602 || Val Loss=1.4069 Acc=0.4677 F1=0.2978\n",
      "Epoch 59/200 | Train Loss=0.7715 Acc=0.6375 F1=0.5090 || Val Loss=1.3709 Acc=0.4677 F1=0.2978\n",
      "Epoch 60/200 | Train Loss=0.7702 Acc=0.6335 F1=0.5357 || Val Loss=1.3820 Acc=0.4677 F1=0.3072\n",
      "Epoch 61/200 | Train Loss=0.7614 Acc=0.6594 F1=0.5486 || Val Loss=1.3530 Acc=0.4516 F1=0.2892\n",
      "[INFO] Early stopping triggered at epoch 61\n",
      "[RESULT] Fold 22: Test Loss=1.2687 Acc=0.4032 F1=0.2632\n",
      "\n",
      "[INFO] Starting Fold 23/36\n",
      "Epoch 1/200 | Train Loss=1.0458 Acc=0.4371 F1=0.3117 || Val Loss=0.9841 Acc=0.4839 F1=0.2395\n",
      "Epoch 2/200 | Train Loss=1.0367 Acc=0.4651 F1=0.2696 || Val Loss=1.0012 Acc=0.5000 F1=0.3532\n",
      "Epoch 3/200 | Train Loss=1.0260 Acc=0.4591 F1=0.3353 || Val Loss=1.0157 Acc=0.5000 F1=0.3525\n",
      "Epoch 4/200 | Train Loss=1.0220 Acc=0.4810 F1=0.3278 || Val Loss=1.0166 Acc=0.5000 F1=0.2816\n",
      "Epoch 5/200 | Train Loss=1.0202 Acc=0.4551 F1=0.2833 || Val Loss=1.0171 Acc=0.5323 F1=0.3492\n",
      "Epoch 6/200 | Train Loss=1.0163 Acc=0.4790 F1=0.3419 || Val Loss=1.0157 Acc=0.5161 F1=0.3514\n",
      "Epoch 7/200 | Train Loss=1.0177 Acc=0.4870 F1=0.3479 || Val Loss=1.0179 Acc=0.5323 F1=0.3492\n",
      "Epoch 8/200 | Train Loss=1.0176 Acc=0.4651 F1=0.3270 || Val Loss=1.0128 Acc=0.5161 F1=0.3331\n",
      "Epoch 9/200 | Train Loss=1.0118 Acc=0.4631 F1=0.3255 || Val Loss=1.0153 Acc=0.5484 F1=0.3704\n",
      "Epoch 10/200 | Train Loss=1.0063 Acc=0.4711 F1=0.3422 || Val Loss=1.0191 Acc=0.5000 F1=0.3489\n",
      "Epoch 11/200 | Train Loss=1.0072 Acc=0.4770 F1=0.3442 || Val Loss=1.0081 Acc=0.5323 F1=0.3229\n",
      "Epoch 12/200 | Train Loss=1.0065 Acc=0.4770 F1=0.3293 || Val Loss=1.0114 Acc=0.5161 F1=0.3248\n",
      "Epoch 13/200 | Train Loss=1.0034 Acc=0.4830 F1=0.3403 || Val Loss=1.0105 Acc=0.5484 F1=0.3504\n",
      "Epoch 14/200 | Train Loss=1.0012 Acc=0.4810 F1=0.3361 || Val Loss=1.0155 Acc=0.5323 F1=0.3556\n",
      "Epoch 15/200 | Train Loss=0.9969 Acc=0.5010 F1=0.3628 || Val Loss=1.0187 Acc=0.5323 F1=0.3556\n",
      "Epoch 16/200 | Train Loss=0.9974 Acc=0.5070 F1=0.3639 || Val Loss=1.0186 Acc=0.5161 F1=0.3248\n",
      "Epoch 17/200 | Train Loss=0.9915 Acc=0.4870 F1=0.3536 || Val Loss=1.0246 Acc=0.5161 F1=0.3404\n",
      "Epoch 18/200 | Train Loss=0.9906 Acc=0.4890 F1=0.3870 || Val Loss=1.0272 Acc=0.4839 F1=0.3359\n",
      "Epoch 19/200 | Train Loss=0.9849 Acc=0.5150 F1=0.4072 || Val Loss=1.0155 Acc=0.5484 F1=0.3504\n",
      "Epoch 20/200 | Train Loss=0.9853 Acc=0.4950 F1=0.3996 || Val Loss=1.0233 Acc=0.4839 F1=0.3306\n",
      "Epoch 21/200 | Train Loss=0.9822 Acc=0.5190 F1=0.4258 || Val Loss=1.0290 Acc=0.5000 F1=0.3342\n",
      "Epoch 22/200 | Train Loss=0.9774 Acc=0.5309 F1=0.4369 || Val Loss=1.0416 Acc=0.4839 F1=0.3351\n",
      "Epoch 23/200 | Train Loss=0.9723 Acc=0.5030 F1=0.4174 || Val Loss=1.0435 Acc=0.5161 F1=0.3524\n",
      "Epoch 24/200 | Train Loss=0.9638 Acc=0.5030 F1=0.4202 || Val Loss=1.0370 Acc=0.4839 F1=0.3351\n",
      "Epoch 25/200 | Train Loss=0.9625 Acc=0.5250 F1=0.4354 || Val Loss=1.0507 Acc=0.5323 F1=0.3649\n",
      "Epoch 26/200 | Train Loss=0.9636 Acc=0.5349 F1=0.4609 || Val Loss=1.0388 Acc=0.5161 F1=0.3463\n",
      "Epoch 27/200 | Train Loss=0.9589 Acc=0.5170 F1=0.4483 || Val Loss=1.0519 Acc=0.4677 F1=0.3282\n",
      "Epoch 28/200 | Train Loss=0.9477 Acc=0.5469 F1=0.4724 || Val Loss=1.0619 Acc=0.5161 F1=0.3524\n",
      "Epoch 29/200 | Train Loss=0.9594 Acc=0.5329 F1=0.4720 || Val Loss=1.0459 Acc=0.5000 F1=0.3123\n",
      "[INFO] Early stopping triggered at epoch 29\n",
      "[RESULT] Fold 23: Test Loss=1.0879 Acc=0.4375 F1=0.3090\n",
      "\n",
      "[INFO] Starting Fold 24/36\n",
      "Epoch 1/200 | Train Loss=1.0300 Acc=0.4206 F1=0.2372 || Val Loss=0.9832 Acc=0.5000 F1=0.2519\n",
      "Epoch 2/200 | Train Loss=0.9622 Acc=0.4861 F1=0.3326 || Val Loss=1.0384 Acc=0.3750 F1=0.1979\n",
      "Epoch 3/200 | Train Loss=0.9610 Acc=0.4663 F1=0.2651 || Val Loss=0.9962 Acc=0.4844 F1=0.2222\n",
      "Epoch 4/200 | Train Loss=0.9551 Acc=0.4266 F1=0.2330 || Val Loss=0.9835 Acc=0.5000 F1=0.2222\n",
      "Epoch 5/200 | Train Loss=0.9450 Acc=0.5040 F1=0.3323 || Val Loss=0.9920 Acc=0.4531 F1=0.3053\n",
      "Epoch 6/200 | Train Loss=0.9467 Acc=0.4544 F1=0.2917 || Val Loss=0.9976 Acc=0.4375 F1=0.2747\n",
      "Epoch 7/200 | Train Loss=0.9406 Acc=0.4782 F1=0.3074 || Val Loss=1.0015 Acc=0.4219 F1=0.2997\n",
      "Epoch 8/200 | Train Loss=0.9361 Acc=0.4861 F1=0.3323 || Val Loss=1.0048 Acc=0.4375 F1=0.2833\n",
      "Epoch 9/200 | Train Loss=0.9364 Acc=0.5179 F1=0.3612 || Val Loss=1.0032 Acc=0.4375 F1=0.2540\n",
      "Epoch 10/200 | Train Loss=0.9343 Acc=0.4940 F1=0.3462 || Val Loss=1.0005 Acc=0.4219 F1=0.2815\n",
      "Epoch 11/200 | Train Loss=0.9286 Acc=0.5099 F1=0.3567 || Val Loss=0.9983 Acc=0.4219 F1=0.2577\n",
      "Epoch 12/200 | Train Loss=0.9304 Acc=0.5119 F1=0.3587 || Val Loss=1.0007 Acc=0.4219 F1=0.2670\n",
      "Epoch 13/200 | Train Loss=0.9271 Acc=0.5119 F1=0.3588 || Val Loss=1.0087 Acc=0.4062 F1=0.2500\n",
      "Epoch 14/200 | Train Loss=0.9251 Acc=0.5099 F1=0.3573 || Val Loss=1.0141 Acc=0.4062 F1=0.2500\n",
      "Epoch 15/200 | Train Loss=0.9246 Acc=0.5040 F1=0.3532 || Val Loss=1.0155 Acc=0.3906 F1=0.2423\n",
      "Epoch 16/200 | Train Loss=0.9192 Acc=0.5238 F1=0.3668 || Val Loss=1.0087 Acc=0.4219 F1=0.2870\n",
      "Epoch 17/200 | Train Loss=0.9175 Acc=0.5357 F1=0.3754 || Val Loss=1.0114 Acc=0.3906 F1=0.2423\n",
      "Epoch 18/200 | Train Loss=0.9171 Acc=0.5198 F1=0.3634 || Val Loss=1.0123 Acc=0.4219 F1=0.2468\n",
      "Epoch 19/200 | Train Loss=0.9217 Acc=0.5218 F1=0.3640 || Val Loss=1.0214 Acc=0.4375 F1=0.2754\n",
      "Epoch 20/200 | Train Loss=0.9176 Acc=0.5119 F1=0.3588 || Val Loss=1.0186 Acc=0.5000 F1=0.3428\n",
      "Epoch 21/200 | Train Loss=0.9078 Acc=0.5278 F1=0.3699 || Val Loss=1.0228 Acc=0.4844 F1=0.3162\n",
      "Epoch 22/200 | Train Loss=0.9081 Acc=0.5417 F1=0.3791 || Val Loss=1.0194 Acc=0.5469 F1=0.3796\n",
      "Epoch 23/200 | Train Loss=0.9095 Acc=0.5298 F1=0.3699 || Val Loss=1.0167 Acc=0.5312 F1=0.3695\n",
      "Epoch 24/200 | Train Loss=0.9104 Acc=0.5040 F1=0.3533 || Val Loss=1.0354 Acc=0.4375 F1=0.2540\n",
      "Epoch 25/200 | Train Loss=0.9017 Acc=0.5139 F1=0.3574 || Val Loss=1.0198 Acc=0.5000 F1=0.3506\n",
      "Epoch 26/200 | Train Loss=0.9010 Acc=0.5298 F1=0.3663 || Val Loss=1.0388 Acc=0.5156 F1=0.3603\n",
      "Epoch 27/200 | Train Loss=0.8969 Acc=0.5377 F1=0.3767 || Val Loss=1.0466 Acc=0.5469 F1=0.3750\n",
      "Epoch 28/200 | Train Loss=0.8992 Acc=0.5258 F1=0.3686 || Val Loss=1.0458 Acc=0.5156 F1=0.3480\n",
      "Epoch 29/200 | Train Loss=0.8918 Acc=0.5377 F1=0.3770 || Val Loss=1.0609 Acc=0.5312 F1=0.3615\n",
      "Epoch 30/200 | Train Loss=0.8913 Acc=0.5417 F1=0.3797 || Val Loss=1.0642 Acc=0.4688 F1=0.3327\n",
      "Epoch 31/200 | Train Loss=0.8805 Acc=0.5357 F1=0.3676 || Val Loss=1.0697 Acc=0.4688 F1=0.3300\n",
      "Epoch 32/200 | Train Loss=0.8854 Acc=0.5278 F1=0.3701 || Val Loss=1.0560 Acc=0.4688 F1=0.3300\n",
      "Epoch 33/200 | Train Loss=0.8754 Acc=0.5377 F1=0.3750 || Val Loss=1.0904 Acc=0.4531 F1=0.3175\n",
      "Epoch 34/200 | Train Loss=0.8736 Acc=0.5476 F1=0.3834 || Val Loss=1.0972 Acc=0.4531 F1=0.3215\n",
      "Epoch 35/200 | Train Loss=0.8663 Acc=0.5536 F1=0.3864 || Val Loss=1.1298 Acc=0.4844 F1=0.3289\n",
      "Epoch 36/200 | Train Loss=0.8695 Acc=0.5437 F1=0.3812 || Val Loss=1.1009 Acc=0.3750 F1=0.2590\n",
      "Epoch 37/200 | Train Loss=0.8656 Acc=0.5556 F1=0.3823 || Val Loss=1.1644 Acc=0.4062 F1=0.2821\n",
      "Epoch 38/200 | Train Loss=0.8635 Acc=0.5417 F1=0.3797 || Val Loss=1.1416 Acc=0.3750 F1=0.2552\n",
      "Epoch 39/200 | Train Loss=0.8531 Acc=0.5655 F1=0.3974 || Val Loss=1.1885 Acc=0.3594 F1=0.2554\n",
      "Epoch 40/200 | Train Loss=0.8475 Acc=0.5397 F1=0.3750 || Val Loss=1.1377 Acc=0.3438 F1=0.2364\n",
      "Epoch 41/200 | Train Loss=0.8296 Acc=0.5655 F1=0.3957 || Val Loss=1.1953 Acc=0.2812 F1=0.2000\n",
      "Epoch 42/200 | Train Loss=0.8468 Acc=0.5972 F1=0.4258 || Val Loss=1.2737 Acc=0.3438 F1=0.2426\n",
      "[INFO] Early stopping triggered at epoch 42\n",
      "[RESULT] Fold 24: Test Loss=1.0281 Acc=0.3770 F1=0.1997\n",
      "\n",
      "[INFO] Starting Fold 25/36\n",
      "Epoch 1/200 | Train Loss=1.0655 Acc=0.3884 F1=0.3376 || Val Loss=1.0500 Acc=0.5161 F1=0.3184\n",
      "Epoch 2/200 | Train Loss=1.0132 Acc=0.4661 F1=0.2543 || Val Loss=1.0773 Acc=0.5161 F1=0.3030\n",
      "Epoch 3/200 | Train Loss=1.0157 Acc=0.4741 F1=0.2969 || Val Loss=1.0703 Acc=0.4355 F1=0.3019\n",
      "Epoch 4/200 | Train Loss=1.0098 Acc=0.4681 F1=0.3336 || Val Loss=1.0600 Acc=0.4516 F1=0.3037\n",
      "Epoch 5/200 | Train Loss=1.0008 Acc=0.5080 F1=0.3597 || Val Loss=1.0569 Acc=0.4839 F1=0.3125\n",
      "Epoch 6/200 | Train Loss=0.9950 Acc=0.5139 F1=0.3487 || Val Loss=1.0556 Acc=0.5000 F1=0.3225\n",
      "Epoch 7/200 | Train Loss=0.9892 Acc=0.4801 F1=0.2915 || Val Loss=1.0544 Acc=0.5000 F1=0.3225\n",
      "Epoch 8/200 | Train Loss=0.9908 Acc=0.4920 F1=0.3129 || Val Loss=1.0576 Acc=0.4839 F1=0.3125\n",
      "Epoch 9/200 | Train Loss=0.9835 Acc=0.5339 F1=0.3707 || Val Loss=1.0552 Acc=0.5000 F1=0.3402\n",
      "Epoch 10/200 | Train Loss=0.9847 Acc=0.5139 F1=0.3624 || Val Loss=1.0529 Acc=0.5000 F1=0.3402\n",
      "Epoch 11/200 | Train Loss=0.9746 Acc=0.5159 F1=0.3606 || Val Loss=1.0423 Acc=0.5323 F1=0.3688\n",
      "Epoch 12/200 | Train Loss=0.9766 Acc=0.5100 F1=0.3561 || Val Loss=1.0404 Acc=0.4839 F1=0.3385\n",
      "Epoch 13/200 | Train Loss=0.9701 Acc=0.5359 F1=0.3809 || Val Loss=1.0383 Acc=0.4839 F1=0.3377\n",
      "Epoch 14/200 | Train Loss=0.9617 Acc=0.5239 F1=0.3702 || Val Loss=1.0388 Acc=0.4677 F1=0.3372\n",
      "Epoch 15/200 | Train Loss=0.9595 Acc=0.5239 F1=0.3761 || Val Loss=1.0482 Acc=0.4516 F1=0.3265\n",
      "Epoch 16/200 | Train Loss=0.9587 Acc=0.5120 F1=0.3663 || Val Loss=1.0601 Acc=0.4516 F1=0.3265\n",
      "Epoch 17/200 | Train Loss=0.9512 Acc=0.5279 F1=0.3778 || Val Loss=1.0639 Acc=0.4355 F1=0.3187\n",
      "Epoch 18/200 | Train Loss=0.9658 Acc=0.5000 F1=0.3599 || Val Loss=1.0567 Acc=0.4355 F1=0.3160\n",
      "Epoch 19/200 | Train Loss=0.9655 Acc=0.5279 F1=0.3762 || Val Loss=1.0436 Acc=0.5000 F1=0.3537\n",
      "Epoch 20/200 | Train Loss=0.9563 Acc=0.5359 F1=0.3847 || Val Loss=1.0610 Acc=0.4677 F1=0.3324\n",
      "Epoch 21/200 | Train Loss=0.9540 Acc=0.5139 F1=0.3693 || Val Loss=1.0723 Acc=0.5161 F1=0.3639\n",
      "Epoch 22/200 | Train Loss=0.9459 Acc=0.5378 F1=0.3889 || Val Loss=1.0623 Acc=0.5000 F1=0.3537\n",
      "Epoch 23/200 | Train Loss=0.9522 Acc=0.5259 F1=0.3784 || Val Loss=1.0573 Acc=0.4194 F1=0.3020\n",
      "Epoch 24/200 | Train Loss=0.9425 Acc=0.5538 F1=0.3984 || Val Loss=1.0391 Acc=0.5000 F1=0.3537\n",
      "Epoch 25/200 | Train Loss=0.9394 Acc=0.5378 F1=0.3925 || Val Loss=1.0529 Acc=0.4677 F1=0.3333\n",
      "Epoch 26/200 | Train Loss=0.9313 Acc=0.5498 F1=0.3930 || Val Loss=1.0791 Acc=0.4355 F1=0.3123\n",
      "Epoch 27/200 | Train Loss=0.9261 Acc=0.5418 F1=0.3875 || Val Loss=1.0891 Acc=0.4355 F1=0.3123\n",
      "Epoch 28/200 | Train Loss=0.9383 Acc=0.5458 F1=0.3925 || Val Loss=1.0929 Acc=0.4194 F1=0.3020\n",
      "Epoch 29/200 | Train Loss=0.9411 Acc=0.5239 F1=0.3761 || Val Loss=1.0655 Acc=0.4194 F1=0.3020\n",
      "Epoch 30/200 | Train Loss=0.9366 Acc=0.5259 F1=0.3785 || Val Loss=1.0540 Acc=0.4516 F1=0.3226\n",
      "Epoch 31/200 | Train Loss=0.9138 Acc=0.5319 F1=0.3816 || Val Loss=1.0617 Acc=0.4677 F1=0.3328\n",
      "[INFO] Early stopping triggered at epoch 31\n",
      "[RESULT] Fold 25: Test Loss=1.1755 Acc=0.3226 F1=0.2482\n",
      "\n",
      "[INFO] Starting Fold 26/36\n",
      "Epoch 1/200 | Train Loss=1.0433 Acc=0.4671 F1=0.2363 || Val Loss=0.9856 Acc=0.4032 F1=0.1916\n",
      "Epoch 2/200 | Train Loss=0.9902 Acc=0.4870 F1=0.2548 || Val Loss=0.9770 Acc=0.4839 F1=0.3321\n",
      "Epoch 3/200 | Train Loss=0.9970 Acc=0.4711 F1=0.3252 || Val Loss=0.9751 Acc=0.4516 F1=0.3175\n",
      "Epoch 4/200 | Train Loss=0.9866 Acc=0.5090 F1=0.3212 || Val Loss=0.9826 Acc=0.4032 F1=0.1916\n",
      "Epoch 5/200 | Train Loss=0.9857 Acc=0.4870 F1=0.2573 || Val Loss=0.9838 Acc=0.4032 F1=0.1916\n",
      "Epoch 6/200 | Train Loss=0.9801 Acc=0.4830 F1=0.2706 || Val Loss=0.9754 Acc=0.3871 F1=0.1860\n",
      "Epoch 7/200 | Train Loss=0.9805 Acc=0.5070 F1=0.3281 || Val Loss=0.9707 Acc=0.3710 F1=0.2317\n",
      "Epoch 8/200 | Train Loss=0.9762 Acc=0.4810 F1=0.3175 || Val Loss=0.9692 Acc=0.4677 F1=0.3258\n",
      "Epoch 9/200 | Train Loss=0.9719 Acc=0.5090 F1=0.3437 || Val Loss=0.9695 Acc=0.5000 F1=0.3491\n",
      "Epoch 10/200 | Train Loss=0.9690 Acc=0.4830 F1=0.3187 || Val Loss=0.9722 Acc=0.3710 F1=0.2222\n",
      "Epoch 11/200 | Train Loss=0.9695 Acc=0.5050 F1=0.3402 || Val Loss=0.9685 Acc=0.5323 F1=0.3718\n",
      "Epoch 12/200 | Train Loss=0.9584 Acc=0.5269 F1=0.3611 || Val Loss=0.9716 Acc=0.5000 F1=0.3534\n",
      "Epoch 13/200 | Train Loss=0.9663 Acc=0.5170 F1=0.3543 || Val Loss=0.9766 Acc=0.5161 F1=0.3566\n",
      "Epoch 14/200 | Train Loss=0.9602 Acc=0.5269 F1=0.3681 || Val Loss=0.9722 Acc=0.5161 F1=0.3566\n",
      "Epoch 15/200 | Train Loss=0.9514 Acc=0.5110 F1=0.3527 || Val Loss=0.9858 Acc=0.4516 F1=0.3155\n",
      "Epoch 16/200 | Train Loss=0.9549 Acc=0.5230 F1=0.3524 || Val Loss=0.9831 Acc=0.5000 F1=0.3384\n",
      "Epoch 17/200 | Train Loss=0.9411 Acc=0.5190 F1=0.3665 || Val Loss=0.9672 Acc=0.5484 F1=0.3434\n",
      "Epoch 18/200 | Train Loss=0.9414 Acc=0.5289 F1=0.3785 || Val Loss=0.9882 Acc=0.5161 F1=0.3345\n",
      "Epoch 19/200 | Train Loss=0.9390 Acc=0.5429 F1=0.3775 || Val Loss=0.9938 Acc=0.4516 F1=0.3128\n",
      "Epoch 20/200 | Train Loss=0.9392 Acc=0.5409 F1=0.3765 || Val Loss=0.9760 Acc=0.5161 F1=0.3419\n",
      "Epoch 21/200 | Train Loss=0.9410 Acc=0.5389 F1=0.3822 || Val Loss=1.0009 Acc=0.5484 F1=0.3675\n",
      "Epoch 22/200 | Train Loss=0.9365 Acc=0.5150 F1=0.3575 || Val Loss=0.9894 Acc=0.4677 F1=0.3225\n",
      "Epoch 23/200 | Train Loss=0.9288 Acc=0.5329 F1=0.3766 || Val Loss=0.9870 Acc=0.5645 F1=0.3622\n",
      "Epoch 24/200 | Train Loss=0.9267 Acc=0.5210 F1=0.3714 || Val Loss=1.0006 Acc=0.5484 F1=0.3528\n",
      "Epoch 25/200 | Train Loss=0.9233 Acc=0.5389 F1=0.3916 || Val Loss=1.0106 Acc=0.4677 F1=0.3286\n",
      "Epoch 26/200 | Train Loss=0.9184 Acc=0.5349 F1=0.3857 || Val Loss=1.0276 Acc=0.5323 F1=0.3578\n",
      "Epoch 27/200 | Train Loss=0.9177 Acc=0.5449 F1=0.4090 || Val Loss=1.0492 Acc=0.5484 F1=0.3608\n",
      "Epoch 28/200 | Train Loss=0.9165 Acc=0.5469 F1=0.3875 || Val Loss=1.0050 Acc=0.5484 F1=0.3528\n",
      "Epoch 29/200 | Train Loss=0.9132 Acc=0.5269 F1=0.3814 || Val Loss=1.0046 Acc=0.5323 F1=0.3578\n",
      "Epoch 30/200 | Train Loss=0.9032 Acc=0.5489 F1=0.4067 || Val Loss=1.0158 Acc=0.5161 F1=0.3345\n",
      "Epoch 31/200 | Train Loss=0.8912 Acc=0.5309 F1=0.3814 || Val Loss=1.0766 Acc=0.5806 F1=0.3872\n",
      "Epoch 32/200 | Train Loss=0.8996 Acc=0.5289 F1=0.3911 || Val Loss=1.0181 Acc=0.5161 F1=0.3481\n",
      "Epoch 33/200 | Train Loss=0.8901 Acc=0.5629 F1=0.4093 || Val Loss=1.0527 Acc=0.5161 F1=0.3473\n",
      "Epoch 34/200 | Train Loss=0.8897 Acc=0.5489 F1=0.4139 || Val Loss=1.0431 Acc=0.5484 F1=0.3608\n",
      "Epoch 35/200 | Train Loss=0.8946 Acc=0.5549 F1=0.4314 || Val Loss=1.0481 Acc=0.5161 F1=0.3566\n",
      "Epoch 36/200 | Train Loss=0.8790 Acc=0.5649 F1=0.4615 || Val Loss=1.0235 Acc=0.5323 F1=0.3436\n",
      "Epoch 37/200 | Train Loss=0.8849 Acc=0.5589 F1=0.4500 || Val Loss=1.0446 Acc=0.5645 F1=0.3761\n",
      "Epoch 38/200 | Train Loss=0.8853 Acc=0.5429 F1=0.3908 || Val Loss=1.0427 Acc=0.4516 F1=0.3192\n",
      "Epoch 39/200 | Train Loss=0.8674 Acc=0.5649 F1=0.4590 || Val Loss=1.0869 Acc=0.5161 F1=0.3257\n",
      "Epoch 40/200 | Train Loss=0.8923 Acc=0.5529 F1=0.4178 || Val Loss=1.1016 Acc=0.5000 F1=0.3465\n",
      "Epoch 41/200 | Train Loss=0.8702 Acc=0.5709 F1=0.4540 || Val Loss=1.0286 Acc=0.5323 F1=0.3436\n",
      "Epoch 42/200 | Train Loss=0.8528 Acc=0.5729 F1=0.4483 || Val Loss=1.1110 Acc=0.4839 F1=0.3231\n",
      "Epoch 43/200 | Train Loss=0.8616 Acc=0.5549 F1=0.4300 || Val Loss=1.0809 Acc=0.4839 F1=0.3370\n",
      "Epoch 44/200 | Train Loss=0.8535 Acc=0.5709 F1=0.4383 || Val Loss=1.1191 Acc=0.5000 F1=0.3384\n",
      "Epoch 45/200 | Train Loss=0.8456 Acc=0.5788 F1=0.4745 || Val Loss=1.1012 Acc=0.5000 F1=0.3384\n",
      "Epoch 46/200 | Train Loss=0.8473 Acc=0.5749 F1=0.4342 || Val Loss=1.1382 Acc=0.5161 F1=0.3574\n",
      "Epoch 47/200 | Train Loss=0.8337 Acc=0.5689 F1=0.4576 || Val Loss=1.0757 Acc=0.4839 F1=0.3231\n",
      "Epoch 48/200 | Train Loss=0.8357 Acc=0.5848 F1=0.4671 || Val Loss=1.0967 Acc=0.5000 F1=0.3518\n",
      "Epoch 49/200 | Train Loss=0.8274 Acc=0.5848 F1=0.4613 || Val Loss=1.1464 Acc=0.4516 F1=0.2818\n",
      "Epoch 50/200 | Train Loss=0.8232 Acc=0.5928 F1=0.5182 || Val Loss=1.1095 Acc=0.5806 F1=0.4062\n",
      "Epoch 51/200 | Train Loss=0.8157 Acc=0.6168 F1=0.4995 || Val Loss=1.1724 Acc=0.5323 F1=0.3632\n",
      "Epoch 52/200 | Train Loss=0.8059 Acc=0.5808 F1=0.4873 || Val Loss=1.1171 Acc=0.5161 F1=0.3644\n",
      "Epoch 53/200 | Train Loss=0.7886 Acc=0.6048 F1=0.4909 || Val Loss=1.1578 Acc=0.5161 F1=0.3345\n",
      "Epoch 54/200 | Train Loss=0.8158 Acc=0.5808 F1=0.4896 || Val Loss=1.1387 Acc=0.5323 F1=0.3696\n",
      "Epoch 55/200 | Train Loss=0.7887 Acc=0.5948 F1=0.4754 || Val Loss=1.1840 Acc=0.5323 F1=0.3616\n",
      "Epoch 56/200 | Train Loss=0.7712 Acc=0.6108 F1=0.5196 || Val Loss=1.1942 Acc=0.4839 F1=0.3231\n",
      "Epoch 57/200 | Train Loss=0.7904 Acc=0.6128 F1=0.5108 || Val Loss=0.9971 Acc=0.5645 F1=0.3773\n",
      "Epoch 58/200 | Train Loss=0.7876 Acc=0.6248 F1=0.5333 || Val Loss=1.0943 Acc=0.5161 F1=0.3473\n",
      "Epoch 59/200 | Train Loss=0.7748 Acc=0.6188 F1=0.5298 || Val Loss=1.2193 Acc=0.5484 F1=0.3675\n",
      "Epoch 60/200 | Train Loss=0.7794 Acc=0.6168 F1=0.5330 || Val Loss=1.2474 Acc=0.4839 F1=0.3281\n",
      "Epoch 61/200 | Train Loss=0.7584 Acc=0.6567 F1=0.5761 || Val Loss=1.2604 Acc=0.4839 F1=0.3287\n",
      "Epoch 62/200 | Train Loss=0.7781 Acc=0.6128 F1=0.5017 || Val Loss=1.1842 Acc=0.5323 F1=0.3632\n",
      "Epoch 63/200 | Train Loss=0.7509 Acc=0.6327 F1=0.5304 || Val Loss=1.2876 Acc=0.5161 F1=0.3411\n",
      "Epoch 64/200 | Train Loss=0.7485 Acc=0.6407 F1=0.5270 || Val Loss=1.1618 Acc=0.5806 F1=0.4044\n",
      "Epoch 65/200 | Train Loss=0.7506 Acc=0.6567 F1=0.5981 || Val Loss=1.2323 Acc=0.5484 F1=0.3767\n",
      "Epoch 66/200 | Train Loss=0.7584 Acc=0.6327 F1=0.5099 || Val Loss=1.1621 Acc=0.5484 F1=0.3608\n",
      "Epoch 67/200 | Train Loss=0.7575 Acc=0.6507 F1=0.5597 || Val Loss=1.1524 Acc=0.5161 F1=0.3481\n",
      "Epoch 68/200 | Train Loss=0.7348 Acc=0.6287 F1=0.5340 || Val Loss=1.2739 Acc=0.5323 F1=0.3667\n",
      "Epoch 69/200 | Train Loss=0.7308 Acc=0.6347 F1=0.5359 || Val Loss=1.2807 Acc=0.5323 F1=0.3345\n",
      "Epoch 70/200 | Train Loss=0.7558 Acc=0.6267 F1=0.5813 || Val Loss=1.1954 Acc=0.5000 F1=0.3318\n",
      "[INFO] Early stopping triggered at epoch 70\n",
      "[RESULT] Fold 26: Test Loss=0.9514 Acc=0.5469 F1=0.3697\n",
      "\n",
      "[INFO] Starting Fold 27/36\n",
      "Epoch 1/200 | Train Loss=1.0642 Acc=0.3690 F1=0.2984 || Val Loss=1.0023 Acc=0.4375 F1=0.2556\n",
      "Epoch 2/200 | Train Loss=0.9040 Acc=0.5000 F1=0.2745 || Val Loss=1.0607 Acc=0.4219 F1=0.2000\n",
      "Epoch 3/200 | Train Loss=0.9005 Acc=0.5060 F1=0.3267 || Val Loss=1.0935 Acc=0.4062 F1=0.2298\n",
      "Epoch 4/200 | Train Loss=0.8988 Acc=0.5298 F1=0.3168 || Val Loss=1.0734 Acc=0.4219 F1=0.2353\n",
      "Epoch 5/200 | Train Loss=0.8890 Acc=0.5258 F1=0.3185 || Val Loss=1.0440 Acc=0.3906 F1=0.2530\n",
      "Epoch 6/200 | Train Loss=0.8906 Acc=0.5238 F1=0.3353 || Val Loss=1.0286 Acc=0.3906 F1=0.2218\n",
      "Epoch 7/200 | Train Loss=0.8887 Acc=0.5099 F1=0.2842 || Val Loss=1.0391 Acc=0.4531 F1=0.2486\n",
      "Epoch 8/200 | Train Loss=0.8819 Acc=0.5198 F1=0.3289 || Val Loss=1.0469 Acc=0.3750 F1=0.2568\n",
      "Epoch 9/200 | Train Loss=0.8827 Acc=0.5357 F1=0.3527 || Val Loss=1.0586 Acc=0.4062 F1=0.2687\n",
      "Epoch 10/200 | Train Loss=0.8791 Acc=0.5119 F1=0.3219 || Val Loss=1.0564 Acc=0.3750 F1=0.2513\n",
      "Epoch 11/200 | Train Loss=0.8784 Acc=0.5417 F1=0.3608 || Val Loss=1.0449 Acc=0.3438 F1=0.2467\n",
      "Epoch 12/200 | Train Loss=0.8766 Acc=0.5258 F1=0.3451 || Val Loss=1.0383 Acc=0.3906 F1=0.2601\n",
      "Epoch 13/200 | Train Loss=0.8733 Acc=0.5317 F1=0.3499 || Val Loss=1.0394 Acc=0.3594 F1=0.2577\n",
      "Epoch 14/200 | Train Loss=0.8691 Acc=0.5377 F1=0.3538 || Val Loss=1.0406 Acc=0.3750 F1=0.2568\n",
      "Epoch 15/200 | Train Loss=0.8690 Acc=0.5397 F1=0.3517 || Val Loss=1.0391 Acc=0.3594 F1=0.2553\n",
      "Epoch 16/200 | Train Loss=0.8677 Acc=0.5198 F1=0.3560 || Val Loss=1.0371 Acc=0.3281 F1=0.2290\n",
      "Epoch 17/200 | Train Loss=0.8672 Acc=0.5218 F1=0.3471 || Val Loss=1.0382 Acc=0.3438 F1=0.2420\n",
      "Epoch 18/200 | Train Loss=0.8588 Acc=0.5516 F1=0.3637 || Val Loss=1.0331 Acc=0.3750 F1=0.2684\n",
      "Epoch 19/200 | Train Loss=0.8600 Acc=0.5536 F1=0.3693 || Val Loss=1.0354 Acc=0.3906 F1=0.2792\n",
      "Epoch 20/200 | Train Loss=0.8546 Acc=0.5595 F1=0.3811 || Val Loss=1.0375 Acc=0.3125 F1=0.2109\n",
      "Epoch 21/200 | Train Loss=0.8665 Acc=0.5397 F1=0.3575 || Val Loss=1.0396 Acc=0.3750 F1=0.2670\n",
      "Epoch 22/200 | Train Loss=0.8544 Acc=0.5615 F1=0.3799 || Val Loss=1.0526 Acc=0.3125 F1=0.2109\n",
      "Epoch 23/200 | Train Loss=0.8528 Acc=0.5397 F1=0.3695 || Val Loss=1.0446 Acc=0.3438 F1=0.2447\n",
      "Epoch 24/200 | Train Loss=0.8522 Acc=0.5575 F1=0.3701 || Val Loss=1.0353 Acc=0.3750 F1=0.2672\n",
      "Epoch 25/200 | Train Loss=0.8500 Acc=0.5694 F1=0.3887 || Val Loss=1.0378 Acc=0.3125 F1=0.2095\n",
      "Epoch 26/200 | Train Loss=0.8478 Acc=0.5575 F1=0.3833 || Val Loss=1.0343 Acc=0.3438 F1=0.2461\n",
      "Epoch 27/200 | Train Loss=0.8486 Acc=0.5754 F1=0.3768 || Val Loss=1.0409 Acc=0.3750 F1=0.2614\n",
      "Epoch 28/200 | Train Loss=0.8455 Acc=0.5437 F1=0.3711 || Val Loss=1.0547 Acc=0.3594 F1=0.2274\n",
      "Epoch 29/200 | Train Loss=0.8469 Acc=0.5675 F1=0.3902 || Val Loss=1.0357 Acc=0.3594 F1=0.2549\n",
      "Epoch 30/200 | Train Loss=0.8412 Acc=0.5675 F1=0.3781 || Val Loss=1.0275 Acc=0.3594 F1=0.2574\n",
      "Epoch 31/200 | Train Loss=0.8386 Acc=0.5556 F1=0.3799 || Val Loss=1.0366 Acc=0.3438 F1=0.2331\n",
      "Epoch 32/200 | Train Loss=0.8363 Acc=0.5774 F1=0.3917 || Val Loss=1.0395 Acc=0.3906 F1=0.2746\n",
      "Epoch 33/200 | Train Loss=0.8394 Acc=0.5833 F1=0.3895 || Val Loss=1.0417 Acc=0.3125 F1=0.2235\n",
      "Epoch 34/200 | Train Loss=0.8305 Acc=0.5714 F1=0.3915 || Val Loss=1.0433 Acc=0.3594 F1=0.2474\n",
      "Epoch 35/200 | Train Loss=0.8229 Acc=0.5754 F1=0.3939 || Val Loss=1.0376 Acc=0.3594 F1=0.2581\n",
      "Epoch 36/200 | Train Loss=0.8339 Acc=0.5754 F1=0.3821 || Val Loss=1.0335 Acc=0.3594 F1=0.2579\n",
      "Epoch 37/200 | Train Loss=0.8261 Acc=0.5536 F1=0.3797 || Val Loss=1.0285 Acc=0.3438 F1=0.2419\n",
      "Epoch 38/200 | Train Loss=0.8224 Acc=0.5694 F1=0.3896 || Val Loss=1.0213 Acc=0.3906 F1=0.2752\n",
      "Epoch 39/200 | Train Loss=0.8126 Acc=0.5893 F1=0.3925 || Val Loss=1.0283 Acc=0.3125 F1=0.2239\n",
      "[INFO] Early stopping triggered at epoch 39\n",
      "[RESULT] Fold 27: Test Loss=0.9182 Acc=0.3934 F1=0.2571\n",
      "\n",
      "[INFO] Starting Fold 28/36\n",
      "Epoch 1/200 | Train Loss=1.0548 Acc=0.3406 F1=0.2910 || Val Loss=0.9861 Acc=0.4516 F1=0.2675\n",
      "Epoch 2/200 | Train Loss=0.9873 Acc=0.4104 F1=0.3109 || Val Loss=0.9869 Acc=0.4516 F1=0.3524\n",
      "Epoch 3/200 | Train Loss=0.9838 Acc=0.4183 F1=0.2966 || Val Loss=0.9826 Acc=0.4516 F1=0.2883\n",
      "Epoch 4/200 | Train Loss=0.9751 Acc=0.4741 F1=0.3049 || Val Loss=0.9817 Acc=0.4355 F1=0.3010\n",
      "Epoch 5/200 | Train Loss=0.9664 Acc=0.4920 F1=0.3486 || Val Loss=0.9826 Acc=0.4032 F1=0.3438\n",
      "Epoch 6/200 | Train Loss=0.9677 Acc=0.4183 F1=0.3091 || Val Loss=0.9841 Acc=0.4677 F1=0.3841\n",
      "Epoch 7/200 | Train Loss=0.9696 Acc=0.4701 F1=0.3432 || Val Loss=0.9841 Acc=0.4677 F1=0.3751\n",
      "Epoch 8/200 | Train Loss=0.9653 Acc=0.4681 F1=0.3343 || Val Loss=0.9855 Acc=0.4355 F1=0.3657\n",
      "Epoch 9/200 | Train Loss=0.9612 Acc=0.4761 F1=0.3400 || Val Loss=0.9879 Acc=0.4516 F1=0.3797\n",
      "Epoch 10/200 | Train Loss=0.9538 Acc=0.5000 F1=0.3665 || Val Loss=0.9897 Acc=0.4839 F1=0.4009\n",
      "Epoch 11/200 | Train Loss=0.9513 Acc=0.5020 F1=0.3578 || Val Loss=0.9914 Acc=0.4355 F1=0.3709\n",
      "Epoch 12/200 | Train Loss=0.9520 Acc=0.4801 F1=0.3284 || Val Loss=0.9925 Acc=0.4839 F1=0.4037\n",
      "Epoch 13/200 | Train Loss=0.9470 Acc=0.5040 F1=0.3563 || Val Loss=0.9963 Acc=0.4677 F1=0.3904\n",
      "Epoch 14/200 | Train Loss=0.9489 Acc=0.4761 F1=0.3392 || Val Loss=0.9990 Acc=0.4355 F1=0.3690\n",
      "Epoch 15/200 | Train Loss=0.9361 Acc=0.5020 F1=0.3569 || Val Loss=1.0024 Acc=0.5000 F1=0.4114\n",
      "Epoch 16/200 | Train Loss=0.9375 Acc=0.5299 F1=0.3761 || Val Loss=1.0064 Acc=0.4516 F1=0.3815\n",
      "Epoch 17/200 | Train Loss=0.9369 Acc=0.4861 F1=0.3428 || Val Loss=1.0089 Acc=0.4516 F1=0.3815\n",
      "Epoch 18/200 | Train Loss=0.9355 Acc=0.5199 F1=0.3694 || Val Loss=1.0163 Acc=0.5000 F1=0.4114\n",
      "Epoch 19/200 | Train Loss=0.9297 Acc=0.5239 F1=0.3716 || Val Loss=1.0171 Acc=0.4839 F1=0.4096\n",
      "Epoch 20/200 | Train Loss=0.9288 Acc=0.5060 F1=0.3549 || Val Loss=1.0207 Acc=0.4677 F1=0.3971\n",
      "Epoch 21/200 | Train Loss=0.9331 Acc=0.5299 F1=0.3735 || Val Loss=1.0258 Acc=0.5161 F1=0.4293\n",
      "Epoch 22/200 | Train Loss=0.9283 Acc=0.5139 F1=0.3610 || Val Loss=1.0198 Acc=0.4677 F1=0.3333\n",
      "Epoch 23/200 | Train Loss=0.9225 Acc=0.5438 F1=0.3846 || Val Loss=1.0289 Acc=0.4839 F1=0.3387\n",
      "Epoch 24/200 | Train Loss=0.9223 Acc=0.5458 F1=0.3852 || Val Loss=1.0196 Acc=0.4516 F1=0.3215\n",
      "Epoch 25/200 | Train Loss=0.9141 Acc=0.5219 F1=0.3647 || Val Loss=1.0255 Acc=0.5161 F1=0.3664\n",
      "Epoch 26/200 | Train Loss=0.9114 Acc=0.5359 F1=0.3788 || Val Loss=1.0275 Acc=0.4839 F1=0.3446\n",
      "Epoch 27/200 | Train Loss=0.9164 Acc=0.5438 F1=0.3829 || Val Loss=1.0343 Acc=0.5161 F1=0.3620\n",
      "Epoch 28/200 | Train Loss=0.9057 Acc=0.5498 F1=0.3888 || Val Loss=1.0350 Acc=0.4677 F1=0.3332\n",
      "Epoch 29/200 | Train Loss=0.9020 Acc=0.5518 F1=0.3885 || Val Loss=1.0535 Acc=0.4677 F1=0.3324\n",
      "Epoch 30/200 | Train Loss=0.9148 Acc=0.5418 F1=0.3822 || Val Loss=1.0456 Acc=0.4677 F1=0.3284\n",
      "Epoch 31/200 | Train Loss=0.8997 Acc=0.5777 F1=0.4088 || Val Loss=1.0491 Acc=0.5000 F1=0.3540\n",
      "Epoch 32/200 | Train Loss=0.8886 Acc=0.5837 F1=0.4126 || Val Loss=1.0587 Acc=0.5323 F1=0.3786\n",
      "Epoch 33/200 | Train Loss=0.9002 Acc=0.5637 F1=0.3975 || Val Loss=1.0491 Acc=0.5000 F1=0.3487\n",
      "Epoch 34/200 | Train Loss=0.8929 Acc=0.5697 F1=0.4031 || Val Loss=1.0546 Acc=0.4839 F1=0.3351\n",
      "Epoch 35/200 | Train Loss=0.8754 Acc=0.5857 F1=0.4135 || Val Loss=1.0783 Acc=0.4677 F1=0.3325\n",
      "Epoch 36/200 | Train Loss=0.8856 Acc=0.5916 F1=0.4173 || Val Loss=1.0699 Acc=0.4516 F1=0.3183\n",
      "Epoch 37/200 | Train Loss=0.8736 Acc=0.5916 F1=0.4185 || Val Loss=1.0950 Acc=0.4355 F1=0.2974\n",
      "Epoch 38/200 | Train Loss=0.8667 Acc=0.5976 F1=0.4212 || Val Loss=1.0748 Acc=0.4032 F1=0.2873\n",
      "Epoch 39/200 | Train Loss=0.8713 Acc=0.5976 F1=0.4215 || Val Loss=1.1423 Acc=0.4194 F1=0.2971\n",
      "Epoch 40/200 | Train Loss=0.8549 Acc=0.5916 F1=0.4186 || Val Loss=1.0835 Acc=0.4194 F1=0.2973\n",
      "Epoch 41/200 | Train Loss=0.8699 Acc=0.5837 F1=0.4069 || Val Loss=1.1461 Acc=0.4355 F1=0.3018\n",
      "[INFO] Early stopping triggered at epoch 41\n",
      "[RESULT] Fold 28: Test Loss=1.0752 Acc=0.4194 F1=0.2950\n",
      "\n",
      "[INFO] Starting Fold 29/36\n",
      "Epoch 1/200 | Train Loss=1.0497 Acc=0.4112 F1=0.3109 || Val Loss=1.0628 Acc=0.4194 F1=0.1970\n",
      "Epoch 2/200 | Train Loss=1.0209 Acc=0.4571 F1=0.2242 || Val Loss=1.0514 Acc=0.4355 F1=0.2022\n",
      "Epoch 3/200 | Train Loss=1.0139 Acc=0.4930 F1=0.3376 || Val Loss=1.0396 Acc=0.4516 F1=0.2893\n",
      "Epoch 4/200 | Train Loss=1.0104 Acc=0.4631 F1=0.3196 || Val Loss=1.0399 Acc=0.4355 F1=0.2022\n",
      "Epoch 5/200 | Train Loss=1.0075 Acc=0.4770 F1=0.2722 || Val Loss=1.0463 Acc=0.4355 F1=0.2022\n",
      "Epoch 6/200 | Train Loss=1.0023 Acc=0.4611 F1=0.2197 || Val Loss=1.0560 Acc=0.4355 F1=0.2022\n",
      "Epoch 7/200 | Train Loss=1.0017 Acc=0.4631 F1=0.2110 || Val Loss=1.0492 Acc=0.4355 F1=0.2022\n",
      "Epoch 8/200 | Train Loss=0.9959 Acc=0.4790 F1=0.3047 || Val Loss=1.0408 Acc=0.4839 F1=0.3323\n",
      "Epoch 9/200 | Train Loss=0.9961 Acc=0.4990 F1=0.3562 || Val Loss=1.0409 Acc=0.4194 F1=0.2495\n",
      "Epoch 10/200 | Train Loss=0.9901 Acc=0.4910 F1=0.3277 || Val Loss=1.0448 Acc=0.4355 F1=0.2022\n",
      "Epoch 11/200 | Train Loss=0.9954 Acc=0.4790 F1=0.2623 || Val Loss=1.0506 Acc=0.4355 F1=0.2022\n",
      "Epoch 12/200 | Train Loss=0.9906 Acc=0.4790 F1=0.2823 || Val Loss=1.0460 Acc=0.4194 F1=0.1970\n",
      "Epoch 13/200 | Train Loss=0.9867 Acc=0.4950 F1=0.3318 || Val Loss=1.0446 Acc=0.4355 F1=0.2421\n",
      "Epoch 14/200 | Train Loss=0.9852 Acc=0.5150 F1=0.3580 || Val Loss=1.0441 Acc=0.4194 F1=0.2176\n",
      "Epoch 15/200 | Train Loss=0.9849 Acc=0.5050 F1=0.3350 || Val Loss=1.0477 Acc=0.4194 F1=0.1970\n",
      "Epoch 16/200 | Train Loss=0.9843 Acc=0.4950 F1=0.3209 || Val Loss=1.0489 Acc=0.4194 F1=0.2176\n",
      "Epoch 17/200 | Train Loss=0.9843 Acc=0.4990 F1=0.3482 || Val Loss=1.0505 Acc=0.5000 F1=0.3421\n",
      "Epoch 18/200 | Train Loss=0.9709 Acc=0.5150 F1=0.3516 || Val Loss=1.0467 Acc=0.4355 F1=0.2421\n",
      "Epoch 19/200 | Train Loss=0.9758 Acc=0.5190 F1=0.3558 || Val Loss=1.0456 Acc=0.4516 F1=0.2893\n",
      "Epoch 20/200 | Train Loss=0.9709 Acc=0.5269 F1=0.3583 || Val Loss=1.0491 Acc=0.4516 F1=0.2651\n",
      "Epoch 21/200 | Train Loss=0.9689 Acc=0.5190 F1=0.3444 || Val Loss=1.0459 Acc=0.4677 F1=0.2866\n",
      "Epoch 22/200 | Train Loss=0.9667 Acc=0.5389 F1=0.3799 || Val Loss=1.0393 Acc=0.4677 F1=0.3225\n",
      "Epoch 23/200 | Train Loss=0.9617 Acc=0.5389 F1=0.3799 || Val Loss=1.0455 Acc=0.4516 F1=0.2499\n",
      "Epoch 24/200 | Train Loss=0.9621 Acc=0.5150 F1=0.3434 || Val Loss=1.0458 Acc=0.4677 F1=0.3072\n",
      "Epoch 25/200 | Train Loss=0.9672 Acc=0.5269 F1=0.3777 || Val Loss=1.0455 Acc=0.4677 F1=0.3219\n",
      "Epoch 26/200 | Train Loss=0.9632 Acc=0.5349 F1=0.3739 || Val Loss=1.0527 Acc=0.4355 F1=0.2239\n",
      "Epoch 27/200 | Train Loss=0.9576 Acc=0.5070 F1=0.3208 || Val Loss=1.0403 Acc=0.4516 F1=0.2892\n",
      "Epoch 28/200 | Train Loss=0.9615 Acc=0.5389 F1=0.3938 || Val Loss=1.0455 Acc=0.4194 F1=0.3000\n",
      "Epoch 29/200 | Train Loss=0.9607 Acc=0.5170 F1=0.3556 || Val Loss=1.0551 Acc=0.4355 F1=0.2254\n",
      "Epoch 30/200 | Train Loss=0.9495 Acc=0.5250 F1=0.3371 || Val Loss=1.0388 Acc=0.4677 F1=0.2734\n",
      "Epoch 31/200 | Train Loss=0.9344 Acc=0.5549 F1=0.4082 || Val Loss=1.0237 Acc=0.4677 F1=0.3346\n",
      "Epoch 32/200 | Train Loss=0.9434 Acc=0.5309 F1=0.4050 || Val Loss=1.0351 Acc=0.4355 F1=0.3024\n",
      "Epoch 33/200 | Train Loss=0.9400 Acc=0.5529 F1=0.3826 || Val Loss=1.0533 Acc=0.4516 F1=0.2658\n",
      "Epoch 34/200 | Train Loss=0.9283 Acc=0.5629 F1=0.3832 || Val Loss=1.0414 Acc=0.4677 F1=0.2978\n",
      "Epoch 35/200 | Train Loss=0.9278 Acc=0.5768 F1=0.4165 || Val Loss=1.0282 Acc=0.4516 F1=0.3122\n",
      "Epoch 36/200 | Train Loss=0.9264 Acc=0.5709 F1=0.4241 || Val Loss=1.0340 Acc=0.4516 F1=0.3059\n",
      "Epoch 37/200 | Train Loss=0.9150 Acc=0.5749 F1=0.4316 || Val Loss=1.0455 Acc=0.4194 F1=0.2719\n",
      "[INFO] Early stopping triggered at epoch 37\n",
      "[RESULT] Fold 29: Test Loss=0.9469 Acc=0.4375 F1=0.2997\n",
      "\n",
      "[INFO] Starting Fold 30/36\n",
      "Epoch 1/200 | Train Loss=1.0342 Acc=0.3968 F1=0.3059 || Val Loss=0.9235 Acc=0.4531 F1=0.2871\n",
      "Epoch 2/200 | Train Loss=0.9789 Acc=0.4583 F1=0.2916 || Val Loss=0.9079 Acc=0.4531 F1=0.2871\n",
      "Epoch 3/200 | Train Loss=0.9785 Acc=0.4702 F1=0.2692 || Val Loss=0.9019 Acc=0.4531 F1=0.2871\n",
      "Epoch 4/200 | Train Loss=0.9682 Acc=0.4782 F1=0.2711 || Val Loss=0.9324 Acc=0.4688 F1=0.2954\n",
      "Epoch 5/200 | Train Loss=0.9683 Acc=0.4841 F1=0.2822 || Val Loss=0.9308 Acc=0.4531 F1=0.2871\n",
      "Epoch 6/200 | Train Loss=0.9662 Acc=0.4603 F1=0.3038 || Val Loss=0.9085 Acc=0.4688 F1=0.3015\n",
      "Epoch 7/200 | Train Loss=0.9601 Acc=0.4782 F1=0.3265 || Val Loss=0.9096 Acc=0.4531 F1=0.2871\n",
      "Epoch 8/200 | Train Loss=0.9615 Acc=0.4702 F1=0.2831 || Val Loss=0.9193 Acc=0.4688 F1=0.2954\n",
      "Epoch 9/200 | Train Loss=0.9587 Acc=0.4742 F1=0.2676 || Val Loss=0.9256 Acc=0.4531 F1=0.2871\n",
      "Epoch 10/200 | Train Loss=0.9559 Acc=0.4762 F1=0.2976 || Val Loss=0.9160 Acc=0.4688 F1=0.3015\n",
      "Epoch 11/200 | Train Loss=0.9561 Acc=0.4921 F1=0.3354 || Val Loss=0.9248 Acc=0.4688 F1=0.3015\n",
      "Epoch 12/200 | Train Loss=0.9469 Acc=0.5040 F1=0.3422 || Val Loss=0.9256 Acc=0.4531 F1=0.2929\n",
      "Epoch 13/200 | Train Loss=0.9473 Acc=0.5060 F1=0.3415 || Val Loss=0.9253 Acc=0.4531 F1=0.2929\n",
      "Epoch 14/200 | Train Loss=0.9434 Acc=0.4861 F1=0.3097 || Val Loss=0.9335 Acc=0.4531 F1=0.2929\n",
      "Epoch 15/200 | Train Loss=0.9454 Acc=0.4821 F1=0.3161 || Val Loss=0.9230 Acc=0.4531 F1=0.3048\n",
      "Epoch 16/200 | Train Loss=0.9400 Acc=0.5099 F1=0.3506 || Val Loss=0.9257 Acc=0.4375 F1=0.2925\n",
      "Epoch 17/200 | Train Loss=0.9382 Acc=0.5119 F1=0.3487 || Val Loss=0.9384 Acc=0.4531 F1=0.2929\n",
      "Epoch 18/200 | Train Loss=0.9386 Acc=0.4980 F1=0.3241 || Val Loss=0.9323 Acc=0.4375 F1=0.2843\n",
      "Epoch 19/200 | Train Loss=0.9365 Acc=0.5000 F1=0.3405 || Val Loss=0.9321 Acc=0.4219 F1=0.2831\n",
      "Epoch 20/200 | Train Loss=0.9314 Acc=0.5099 F1=0.3560 || Val Loss=0.9319 Acc=0.4219 F1=0.2831\n",
      "Epoch 21/200 | Train Loss=0.9256 Acc=0.5278 F1=0.3720 || Val Loss=0.9343 Acc=0.4531 F1=0.3071\n",
      "Epoch 22/200 | Train Loss=0.9269 Acc=0.5040 F1=0.3440 || Val Loss=0.9476 Acc=0.4688 F1=0.3067\n",
      "Epoch 23/200 | Train Loss=0.9195 Acc=0.5099 F1=0.3402 || Val Loss=0.9414 Acc=0.4688 F1=0.3143\n",
      "Epoch 24/200 | Train Loss=0.9136 Acc=0.5298 F1=0.3669 || Val Loss=0.9477 Acc=0.4375 F1=0.2984\n",
      "Epoch 25/200 | Train Loss=0.9134 Acc=0.5357 F1=0.3777 || Val Loss=0.9529 Acc=0.4531 F1=0.3094\n",
      "Epoch 26/200 | Train Loss=0.9033 Acc=0.5417 F1=0.3881 || Val Loss=0.9503 Acc=0.4688 F1=0.3198\n",
      "Epoch 27/200 | Train Loss=0.9025 Acc=0.5476 F1=0.3958 || Val Loss=0.9611 Acc=0.4688 F1=0.3169\n",
      "Epoch 28/200 | Train Loss=0.9058 Acc=0.5298 F1=0.3696 || Val Loss=0.9664 Acc=0.4219 F1=0.2872\n",
      "Epoch 29/200 | Train Loss=0.8944 Acc=0.5556 F1=0.4085 || Val Loss=0.9785 Acc=0.4062 F1=0.2770\n",
      "Epoch 30/200 | Train Loss=0.8888 Acc=0.5258 F1=0.3907 || Val Loss=0.9676 Acc=0.4219 F1=0.2881\n",
      "Epoch 31/200 | Train Loss=0.8922 Acc=0.5218 F1=0.3951 || Val Loss=0.9897 Acc=0.4375 F1=0.2925\n",
      "Epoch 32/200 | Train Loss=0.8838 Acc=0.5179 F1=0.4016 || Val Loss=0.9694 Acc=0.4062 F1=0.2775\n",
      "Epoch 33/200 | Train Loss=0.8721 Acc=0.5694 F1=0.4319 || Val Loss=0.9881 Acc=0.4219 F1=0.2872\n",
      "Epoch 34/200 | Train Loss=0.8695 Acc=0.5496 F1=0.4321 || Val Loss=0.9932 Acc=0.4375 F1=0.2988\n",
      "Epoch 35/200 | Train Loss=0.8778 Acc=0.5179 F1=0.3982 || Val Loss=1.0159 Acc=0.3906 F1=0.2667\n",
      "Epoch 36/200 | Train Loss=0.8649 Acc=0.5595 F1=0.4632 || Val Loss=0.9999 Acc=0.4375 F1=0.3008\n",
      "Epoch 37/200 | Train Loss=0.8647 Acc=0.5437 F1=0.4668 || Val Loss=0.9832 Acc=0.4375 F1=0.2985\n",
      "Epoch 38/200 | Train Loss=0.8508 Acc=0.5694 F1=0.4649 || Val Loss=0.9865 Acc=0.4219 F1=0.2881\n",
      "Epoch 39/200 | Train Loss=0.8480 Acc=0.5655 F1=0.4717 || Val Loss=1.0138 Acc=0.4531 F1=0.3110\n",
      "Epoch 40/200 | Train Loss=0.8480 Acc=0.5615 F1=0.4613 || Val Loss=1.0127 Acc=0.4688 F1=0.3238\n",
      "Epoch 41/200 | Train Loss=0.8415 Acc=0.5893 F1=0.4936 || Val Loss=1.0068 Acc=0.4062 F1=0.2775\n",
      "Epoch 42/200 | Train Loss=0.8478 Acc=0.5694 F1=0.4759 || Val Loss=1.0199 Acc=0.4062 F1=0.2815\n",
      "Epoch 43/200 | Train Loss=0.8310 Acc=0.5992 F1=0.5435 || Val Loss=1.0421 Acc=0.4219 F1=0.2903\n",
      "Epoch 44/200 | Train Loss=0.8258 Acc=0.5813 F1=0.4930 || Val Loss=1.0414 Acc=0.4219 F1=0.2903\n",
      "Epoch 45/200 | Train Loss=0.8214 Acc=0.5913 F1=0.5173 || Val Loss=1.0242 Acc=0.3906 F1=0.2661\n",
      "Epoch 46/200 | Train Loss=0.8241 Acc=0.6190 F1=0.5292 || Val Loss=1.0325 Acc=0.3906 F1=0.2589\n",
      "Epoch 47/200 | Train Loss=0.8134 Acc=0.5813 F1=0.5191 || Val Loss=1.0541 Acc=0.4219 F1=0.2880\n",
      "Epoch 48/200 | Train Loss=0.8228 Acc=0.5833 F1=0.4836 || Val Loss=1.0551 Acc=0.4375 F1=0.2985\n",
      "Epoch 49/200 | Train Loss=0.8047 Acc=0.6091 F1=0.5296 || Val Loss=1.0638 Acc=0.4375 F1=0.3007\n",
      "Epoch 50/200 | Train Loss=0.7970 Acc=0.5893 F1=0.5279 || Val Loss=1.0734 Acc=0.4062 F1=0.2781\n",
      "Epoch 51/200 | Train Loss=0.7987 Acc=0.6131 F1=0.5270 || Val Loss=1.0926 Acc=0.3906 F1=0.2667\n",
      "Epoch 52/200 | Train Loss=0.8130 Acc=0.5714 F1=0.4918 || Val Loss=1.0923 Acc=0.3125 F1=0.2186\n",
      "Epoch 53/200 | Train Loss=0.7875 Acc=0.5972 F1=0.5258 || Val Loss=1.1389 Acc=0.3906 F1=0.2667\n",
      "Epoch 54/200 | Train Loss=0.7831 Acc=0.5833 F1=0.4997 || Val Loss=1.1389 Acc=0.3906 F1=0.2617\n",
      "Epoch 55/200 | Train Loss=0.7695 Acc=0.6171 F1=0.5392 || Val Loss=1.0981 Acc=0.3594 F1=0.2504\n",
      "Epoch 56/200 | Train Loss=0.7697 Acc=0.6151 F1=0.5456 || Val Loss=1.1405 Acc=0.3906 F1=0.2668\n",
      "Epoch 57/200 | Train Loss=0.7707 Acc=0.6210 F1=0.5218 || Val Loss=1.1449 Acc=0.3906 F1=0.2687\n",
      "Epoch 58/200 | Train Loss=0.7650 Acc=0.6151 F1=0.5535 || Val Loss=1.1307 Acc=0.3906 F1=0.2705\n",
      "Epoch 59/200 | Train Loss=0.7689 Acc=0.6250 F1=0.5576 || Val Loss=1.2714 Acc=0.3906 F1=0.2600\n",
      "Epoch 60/200 | Train Loss=0.7771 Acc=0.6071 F1=0.5227 || Val Loss=1.0916 Acc=0.4531 F1=0.2769\n",
      "[INFO] Early stopping triggered at epoch 60\n",
      "[RESULT] Fold 30: Test Loss=1.2291 Acc=0.4754 F1=0.3273\n",
      "\n",
      "[INFO] Starting Fold 31/36\n",
      "Epoch 1/200 | Train Loss=1.0889 Acc=0.3964 F1=0.2422 || Val Loss=1.0768 Acc=0.3548 F1=0.2140\n",
      "Epoch 2/200 | Train Loss=1.0566 Acc=0.4243 F1=0.2856 || Val Loss=1.0770 Acc=0.4194 F1=0.2264\n",
      "Epoch 3/200 | Train Loss=1.0675 Acc=0.4323 F1=0.2772 || Val Loss=1.0869 Acc=0.3226 F1=0.2442\n",
      "Epoch 4/200 | Train Loss=1.0507 Acc=0.4382 F1=0.3263 || Val Loss=1.0947 Acc=0.3065 F1=0.1889\n",
      "Epoch 5/200 | Train Loss=1.0484 Acc=0.4283 F1=0.3062 || Val Loss=1.0939 Acc=0.2903 F1=0.1808\n",
      "Epoch 6/200 | Train Loss=1.0495 Acc=0.4542 F1=0.3362 || Val Loss=1.0899 Acc=0.2742 F1=0.1925\n",
      "Epoch 7/200 | Train Loss=1.0434 Acc=0.4801 F1=0.3574 || Val Loss=1.0913 Acc=0.2742 F1=0.1991\n",
      "Epoch 8/200 | Train Loss=1.0423 Acc=0.4841 F1=0.3597 || Val Loss=1.1037 Acc=0.2903 F1=0.2139\n",
      "Epoch 9/200 | Train Loss=1.0369 Acc=0.4622 F1=0.3442 || Val Loss=1.1073 Acc=0.3226 F1=0.2440\n",
      "Epoch 10/200 | Train Loss=1.0345 Acc=0.4781 F1=0.3540 || Val Loss=1.1150 Acc=0.3065 F1=0.2242\n",
      "Epoch 11/200 | Train Loss=1.0301 Acc=0.4801 F1=0.3584 || Val Loss=1.1324 Acc=0.2903 F1=0.2017\n",
      "Epoch 12/200 | Train Loss=1.0327 Acc=0.4641 F1=0.3433 || Val Loss=1.1308 Acc=0.3065 F1=0.2243\n",
      "Epoch 13/200 | Train Loss=1.0306 Acc=0.4801 F1=0.3560 || Val Loss=1.1236 Acc=0.3226 F1=0.2447\n",
      "Epoch 14/200 | Train Loss=1.0251 Acc=0.4900 F1=0.3645 || Val Loss=1.1515 Acc=0.3065 F1=0.2186\n",
      "Epoch 15/200 | Train Loss=1.0248 Acc=0.4841 F1=0.3594 || Val Loss=1.1497 Acc=0.3065 F1=0.2186\n",
      "Epoch 16/200 | Train Loss=1.0188 Acc=0.5000 F1=0.3731 || Val Loss=1.1416 Acc=0.2903 F1=0.2088\n",
      "Epoch 17/200 | Train Loss=1.0202 Acc=0.4940 F1=0.3683 || Val Loss=1.1497 Acc=0.2903 F1=0.2088\n",
      "Epoch 18/200 | Train Loss=1.0145 Acc=0.4801 F1=0.3582 || Val Loss=1.1794 Acc=0.2903 F1=0.2087\n",
      "Epoch 19/200 | Train Loss=1.0012 Acc=0.4980 F1=0.3716 || Val Loss=1.1765 Acc=0.3065 F1=0.2284\n",
      "Epoch 20/200 | Train Loss=1.0034 Acc=0.4940 F1=0.3693 || Val Loss=1.1955 Acc=0.2581 F1=0.1751\n",
      "Epoch 21/200 | Train Loss=1.0022 Acc=0.5040 F1=0.3813 || Val Loss=1.1808 Acc=0.3387 F1=0.2535\n",
      "Epoch 22/200 | Train Loss=0.9930 Acc=0.5100 F1=0.3857 || Val Loss=1.1949 Acc=0.3387 F1=0.2539\n",
      "Epoch 23/200 | Train Loss=0.9916 Acc=0.5159 F1=0.3952 || Val Loss=1.2194 Acc=0.3065 F1=0.2264\n",
      "Epoch 24/200 | Train Loss=0.9842 Acc=0.5000 F1=0.3740 || Val Loss=1.2124 Acc=0.2903 F1=0.2107\n",
      "Epoch 25/200 | Train Loss=0.9840 Acc=0.5080 F1=0.3933 || Val Loss=1.2331 Acc=0.2742 F1=0.1941\n",
      "Epoch 26/200 | Train Loss=0.9874 Acc=0.5199 F1=0.4043 || Val Loss=1.1966 Acc=0.3387 F1=0.2520\n",
      "Epoch 27/200 | Train Loss=0.9734 Acc=0.5139 F1=0.4001 || Val Loss=1.2418 Acc=0.2742 F1=0.1860\n",
      "Epoch 28/200 | Train Loss=0.9624 Acc=0.5398 F1=0.4181 || Val Loss=1.2465 Acc=0.2903 F1=0.2036\n",
      "Epoch 29/200 | Train Loss=0.9572 Acc=0.5578 F1=0.4325 || Val Loss=1.2536 Acc=0.3065 F1=0.2130\n",
      "Epoch 30/200 | Train Loss=0.9607 Acc=0.5478 F1=0.4663 || Val Loss=1.2753 Acc=0.3065 F1=0.2130\n",
      "Epoch 31/200 | Train Loss=0.9531 Acc=0.5438 F1=0.4314 || Val Loss=1.2855 Acc=0.3065 F1=0.2130\n",
      "Epoch 32/200 | Train Loss=0.9611 Acc=0.5319 F1=0.4256 || Val Loss=1.2857 Acc=0.3065 F1=0.2130\n",
      "Epoch 33/200 | Train Loss=0.9394 Acc=0.5478 F1=0.4663 || Val Loss=1.3504 Acc=0.3065 F1=0.2206\n",
      "Epoch 34/200 | Train Loss=0.9431 Acc=0.5478 F1=0.4762 || Val Loss=1.2947 Acc=0.3387 F1=0.2579\n",
      "Epoch 35/200 | Train Loss=0.9343 Acc=0.5398 F1=0.4598 || Val Loss=1.3610 Acc=0.2903 F1=0.1949\n",
      "Epoch 36/200 | Train Loss=0.9344 Acc=0.5259 F1=0.4508 || Val Loss=1.3329 Acc=0.3065 F1=0.2286\n",
      "Epoch 37/200 | Train Loss=0.9146 Acc=0.5956 F1=0.5323 || Val Loss=1.3612 Acc=0.2903 F1=0.2202\n",
      "Epoch 38/200 | Train Loss=0.9139 Acc=0.5578 F1=0.4769 || Val Loss=1.3784 Acc=0.3065 F1=0.2206\n",
      "Epoch 39/200 | Train Loss=0.9196 Acc=0.5458 F1=0.4739 || Val Loss=1.3626 Acc=0.2903 F1=0.2127\n",
      "Epoch 40/200 | Train Loss=0.8958 Acc=0.5578 F1=0.4839 || Val Loss=1.4549 Acc=0.3226 F1=0.2367\n",
      "Epoch 41/200 | Train Loss=0.9079 Acc=0.5637 F1=0.4818 || Val Loss=1.4163 Acc=0.3226 F1=0.2444\n",
      "Epoch 42/200 | Train Loss=0.8883 Acc=0.5916 F1=0.5246 || Val Loss=1.4488 Acc=0.2742 F1=0.1958\n",
      "Epoch 43/200 | Train Loss=0.8735 Acc=0.5677 F1=0.5199 || Val Loss=1.4525 Acc=0.3065 F1=0.2428\n",
      "Epoch 44/200 | Train Loss=0.8718 Acc=0.5637 F1=0.5051 || Val Loss=1.4163 Acc=0.3226 F1=0.2502\n",
      "Epoch 45/200 | Train Loss=0.8833 Acc=0.5837 F1=0.5316 || Val Loss=1.3902 Acc=0.2903 F1=0.2127\n",
      "Epoch 46/200 | Train Loss=0.8740 Acc=0.5857 F1=0.5257 || Val Loss=1.4181 Acc=0.2742 F1=0.1960\n",
      "Epoch 47/200 | Train Loss=0.8697 Acc=0.5637 F1=0.4913 || Val Loss=1.3918 Acc=0.2742 F1=0.1964\n",
      "Epoch 48/200 | Train Loss=0.8505 Acc=0.6215 F1=0.5785 || Val Loss=1.3985 Acc=0.2742 F1=0.1960\n",
      "Epoch 49/200 | Train Loss=0.8636 Acc=0.5896 F1=0.5282 || Val Loss=1.3884 Acc=0.2903 F1=0.2126\n",
      "Epoch 50/200 | Train Loss=0.8556 Acc=0.5956 F1=0.5260 || Val Loss=1.4197 Acc=0.2903 F1=0.2126\n",
      "Epoch 51/200 | Train Loss=0.8570 Acc=0.5876 F1=0.5309 || Val Loss=1.4388 Acc=0.3226 F1=0.2345\n",
      "Epoch 52/200 | Train Loss=0.8438 Acc=0.5876 F1=0.5342 || Val Loss=1.3851 Acc=0.2903 F1=0.2126\n",
      "Epoch 53/200 | Train Loss=0.8468 Acc=0.6175 F1=0.5603 || Val Loss=1.5287 Acc=0.3065 F1=0.2206\n",
      "Epoch 54/200 | Train Loss=0.8450 Acc=0.5976 F1=0.5341 || Val Loss=1.3960 Acc=0.2903 F1=0.2271\n",
      "[INFO] Early stopping triggered at epoch 54\n",
      "[RESULT] Fold 31: Test Loss=1.2618 Acc=0.3387 F1=0.3303\n",
      "\n",
      "[INFO] Starting Fold 32/36\n",
      "Epoch 1/200 | Train Loss=1.0974 Acc=0.3952 F1=0.2602 || Val Loss=1.0584 Acc=0.3710 F1=0.1804\n",
      "Epoch 2/200 | Train Loss=1.0746 Acc=0.4072 F1=0.2712 || Val Loss=1.0140 Acc=0.4677 F1=0.2368\n",
      "Epoch 3/200 | Train Loss=1.0708 Acc=0.4012 F1=0.3008 || Val Loss=1.0188 Acc=0.4677 F1=0.3360\n",
      "Epoch 4/200 | Train Loss=1.0622 Acc=0.4212 F1=0.2778 || Val Loss=1.0293 Acc=0.3710 F1=0.1804\n",
      "Epoch 5/200 | Train Loss=1.0594 Acc=0.4232 F1=0.2214 || Val Loss=1.0379 Acc=0.3710 F1=0.1804\n",
      "Epoch 6/200 | Train Loss=1.0548 Acc=0.4212 F1=0.2009 || Val Loss=1.0438 Acc=0.3710 F1=0.1804\n",
      "Epoch 7/200 | Train Loss=1.0536 Acc=0.4291 F1=0.2223 || Val Loss=1.0339 Acc=0.3710 F1=0.1804\n",
      "Epoch 8/200 | Train Loss=1.0508 Acc=0.4371 F1=0.2628 || Val Loss=1.0267 Acc=0.3710 F1=0.1804\n",
      "Epoch 9/200 | Train Loss=1.0454 Acc=0.4451 F1=0.2984 || Val Loss=1.0221 Acc=0.3387 F1=0.1842\n",
      "Epoch 10/200 | Train Loss=1.0490 Acc=0.4391 F1=0.3060 || Val Loss=1.0232 Acc=0.3387 F1=0.1974\n",
      "Epoch 11/200 | Train Loss=1.0427 Acc=0.4711 F1=0.3528 || Val Loss=1.0259 Acc=0.3710 F1=0.1975\n",
      "Epoch 12/200 | Train Loss=1.0376 Acc=0.4631 F1=0.3293 || Val Loss=1.0302 Acc=0.3871 F1=0.2040\n",
      "Epoch 13/200 | Train Loss=1.0379 Acc=0.4431 F1=0.3127 || Val Loss=1.0290 Acc=0.3710 F1=0.2243\n",
      "Epoch 14/200 | Train Loss=1.0324 Acc=0.4631 F1=0.3479 || Val Loss=1.0290 Acc=0.3226 F1=0.2169\n",
      "Epoch 15/200 | Train Loss=1.0369 Acc=0.4431 F1=0.3330 || Val Loss=1.0300 Acc=0.4032 F1=0.2900\n",
      "Epoch 16/200 | Train Loss=1.0396 Acc=0.4611 F1=0.3639 || Val Loss=1.0367 Acc=0.3710 F1=0.2667\n",
      "Epoch 17/200 | Train Loss=1.0321 Acc=0.4890 F1=0.3816 || Val Loss=1.0503 Acc=0.3548 F1=0.2164\n",
      "Epoch 18/200 | Train Loss=1.0238 Acc=0.4790 F1=0.3615 || Val Loss=1.0629 Acc=0.3548 F1=0.2164\n",
      "Epoch 19/200 | Train Loss=1.0265 Acc=0.4810 F1=0.3811 || Val Loss=1.0592 Acc=0.3387 F1=0.2437\n",
      "Epoch 20/200 | Train Loss=1.0181 Acc=0.4810 F1=0.3874 || Val Loss=1.0686 Acc=0.3065 F1=0.1937\n",
      "Epoch 21/200 | Train Loss=1.0205 Acc=0.4691 F1=0.3600 || Val Loss=1.0940 Acc=0.3871 F1=0.2040\n",
      "Epoch 22/200 | Train Loss=1.0141 Acc=0.4890 F1=0.3644 || Val Loss=1.0711 Acc=0.3226 F1=0.2166\n",
      "Epoch 23/200 | Train Loss=1.0261 Acc=0.4611 F1=0.3919 || Val Loss=1.0754 Acc=0.3226 F1=0.2169\n",
      "[INFO] Early stopping triggered at epoch 23\n",
      "[RESULT] Fold 32: Test Loss=0.9879 Acc=0.4688 F1=0.3300\n",
      "\n",
      "[INFO] Starting Fold 33/36\n",
      "Epoch 1/200 | Train Loss=1.0369 Acc=0.4167 F1=0.3599 || Val Loss=1.1696 Acc=0.3906 F1=0.2117\n",
      "Epoch 2/200 | Train Loss=1.0115 Acc=0.4623 F1=0.2319 || Val Loss=1.1568 Acc=0.5000 F1=0.3704\n",
      "Epoch 3/200 | Train Loss=0.9989 Acc=0.4464 F1=0.2987 || Val Loss=1.1184 Acc=0.4375 F1=0.3247\n",
      "Epoch 4/200 | Train Loss=0.9995 Acc=0.4643 F1=0.2504 || Val Loss=1.1034 Acc=0.4219 F1=0.2623\n",
      "Epoch 5/200 | Train Loss=0.9951 Acc=0.4683 F1=0.2441 || Val Loss=1.1239 Acc=0.3906 F1=0.2106\n",
      "Epoch 6/200 | Train Loss=0.9882 Acc=0.4881 F1=0.2974 || Val Loss=1.1619 Acc=0.4219 F1=0.3213\n",
      "Epoch 7/200 | Train Loss=0.9869 Acc=0.4940 F1=0.3530 || Val Loss=1.1614 Acc=0.4219 F1=0.3039\n",
      "Epoch 8/200 | Train Loss=0.9788 Acc=0.4742 F1=0.2862 || Val Loss=1.1313 Acc=0.4219 F1=0.2886\n",
      "Epoch 9/200 | Train Loss=0.9754 Acc=0.4782 F1=0.2843 || Val Loss=1.1206 Acc=0.4531 F1=0.3253\n",
      "Epoch 10/200 | Train Loss=0.9752 Acc=0.4881 F1=0.3387 || Val Loss=1.1345 Acc=0.4062 F1=0.3000\n",
      "Epoch 11/200 | Train Loss=0.9692 Acc=0.4881 F1=0.3543 || Val Loss=1.1616 Acc=0.3906 F1=0.2951\n",
      "Epoch 12/200 | Train Loss=0.9695 Acc=0.4980 F1=0.3639 || Val Loss=1.1670 Acc=0.3906 F1=0.2966\n",
      "Epoch 13/200 | Train Loss=0.9605 Acc=0.5099 F1=0.3582 || Val Loss=1.1244 Acc=0.3750 F1=0.2666\n",
      "Epoch 14/200 | Train Loss=0.9554 Acc=0.5060 F1=0.3625 || Val Loss=1.1504 Acc=0.4219 F1=0.3213\n",
      "Epoch 15/200 | Train Loss=0.9573 Acc=0.5060 F1=0.3851 || Val Loss=1.1664 Acc=0.3906 F1=0.2976\n",
      "Epoch 16/200 | Train Loss=0.9584 Acc=0.5020 F1=0.3465 || Val Loss=1.1579 Acc=0.4375 F1=0.2707\n",
      "Epoch 17/200 | Train Loss=0.9468 Acc=0.5397 F1=0.3759 || Val Loss=1.1534 Acc=0.4219 F1=0.3075\n",
      "Epoch 18/200 | Train Loss=0.9450 Acc=0.5357 F1=0.4011 || Val Loss=1.1866 Acc=0.4219 F1=0.3160\n",
      "Epoch 19/200 | Train Loss=0.9299 Acc=0.5437 F1=0.3984 || Val Loss=1.1600 Acc=0.4062 F1=0.2182\n",
      "Epoch 20/200 | Train Loss=0.9240 Acc=0.5417 F1=0.3846 || Val Loss=1.1842 Acc=0.3906 F1=0.2613\n",
      "Epoch 21/200 | Train Loss=0.9196 Acc=0.5476 F1=0.4161 || Val Loss=1.1842 Acc=0.3906 F1=0.2785\n",
      "Epoch 22/200 | Train Loss=0.9232 Acc=0.5655 F1=0.4107 || Val Loss=1.2036 Acc=0.3906 F1=0.2152\n",
      "[INFO] Early stopping triggered at epoch 22\n",
      "[RESULT] Fold 33: Test Loss=1.0996 Acc=0.4754 F1=0.3515\n",
      "\n",
      "[INFO] Starting Fold 34/36\n",
      "Epoch 1/200 | Train Loss=1.0896 Acc=0.3725 F1=0.2791 || Val Loss=1.0796 Acc=0.4355 F1=0.2956\n",
      "Epoch 2/200 | Train Loss=1.0737 Acc=0.3964 F1=0.2212 || Val Loss=1.0727 Acc=0.4355 F1=0.2692\n",
      "Epoch 3/200 | Train Loss=1.0682 Acc=0.3884 F1=0.2884 || Val Loss=1.0690 Acc=0.4355 F1=0.2936\n",
      "Epoch 4/200 | Train Loss=1.0611 Acc=0.4044 F1=0.2493 || Val Loss=1.0704 Acc=0.4355 F1=0.2936\n",
      "Epoch 5/200 | Train Loss=1.0587 Acc=0.3944 F1=0.2634 || Val Loss=1.0692 Acc=0.4516 F1=0.3200\n",
      "Epoch 6/200 | Train Loss=1.0527 Acc=0.4203 F1=0.3088 || Val Loss=1.0676 Acc=0.4355 F1=0.3101\n",
      "Epoch 7/200 | Train Loss=1.0521 Acc=0.4542 F1=0.3392 || Val Loss=1.0666 Acc=0.4355 F1=0.3164\n",
      "Epoch 8/200 | Train Loss=1.0511 Acc=0.4402 F1=0.3289 || Val Loss=1.0663 Acc=0.4355 F1=0.3101\n",
      "Epoch 9/200 | Train Loss=1.0510 Acc=0.4522 F1=0.3377 || Val Loss=1.0659 Acc=0.4516 F1=0.3266\n",
      "Epoch 10/200 | Train Loss=1.0500 Acc=0.4303 F1=0.3214 || Val Loss=1.0664 Acc=0.4355 F1=0.3101\n",
      "Epoch 11/200 | Train Loss=1.0413 Acc=0.4462 F1=0.3332 || Val Loss=1.0680 Acc=0.4516 F1=0.3266\n",
      "Epoch 12/200 | Train Loss=1.0397 Acc=0.4502 F1=0.3358 || Val Loss=1.0690 Acc=0.4355 F1=0.3106\n",
      "Epoch 13/200 | Train Loss=1.0389 Acc=0.4542 F1=0.3378 || Val Loss=1.0701 Acc=0.4355 F1=0.3101\n",
      "Epoch 14/200 | Train Loss=1.0387 Acc=0.4402 F1=0.3293 || Val Loss=1.0729 Acc=0.4355 F1=0.3214\n",
      "Epoch 15/200 | Train Loss=1.0389 Acc=0.4602 F1=0.3430 || Val Loss=1.0750 Acc=0.4677 F1=0.3424\n",
      "Epoch 16/200 | Train Loss=1.0340 Acc=0.4781 F1=0.3569 || Val Loss=1.0797 Acc=0.4516 F1=0.3266\n",
      "Epoch 17/200 | Train Loss=1.0294 Acc=0.4761 F1=0.3558 || Val Loss=1.0813 Acc=0.4516 F1=0.3361\n",
      "Epoch 18/200 | Train Loss=1.0253 Acc=0.4562 F1=0.3404 || Val Loss=1.0867 Acc=0.4677 F1=0.3470\n",
      "Epoch 19/200 | Train Loss=1.0280 Acc=0.4721 F1=0.3584 || Val Loss=1.0918 Acc=0.4677 F1=0.3470\n",
      "Epoch 20/200 | Train Loss=1.0284 Acc=0.4641 F1=0.3521 || Val Loss=1.0945 Acc=0.4516 F1=0.3361\n",
      "Epoch 21/200 | Train Loss=1.0213 Acc=0.4861 F1=0.3639 || Val Loss=1.0992 Acc=0.4677 F1=0.3470\n",
      "Epoch 22/200 | Train Loss=1.0200 Acc=0.4841 F1=0.3718 || Val Loss=1.1064 Acc=0.4677 F1=0.3470\n",
      "Epoch 23/200 | Train Loss=1.0194 Acc=0.4582 F1=0.3574 || Val Loss=1.1018 Acc=0.4516 F1=0.3361\n",
      "Epoch 24/200 | Train Loss=1.0202 Acc=0.4741 F1=0.3661 || Val Loss=1.1125 Acc=0.4677 F1=0.3470\n",
      "Epoch 25/200 | Train Loss=1.0143 Acc=0.4761 F1=0.3761 || Val Loss=1.1059 Acc=0.4516 F1=0.3361\n",
      "Epoch 26/200 | Train Loss=1.0161 Acc=0.4821 F1=0.3974 || Val Loss=1.1108 Acc=0.4677 F1=0.3470\n",
      "Epoch 27/200 | Train Loss=1.0033 Acc=0.4821 F1=0.3850 || Val Loss=1.1156 Acc=0.4839 F1=0.3967\n",
      "Epoch 28/200 | Train Loss=1.0056 Acc=0.4821 F1=0.3766 || Val Loss=1.1201 Acc=0.4677 F1=0.3869\n",
      "Epoch 29/200 | Train Loss=0.9915 Acc=0.4980 F1=0.3937 || Val Loss=1.1315 Acc=0.4355 F1=0.3666\n",
      "Epoch 30/200 | Train Loss=0.9982 Acc=0.5139 F1=0.4246 || Val Loss=1.1319 Acc=0.4032 F1=0.3864\n",
      "Epoch 31/200 | Train Loss=0.9820 Acc=0.5219 F1=0.4348 || Val Loss=1.1552 Acc=0.4194 F1=0.3698\n",
      "Epoch 32/200 | Train Loss=0.9881 Acc=0.5080 F1=0.4178 || Val Loss=1.1285 Acc=0.3871 F1=0.3626\n",
      "Epoch 33/200 | Train Loss=0.9685 Acc=0.5139 F1=0.4278 || Val Loss=1.1538 Acc=0.3871 F1=0.3310\n",
      "Epoch 34/200 | Train Loss=0.9792 Acc=0.5179 F1=0.4323 || Val Loss=1.1443 Acc=0.3710 F1=0.3469\n",
      "Epoch 35/200 | Train Loss=0.9715 Acc=0.5259 F1=0.4418 || Val Loss=1.1586 Acc=0.3710 F1=0.3469\n",
      "Epoch 36/200 | Train Loss=0.9611 Acc=0.5120 F1=0.4307 || Val Loss=1.1792 Acc=0.3548 F1=0.3196\n",
      "Epoch 37/200 | Train Loss=0.9485 Acc=0.5199 F1=0.4363 || Val Loss=1.1848 Acc=0.4032 F1=0.3622\n",
      "Epoch 38/200 | Train Loss=0.9538 Acc=0.5279 F1=0.4594 || Val Loss=1.1990 Acc=0.3871 F1=0.3490\n",
      "Epoch 39/200 | Train Loss=0.9394 Acc=0.5319 F1=0.4396 || Val Loss=1.2171 Acc=0.3226 F1=0.2796\n",
      "Epoch 40/200 | Train Loss=0.9330 Acc=0.5438 F1=0.4630 || Val Loss=1.1949 Acc=0.3710 F1=0.3554\n",
      "Epoch 41/200 | Train Loss=0.9345 Acc=0.5319 F1=0.4498 || Val Loss=1.2494 Acc=0.3387 F1=0.3211\n",
      "Epoch 42/200 | Train Loss=0.9311 Acc=0.5498 F1=0.4770 || Val Loss=1.1724 Acc=0.3548 F1=0.3381\n",
      "Epoch 43/200 | Train Loss=0.9169 Acc=0.5657 F1=0.5072 || Val Loss=1.2642 Acc=0.3387 F1=0.3295\n",
      "Epoch 44/200 | Train Loss=0.9328 Acc=0.5478 F1=0.4698 || Val Loss=1.1676 Acc=0.3387 F1=0.3200\n",
      "Epoch 45/200 | Train Loss=0.9533 Acc=0.5299 F1=0.4589 || Val Loss=1.1608 Acc=0.3710 F1=0.3539\n",
      "Epoch 46/200 | Train Loss=0.9276 Acc=0.5657 F1=0.4827 || Val Loss=1.2812 Acc=0.3710 F1=0.3494\n",
      "Epoch 47/200 | Train Loss=0.9100 Acc=0.5558 F1=0.4800 || Val Loss=1.2041 Acc=0.3710 F1=0.3508\n",
      "[INFO] Early stopping triggered at epoch 47\n",
      "[RESULT] Fold 34: Test Loss=1.1749 Acc=0.3548 F1=0.2870\n",
      "\n",
      "[INFO] Starting Fold 35/36\n",
      "Epoch 1/200 | Train Loss=1.0871 Acc=0.3772 F1=0.2871 || Val Loss=1.0258 Acc=0.5161 F1=0.2270\n",
      "Epoch 2/200 | Train Loss=1.0636 Acc=0.4311 F1=0.2257 || Val Loss=1.0339 Acc=0.5161 F1=0.2270\n",
      "Epoch 3/200 | Train Loss=1.0514 Acc=0.4251 F1=0.2287 || Val Loss=1.0386 Acc=0.5161 F1=0.2270\n",
      "Epoch 4/200 | Train Loss=1.0513 Acc=0.4291 F1=0.2553 || Val Loss=1.0371 Acc=0.5161 F1=0.2270\n",
      "Epoch 5/200 | Train Loss=1.0484 Acc=0.4291 F1=0.2514 || Val Loss=1.0354 Acc=0.4839 F1=0.2439\n",
      "Epoch 6/200 | Train Loss=1.0421 Acc=0.4351 F1=0.2830 || Val Loss=1.0404 Acc=0.4839 F1=0.2639\n",
      "Epoch 7/200 | Train Loss=1.0349 Acc=0.4471 F1=0.2999 || Val Loss=1.0360 Acc=0.4839 F1=0.2639\n",
      "Epoch 8/200 | Train Loss=1.0293 Acc=0.4391 F1=0.2860 || Val Loss=1.0373 Acc=0.4516 F1=0.2508\n",
      "Epoch 9/200 | Train Loss=1.0250 Acc=0.4431 F1=0.2869 || Val Loss=1.0425 Acc=0.3871 F1=0.2237\n",
      "Epoch 10/200 | Train Loss=1.0183 Acc=0.4491 F1=0.3230 || Val Loss=1.0533 Acc=0.3710 F1=0.2378\n",
      "Epoch 11/200 | Train Loss=1.0135 Acc=0.4731 F1=0.3628 || Val Loss=1.0677 Acc=0.3871 F1=0.2628\n",
      "Epoch 12/200 | Train Loss=1.0058 Acc=0.4750 F1=0.3571 || Val Loss=1.0746 Acc=0.3548 F1=0.2202\n",
      "Epoch 13/200 | Train Loss=0.9932 Acc=0.5090 F1=0.3952 || Val Loss=1.1067 Acc=0.3710 F1=0.2543\n",
      "Epoch 14/200 | Train Loss=0.9932 Acc=0.4810 F1=0.3948 || Val Loss=1.1269 Acc=0.3548 F1=0.2449\n",
      "Epoch 15/200 | Train Loss=0.9881 Acc=0.4651 F1=0.3905 || Val Loss=1.1283 Acc=0.3710 F1=0.2600\n",
      "Epoch 16/200 | Train Loss=0.9834 Acc=0.4890 F1=0.4190 || Val Loss=1.1143 Acc=0.3710 F1=0.2543\n",
      "Epoch 17/200 | Train Loss=0.9861 Acc=0.4810 F1=0.4148 || Val Loss=1.1223 Acc=0.4032 F1=0.2938\n",
      "Epoch 18/200 | Train Loss=0.9765 Acc=0.4651 F1=0.4075 || Val Loss=1.0885 Acc=0.4194 F1=0.2929\n",
      "Epoch 19/200 | Train Loss=0.9785 Acc=0.4830 F1=0.4119 || Val Loss=1.1217 Acc=0.4516 F1=0.3262\n",
      "Epoch 20/200 | Train Loss=0.9669 Acc=0.4850 F1=0.4237 || Val Loss=1.1224 Acc=0.4032 F1=0.2932\n",
      "Epoch 21/200 | Train Loss=0.9572 Acc=0.4930 F1=0.4519 || Val Loss=1.1206 Acc=0.3871 F1=0.2812\n",
      "Epoch 22/200 | Train Loss=0.9635 Acc=0.4790 F1=0.4311 || Val Loss=1.1502 Acc=0.4355 F1=0.3153\n",
      "Epoch 23/200 | Train Loss=0.9677 Acc=0.4731 F1=0.3953 || Val Loss=1.1549 Acc=0.4194 F1=0.3056\n",
      "Epoch 24/200 | Train Loss=0.9691 Acc=0.4731 F1=0.4396 || Val Loss=1.1600 Acc=0.3387 F1=0.2195\n",
      "Epoch 25/200 | Train Loss=0.9660 Acc=0.4810 F1=0.4625 || Val Loss=1.1179 Acc=0.4355 F1=0.3068\n",
      "Epoch 26/200 | Train Loss=0.9496 Acc=0.4950 F1=0.4321 || Val Loss=1.1595 Acc=0.3548 F1=0.2487\n",
      "Epoch 27/200 | Train Loss=0.9395 Acc=0.4850 F1=0.4148 || Val Loss=1.1119 Acc=0.4355 F1=0.2597\n",
      "Epoch 28/200 | Train Loss=0.9439 Acc=0.5090 F1=0.4722 || Val Loss=1.1326 Acc=0.3871 F1=0.2706\n",
      "Epoch 29/200 | Train Loss=0.9387 Acc=0.5150 F1=0.4737 || Val Loss=1.1787 Acc=0.4032 F1=0.3229\n",
      "Epoch 30/200 | Train Loss=0.9088 Acc=0.5210 F1=0.4739 || Val Loss=1.1825 Acc=0.4194 F1=0.2995\n",
      "Epoch 31/200 | Train Loss=0.9170 Acc=0.5309 F1=0.4905 || Val Loss=1.1893 Acc=0.4032 F1=0.2955\n",
      "Epoch 32/200 | Train Loss=0.9114 Acc=0.5309 F1=0.4941 || Val Loss=1.2383 Acc=0.4032 F1=0.2932\n",
      "Epoch 33/200 | Train Loss=0.9072 Acc=0.5349 F1=0.4988 || Val Loss=1.2211 Acc=0.4032 F1=0.2893\n",
      "Epoch 34/200 | Train Loss=0.8904 Acc=0.5469 F1=0.5100 || Val Loss=1.2315 Acc=0.4516 F1=0.3236\n",
      "Epoch 35/200 | Train Loss=0.9020 Acc=0.5230 F1=0.4924 || Val Loss=1.2831 Acc=0.4194 F1=0.3028\n",
      "Epoch 36/200 | Train Loss=0.8813 Acc=0.5429 F1=0.5119 || Val Loss=1.2590 Acc=0.4194 F1=0.2994\n",
      "Epoch 37/200 | Train Loss=0.8729 Acc=0.5509 F1=0.5305 || Val Loss=1.3663 Acc=0.4194 F1=0.3053\n",
      "Epoch 38/200 | Train Loss=0.8575 Acc=0.5729 F1=0.5429 || Val Loss=1.2848 Acc=0.4516 F1=0.3145\n",
      "Epoch 39/200 | Train Loss=0.8579 Acc=0.5509 F1=0.5204 || Val Loss=1.3515 Acc=0.4194 F1=0.3506\n",
      "Epoch 40/200 | Train Loss=0.8609 Acc=0.5629 F1=0.5508 || Val Loss=1.3924 Acc=0.4677 F1=0.3831\n",
      "Epoch 41/200 | Train Loss=0.8621 Acc=0.5669 F1=0.5529 || Val Loss=1.4648 Acc=0.4032 F1=0.2951\n",
      "Epoch 42/200 | Train Loss=0.8524 Acc=0.5629 F1=0.5444 || Val Loss=1.4716 Acc=0.4516 F1=0.3268\n",
      "Epoch 43/200 | Train Loss=0.8363 Acc=0.5709 F1=0.5386 || Val Loss=1.4619 Acc=0.4677 F1=0.3415\n",
      "Epoch 44/200 | Train Loss=0.8237 Acc=0.5888 F1=0.5721 || Val Loss=1.4698 Acc=0.3871 F1=0.2815\n",
      "Epoch 45/200 | Train Loss=0.8469 Acc=0.5788 F1=0.5541 || Val Loss=1.4416 Acc=0.4516 F1=0.3294\n",
      "Epoch 46/200 | Train Loss=0.8255 Acc=0.5948 F1=0.5786 || Val Loss=1.5782 Acc=0.4032 F1=0.2923\n",
      "Epoch 47/200 | Train Loss=0.8196 Acc=0.5888 F1=0.5723 || Val Loss=1.4676 Acc=0.4516 F1=0.3266\n",
      "Epoch 48/200 | Train Loss=0.7996 Acc=0.6108 F1=0.5955 || Val Loss=1.6310 Acc=0.4032 F1=0.2919\n",
      "Epoch 49/200 | Train Loss=0.7950 Acc=0.5928 F1=0.5808 || Val Loss=1.5412 Acc=0.4032 F1=0.2959\n",
      "Epoch 50/200 | Train Loss=0.8190 Acc=0.5908 F1=0.5696 || Val Loss=1.6130 Acc=0.4194 F1=0.3074\n",
      "Epoch 51/200 | Train Loss=0.8121 Acc=0.6008 F1=0.5943 || Val Loss=1.7064 Acc=0.4032 F1=0.2904\n",
      "Epoch 52/200 | Train Loss=0.8139 Acc=0.5988 F1=0.5871 || Val Loss=1.5657 Acc=0.4839 F1=0.3445\n",
      "Epoch 53/200 | Train Loss=0.7889 Acc=0.6228 F1=0.6012 || Val Loss=1.6733 Acc=0.4194 F1=0.3092\n",
      "Epoch 54/200 | Train Loss=0.7840 Acc=0.6048 F1=0.5990 || Val Loss=1.6488 Acc=0.3710 F1=0.3087\n",
      "Epoch 55/200 | Train Loss=0.7859 Acc=0.6267 F1=0.6218 || Val Loss=1.5845 Acc=0.4355 F1=0.3214\n",
      "Epoch 56/200 | Train Loss=0.7856 Acc=0.6088 F1=0.5974 || Val Loss=1.6477 Acc=0.4355 F1=0.3176\n",
      "Epoch 57/200 | Train Loss=0.7648 Acc=0.6407 F1=0.6261 || Val Loss=1.6949 Acc=0.3710 F1=0.3134\n",
      "Epoch 58/200 | Train Loss=0.7373 Acc=0.6307 F1=0.6291 || Val Loss=1.6720 Acc=0.3871 F1=0.2881\n",
      "Epoch 59/200 | Train Loss=0.7607 Acc=0.6148 F1=0.6114 || Val Loss=1.6286 Acc=0.4194 F1=0.3066\n",
      "Epoch 60/200 | Train Loss=0.7551 Acc=0.6048 F1=0.5901 || Val Loss=1.6188 Acc=0.3710 F1=0.2766\n",
      "[INFO] Early stopping triggered at epoch 60\n",
      "[RESULT] Fold 35: Test Loss=1.1512 Acc=0.3906 F1=0.2654\n",
      "\n",
      "[INFO] Starting Fold 36/36\n",
      "Epoch 1/200 | Train Loss=1.0780 Acc=0.3770 F1=0.2860 || Val Loss=1.1108 Acc=0.4375 F1=0.2679\n",
      "Epoch 2/200 | Train Loss=1.0473 Acc=0.4405 F1=0.2329 || Val Loss=1.0976 Acc=0.4375 F1=0.2679\n",
      "Epoch 3/200 | Train Loss=1.0342 Acc=0.4425 F1=0.2538 || Val Loss=1.0757 Acc=0.4688 F1=0.3371\n",
      "Epoch 4/200 | Train Loss=1.0328 Acc=0.4405 F1=0.3037 || Val Loss=1.0725 Acc=0.4531 F1=0.3360\n",
      "Epoch 5/200 | Train Loss=1.0299 Acc=0.4583 F1=0.2956 || Val Loss=1.0814 Acc=0.4688 F1=0.3371\n",
      "Epoch 6/200 | Train Loss=1.0270 Acc=0.4643 F1=0.3354 || Val Loss=1.0811 Acc=0.4375 F1=0.3306\n",
      "Epoch 7/200 | Train Loss=1.0219 Acc=0.4881 F1=0.3503 || Val Loss=1.0696 Acc=0.4531 F1=0.3415\n",
      "Epoch 8/200 | Train Loss=1.0230 Acc=0.4722 F1=0.3292 || Val Loss=1.0656 Acc=0.4375 F1=0.3302\n",
      "Epoch 9/200 | Train Loss=1.0168 Acc=0.5060 F1=0.3886 || Val Loss=1.0779 Acc=0.3906 F1=0.2563\n",
      "Epoch 10/200 | Train Loss=1.0093 Acc=0.4921 F1=0.3750 || Val Loss=1.0688 Acc=0.4531 F1=0.3338\n",
      "Epoch 11/200 | Train Loss=1.0055 Acc=0.4921 F1=0.3779 || Val Loss=1.0707 Acc=0.4688 F1=0.3524\n",
      "Epoch 12/200 | Train Loss=1.0054 Acc=0.4861 F1=0.3654 || Val Loss=1.0685 Acc=0.4688 F1=0.3479\n",
      "Epoch 13/200 | Train Loss=1.0031 Acc=0.4921 F1=0.3790 || Val Loss=1.0757 Acc=0.3906 F1=0.2439\n",
      "Epoch 14/200 | Train Loss=0.9981 Acc=0.4742 F1=0.3732 || Val Loss=1.0740 Acc=0.3906 F1=0.2735\n",
      "Epoch 15/200 | Train Loss=1.0003 Acc=0.4663 F1=0.3742 || Val Loss=1.0828 Acc=0.4062 F1=0.2515\n",
      "Epoch 16/200 | Train Loss=0.9961 Acc=0.4960 F1=0.3854 || Val Loss=1.0680 Acc=0.4375 F1=0.3016\n",
      "Epoch 17/200 | Train Loss=0.9996 Acc=0.4742 F1=0.3527 || Val Loss=1.0817 Acc=0.4219 F1=0.2726\n",
      "Epoch 18/200 | Train Loss=0.9935 Acc=0.4960 F1=0.4068 || Val Loss=1.1066 Acc=0.3906 F1=0.2280\n",
      "Epoch 19/200 | Train Loss=0.9902 Acc=0.4524 F1=0.3642 || Val Loss=1.0766 Acc=0.4375 F1=0.3458\n",
      "Epoch 20/200 | Train Loss=0.9826 Acc=0.4861 F1=0.3713 || Val Loss=1.0817 Acc=0.4219 F1=0.3283\n",
      "Epoch 21/200 | Train Loss=0.9714 Acc=0.5099 F1=0.4101 || Val Loss=1.1038 Acc=0.3750 F1=0.2708\n",
      "Epoch 22/200 | Train Loss=0.9690 Acc=0.5079 F1=0.4100 || Val Loss=1.0931 Acc=0.4062 F1=0.2995\n",
      "Epoch 23/200 | Train Loss=0.9666 Acc=0.5000 F1=0.3984 || Val Loss=1.0848 Acc=0.4375 F1=0.2921\n",
      "Epoch 24/200 | Train Loss=0.9592 Acc=0.5040 F1=0.4009 || Val Loss=1.1125 Acc=0.4062 F1=0.2514\n",
      "Epoch 25/200 | Train Loss=0.9518 Acc=0.5159 F1=0.4142 || Val Loss=1.1012 Acc=0.4375 F1=0.3457\n",
      "Epoch 26/200 | Train Loss=0.9516 Acc=0.5159 F1=0.4252 || Val Loss=1.0983 Acc=0.4375 F1=0.3170\n",
      "Epoch 27/200 | Train Loss=0.9431 Acc=0.5377 F1=0.4453 || Val Loss=1.1164 Acc=0.4375 F1=0.3283\n",
      "Epoch 28/200 | Train Loss=0.9276 Acc=0.5238 F1=0.4217 || Val Loss=1.1156 Acc=0.4375 F1=0.3170\n",
      "Epoch 29/200 | Train Loss=0.9220 Acc=0.5377 F1=0.4481 || Val Loss=1.1391 Acc=0.4375 F1=0.3169\n",
      "Epoch 30/200 | Train Loss=0.9089 Acc=0.5337 F1=0.4495 || Val Loss=1.1438 Acc=0.4219 F1=0.2952\n",
      "Epoch 31/200 | Train Loss=0.9318 Acc=0.5357 F1=0.4713 || Val Loss=1.1235 Acc=0.4531 F1=0.3372\n",
      "[INFO] Early stopping triggered at epoch 31\n",
      "[RESULT] Fold 36: Test Loss=1.0537 Acc=0.3115 F1=0.1754\n",
      "\n",
      "[SUMMARY] Final Results across folds:\n",
      "  Test Loss = 1.1029 ± 0.1768\n",
      "  Test Acc  = 0.4373 ± 0.0668\n",
      "  Test F1   = 0.3012 ± 0.0439\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block 10: Run All Rolling Window Folds\n",
    "# ============================================================\n",
    "# Train and evaluate AMFNet across all rolling windows.\n",
    "# Report mean ± std across folds (as in the paper).\n",
    "# ============================================================\n",
    "\n",
    "all_results = []\n",
    "for i, (train_set, val_set, test_set) in enumerate(all_folds):\n",
    "    test_loss, test_acc, test_f1 = train_one_fold(train_set, val_set, test_set, fold_id=i)\n",
    "    all_results.append((test_loss, test_acc, test_f1))\n",
    "\n",
    "all_results = np.array(all_results)\n",
    "mean_results = all_results.mean(axis=0)\n",
    "std_results  = all_results.std(axis=0)\n",
    "\n",
    "print(\"\\n[SUMMARY] Final Results across folds:\")\n",
    "print(f\"  Test Loss = {mean_results[0]:.4f} ± {std_results[0]:.4f}\")\n",
    "print(f\"  Test Acc  = {mean_results[1]:.4f} ± {std_results[1]:.4f}\")\n",
    "print(f\"  Test F1   = {mean_results[2]:.4f} ± {std_results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1997e35e-9c81-4281-acf4-d41b44a0184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Results ==\n",
      "Accuracy: 0.2131\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.1111   |  0.6250  | 0.1887\n",
      "      1 |   0.5000   |  0.2286  | 0.3137\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block A: Classification Metrics (Accuracy, Precision, Recall, F1)\n",
    "# ============================================================\n",
    "# This block evaluates AMFNet on classification performance \n",
    "# exactly as reported in the article (Section IV.C).\n",
    "#\n",
    "# Metrics:\n",
    "#   - Global Accuracy\n",
    "#   - Per-class Precision, Recall, F1\n",
    "# Classes:\n",
    "#   {0 = Neutral, 1 = Buy, 2 = Sell}\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classification_metrics(model, loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics (Accuracy, Precision, Recall, F1) \n",
    "    on a given dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model  : trained AMFNet model\n",
    "        loader : DataLoader (val/test split)\n",
    "        device : \"cuda\" or \"cpu\"\n",
    "    Returns:\n",
    "        dict with accuracy, per-class precision/recall/F1, \n",
    "        and raw predictions/labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for xtech, St, lengths, y in loader:\n",
    "        xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "        logits, _ = model(xtech, St, lengths)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Global Accuracy\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Per-class metrics (0=Neutral, 1=Buy, 2=Sell)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None, labels=[0,1,2], zero_division=0\n",
    "    )\n",
    "\n",
    "    # Logging\n",
    "    print(\"== Classification Results ==\")\n",
    "    print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    print(\"Per-class (0=Neutral, 1=Buy, 2=Sell):\")\n",
    "    print(\"Class | Precision | Recall | F1\")\n",
    "    for cls, p, r, f in zip([0,1,2], precision, recall, f1):\n",
    "        print(f\"  {cls:5d} |   {p:.4f}   |  {r:.4f}  | {f:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_per_class\": precision.tolist(),\n",
    "        \"recall_per_class\": recall.tolist(),\n",
    "        \"f1_per_class\": f1.tolist(),\n",
    "        \"y_true\": all_labels,\n",
    "        \"y_pred\": all_preds,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Example usage (after training one fold)\n",
    "# ============================================================\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(train_set, val_set, test_set)\n",
    "\n",
    "metrics_test = evaluate_classification_metrics(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f004f80-d6bf-47fa-83ee-ade2dd29b222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4528   |  0.9231  | 0.6076\n",
      "      2 |   0.4545   |  0.2174  | 0.2941\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4310   |  0.9615  | 0.5952\n",
      "      2 |   0.5000   |  0.1304  | 0.2069\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4127   |  1.0000  | 0.5843\n",
      "      2 |   1.0000   |  0.0435  | 0.0833\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4340   |  0.8846  | 0.5823\n",
      "      2 |   0.5455   |  0.2609  | 0.3529\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4255   |  0.7692  | 0.5479\n",
      "      2 |   0.5294   |  0.3913  | 0.4500\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4583   |  0.4231  | 0.4400\n",
      "      2 |   0.4250   |  0.7391  | 0.5397\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4118   |  0.6087  | 0.4912\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4231   |  0.4231  | 0.4231\n",
      "      2 |   0.4211   |  0.6957  | 0.5246\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4400   |  0.4231  | 0.4314\n",
      "      2 |   0.4359   |  0.7391  | 0.5484\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4545   |  0.3846  | 0.4167\n",
      "      2 |   0.4286   |  0.7826  | 0.5538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4348   |  0.3846  | 0.4082\n",
      "      2 |   0.4390   |  0.7826  | 0.5625\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.3462  | 0.4091\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.3462  | 0.4091\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4762   |  0.3846  | 0.4255\n",
      "      2 |   0.4186   |  0.7826  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4000   |  0.2308  | 0.2927\n",
      "      2 |   0.3878   |  0.8261  | 0.5278\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3929   |  0.4231  | 0.4074\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5714   |  0.1538  | 0.2424\n",
      "      2 |   0.3860   |  0.9565  | 0.5500\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.2308  | 0.3000\n",
      "      2 |   0.4000   |  0.8696  | 0.5479\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.3462  | 0.3396\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.3462  | 0.3396\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4667   |  0.2692  | 0.3415\n",
      "      2 |   0.4082   |  0.8696  | 0.5556\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4186   |  0.7826  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.3462  | 0.3396\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4186   |  0.7826  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3902   |  0.6957  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.3462  | 0.3396\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.2692  | 0.3500\n",
      "      2 |   0.4000   |  0.8696  | 0.5479\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3750   |  0.4615  | 0.4138\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5385   |  0.2692  | 0.3590\n",
      "      2 |   0.4118   |  0.9130  | 0.5676\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3750   |  0.3462  | 0.3600\n",
      "      2 |   0.4000   |  0.6957  | 0.5079\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.3077  | 0.3810\n",
      "      2 |   0.4167   |  0.8696  | 0.5634\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.3077  | 0.3636\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.2692  | 0.3500\n",
      "      2 |   0.4000   |  0.8696  | 0.5479\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3750   |  0.4615  | 0.4138\n",
      "      2 |   0.3750   |  0.5217  | 0.4364\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.5556   |  0.1923  | 0.2857\n",
      "      2 |   0.3889   |  0.9130  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3704   |  0.3846  | 0.3774\n",
      "      2 |   0.3784   |  0.6087  | 0.4667\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.3462  | 0.4091\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.5000   |  0.3846  | 0.4348\n",
      "      2 |   0.4419   |  0.8261  | 0.5758\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5385   |  0.2692  | 0.3590\n",
      "      2 |   0.4118   |  0.9130  | 0.5676\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4000   |  0.3846  | 0.3922\n",
      "      2 |   0.3590   |  0.6087  | 0.4516\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.6364   |  0.2692  | 0.3784\n",
      "      2 |   0.4038   |  0.9130  | 0.5600\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.4500   |  0.3462  | 0.3913\n",
      "      2 |   0.4186   |  0.7826  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.5625   |  0.3462  | 0.4286\n",
      "      2 |   0.4255   |  0.8696  | 0.5714\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.4706   |  0.3077  | 0.3721\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4146   |  0.7391  | 0.5312\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4844\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.5500   |  0.4231  | 0.4783\n",
      "      2 |   0.4419   |  0.8261  | 0.5758\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.4667   |  0.2692  | 0.3415\n",
      "      2 |   0.3750   |  0.7826  | 0.5070\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.5000   |  0.2308  | 0.3158\n",
      "      2 |   0.4255   |  0.8696  | 0.5714\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2459\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3636   |  0.1143  | 0.1739\n",
      "      2 |   0.2340   |  0.6111  | 0.3385\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4194   |  1.0000  | 0.5909\n",
      "      2 |   0.5000   |  0.0435  | 0.0800\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3438\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3696   |  0.6538  | 0.4722\n",
      "      2 |   0.2778   |  0.2174  | 0.2439\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.4000   |  0.6957  | 0.5079\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3830   |  0.6923  | 0.4932\n",
      "      2 |   0.2941   |  0.2174  | 0.2500\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.9231  | 0.5854\n",
      "      2 |   0.5000   |  0.1739  | 0.2581\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4211   |  0.6154  | 0.5000\n",
      "      2 |   0.3846   |  0.4348  | 0.4082\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3902   |  0.6957  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4375   |  0.5385  | 0.4828\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3750   |  0.5769  | 0.4545\n",
      "      2 |   0.3333   |  0.3478  | 0.3404\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3902   |  0.6957  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4500   |  0.3462  | 0.3913\n",
      "      2 |   0.4091   |  0.7826  | 0.5373\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3939   |  0.5000  | 0.4407\n",
      "      2 |   0.3871   |  0.5217  | 0.4444\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3902   |  0.6957  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  0.5000  | 0.4483\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.4000   |  0.6957  | 0.5079\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3659   |  0.6522  | 0.4688\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4348   |  0.3846  | 0.4082\n",
      "      2 |   0.3902   |  0.6957  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3667   |  0.4231  | 0.3929\n",
      "      2 |   0.3529   |  0.5217  | 0.4211\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3939   |  0.5000  | 0.4407\n",
      "      2 |   0.3871   |  0.5217  | 0.4444\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4118   |  0.2692  | 0.3256\n",
      "      2 |   0.3830   |  0.7826  | 0.5143\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  0.5000  | 0.4483\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3913   |  0.3462  | 0.3673\n",
      "      2 |   0.3659   |  0.6522  | 0.4688\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.3889   |  0.6087  | 0.4746\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4000   |  0.3846  | 0.3922\n",
      "      2 |   0.3429   |  0.5217  | 0.4138\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4103   |  0.6154  | 0.4923\n",
      "      2 |   0.3810   |  0.3478  | 0.3636\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3500   |  0.2692  | 0.3043\n",
      "      2 |   0.3500   |  0.6087  | 0.4444\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3636   |  0.3077  | 0.3333\n",
      "      2 |   0.3421   |  0.5652  | 0.4262\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4000   |  0.5217  | 0.4528\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3438\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.3333   |  0.2692  | 0.2979\n",
      "      2 |   0.3514   |  0.5652  | 0.4333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3636   |  0.3077  | 0.3333\n",
      "      2 |   0.3514   |  0.5652  | 0.4333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3281\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.2917   |  0.2692  | 0.2800\n",
      "      2 |   0.3529   |  0.5217  | 0.4211\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.6667   |  0.1333  | 0.2222\n",
      "      1 |   0.4242   |  0.5385  | 0.4746\n",
      "      2 |   0.3929   |  0.4783  | 0.4314\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2857   |  0.1333  | 0.1818\n",
      "      1 |   0.4500   |  0.3462  | 0.3913\n",
      "      2 |   0.3784   |  0.6087  | 0.4667\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2857   |  0.1333  | 0.1818\n",
      "      1 |   0.4375   |  0.5385  | 0.4828\n",
      "      2 |   0.4800   |  0.5217  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2500   |  0.1333  | 0.1739\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2857   |  0.1333  | 0.1818\n",
      "      1 |   0.4000   |  0.3846  | 0.3922\n",
      "      2 |   0.4062   |  0.5652  | 0.4727\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.4400   |  0.4231  | 0.4314\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2857   |  0.1333  | 0.1818\n",
      "      1 |   0.4643   |  0.5000  | 0.4815\n",
      "      2 |   0.4138   |  0.5217  | 0.4615\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2500   |  0.1333  | 0.1739\n",
      "      1 |   0.4286   |  0.4615  | 0.4444\n",
      "      2 |   0.3929   |  0.4783  | 0.4314\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2500   |  0.0667  | 0.1053\n",
      "      1 |   0.4074   |  0.4231  | 0.4151\n",
      "      2 |   0.3939   |  0.5652  | 0.4643\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4324   |  0.6957  | 0.5333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4483   |  0.5652  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4211   |  0.3077  | 0.3556\n",
      "      2 |   0.3810   |  0.6957  | 0.4923\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.1667   |  0.0667  | 0.0952\n",
      "      1 |   0.4000   |  0.3846  | 0.3922\n",
      "      2 |   0.4242   |  0.6087  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4211   |  0.6957  | 0.5246\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.4375   |  0.2692  | 0.3333\n",
      "      2 |   0.3953   |  0.7391  | 0.5152\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2787\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.1429   |  0.1250  | 0.1333\n",
      "      1 |   0.3636   |  0.1143  | 0.1739\n",
      "      2 |   0.2791   |  0.6667  | 0.3934\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   0.6667   |  0.0870  | 0.1538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.5000\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4643   |  1.0000  | 0.6341\n",
      "      2 |   0.7500   |  0.2609  | 0.3871\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4364   |  0.9231  | 0.5926\n",
      "      2 |   0.6667   |  0.2609  | 0.3750\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.6154  | 0.5161\n",
      "      2 |   0.4286   |  0.5217  | 0.4706\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4800   |  0.4615  | 0.4706\n",
      "      2 |   0.4615   |  0.7826  | 0.5806\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4583   |  0.4231  | 0.4400\n",
      "      2 |   0.4500   |  0.7826  | 0.5714\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4138   |  0.4615  | 0.4364\n",
      "      2 |   0.4286   |  0.6522  | 0.5172\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4667   |  0.2692  | 0.3415\n",
      "      2 |   0.4286   |  0.9130  | 0.5833\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.2308  | 0.3000\n",
      "      2 |   0.4200   |  0.9130  | 0.5753\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4844\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.5000  | 0.5000\n",
      "      2 |   0.4737   |  0.7826  | 0.5902\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.4194   |  0.5000  | 0.4561\n",
      "      2 |   0.4194   |  0.5652  | 0.4815\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4444   |  0.3077  | 0.3636\n",
      "      2 |   0.4186   |  0.7826  | 0.5455\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4815   |  0.5000  | 0.4906\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4815   |  0.5000  | 0.4906\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4815   |  0.5000  | 0.4906\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4815   |  0.5000  | 0.4906\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4583   |  0.4231  | 0.4400\n",
      "      2 |   0.4324   |  0.6957  | 0.5333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.4231   |  0.4231  | 0.4231\n",
      "      2 |   0.4375   |  0.6087  | 0.5091\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4091   |  0.3462  | 0.3750\n",
      "      2 |   0.3846   |  0.6522  | 0.4839\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3478   |  0.3077  | 0.3265\n",
      "      2 |   0.3784   |  0.6087  | 0.4667\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.3462   |  0.3462  | 0.3462\n",
      "      2 |   0.3714   |  0.5652  | 0.4483\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3500   |  0.2692  | 0.3043\n",
      "      2 |   0.3750   |  0.6522  | 0.4762\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.3333   |  0.2692  | 0.2979\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3750   |  0.3462  | 0.3600\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3636   |  0.4615  | 0.4068\n",
      "      2 |   0.4074   |  0.4783  | 0.4400\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2857   |  0.1333  | 0.1818\n",
      "      1 |   0.3182   |  0.2692  | 0.2917\n",
      "      2 |   0.4000   |  0.6087  | 0.4828\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3200   |  0.3077  | 0.3137\n",
      "      2 |   0.4324   |  0.6957  | 0.5333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.3793   |  0.4231  | 0.4000\n",
      "      2 |   0.4242   |  0.6087  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.3333   |  0.3846  | 0.3571\n",
      "      2 |   0.4483   |  0.5652  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3600   |  0.3462  | 0.3529\n",
      "      2 |   0.4103   |  0.6957  | 0.5161\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3667   |  0.4231  | 0.3929\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3571   |  0.3846  | 0.3704\n",
      "      2 |   0.4722   |  0.7391  | 0.5763\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3704   |  0.3846  | 0.3774\n",
      "      2 |   0.3889   |  0.6087  | 0.4746\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2222   |  0.1333  | 0.1667\n",
      "      1 |   0.3462   |  0.3462  | 0.3462\n",
      "      2 |   0.5172   |  0.6522  | 0.5769\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3429   |  0.4615  | 0.3934\n",
      "      2 |   0.4815   |  0.5652  | 0.5200\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4000   |  0.3846  | 0.3922\n",
      "      2 |   0.4359   |  0.7391  | 0.5484\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.2000   |  0.0667  | 0.1000\n",
      "      1 |   0.4500   |  0.3462  | 0.3913\n",
      "      2 |   0.4615   |  0.7826  | 0.5806\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4545   |  0.3846  | 0.4167\n",
      "      2 |   0.4524   |  0.8261  | 0.5846\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4074   |  0.4231  | 0.4151\n",
      "      2 |   0.4286   |  0.6522  | 0.5172\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5455   |  0.2308  | 0.3243\n",
      "      2 |   0.4400   |  0.9565  | 0.6027\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4737   |  0.3462  | 0.4000\n",
      "      2 |   0.4524   |  0.8261  | 0.5846\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3929   |  0.4231  | 0.4074\n",
      "      2 |   0.4412   |  0.6522  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4737   |  0.3462  | 0.4000\n",
      "      2 |   0.4419   |  0.8261  | 0.5758\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.3077  | 0.3636\n",
      "      2 |   0.4545   |  0.8696  | 0.5970\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4643   |  0.5000  | 0.4815\n",
      "      2 |   0.5000   |  0.7391  | 0.5965\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4098\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.6471   |  0.3143  | 0.4231\n",
      "      2 |   0.3415   |  0.7778  | 0.4746\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.9231  | 0.5854\n",
      "      2 |   0.5000   |  0.1739  | 0.2581\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4127   |  1.0000  | 0.5843\n",
      "      2 |   1.0000   |  0.0435  | 0.0833\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4844\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4483   |  1.0000  | 0.6190\n",
      "      2 |   0.8333   |  0.2174  | 0.3448\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4200   |  0.8077  | 0.5526\n",
      "      2 |   0.5000   |  0.3043  | 0.3784\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4375   |  0.8077  | 0.5676\n",
      "      2 |   0.5000   |  0.3478  | 0.4103\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4318   |  0.7308  | 0.5429\n",
      "      2 |   0.5000   |  0.4348  | 0.4651\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.6154  | 0.5161\n",
      "      2 |   0.4286   |  0.5217  | 0.4706\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4688   |  0.5769  | 0.5172\n",
      "      2 |   0.4375   |  0.6087  | 0.5091\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4483   |  0.5000  | 0.4727\n",
      "      2 |   0.4286   |  0.6522  | 0.5172\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4545   |  0.5769  | 0.5085\n",
      "      2 |   0.4194   |  0.5652  | 0.4815\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4138   |  0.4615  | 0.4364\n",
      "      2 |   0.4000   |  0.6087  | 0.4828\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4074   |  0.4231  | 0.4151\n",
      "      2 |   0.4054   |  0.6522  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4000   |  0.4615  | 0.4286\n",
      "      2 |   0.3824   |  0.5652  | 0.4561\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4500   |  0.3462  | 0.3913\n",
      "      2 |   0.4091   |  0.7826  | 0.5373\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4348   |  0.3846  | 0.4082\n",
      "      2 |   0.4146   |  0.7391  | 0.5312\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4348   |  0.3846  | 0.4082\n",
      "      2 |   0.4146   |  0.7391  | 0.5312\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4138   |  0.4615  | 0.4364\n",
      "      2 |   0.3750   |  0.5217  | 0.4364\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.3784   |  0.6087  | 0.4667\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4138   |  0.4615  | 0.4364\n",
      "      2 |   0.3750   |  0.5217  | 0.4364\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4286   |  0.3462  | 0.3830\n",
      "      2 |   0.4000   |  0.6957  | 0.5079\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4074   |  0.4231  | 0.4151\n",
      "      2 |   0.3824   |  0.5652  | 0.4561\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4000   |  0.3077  | 0.3478\n",
      "      2 |   0.3659   |  0.6522  | 0.4688\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4167   |  0.3846  | 0.4000\n",
      "      2 |   0.3784   |  0.6087  | 0.4667\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4000   |  0.4615  | 0.4286\n",
      "      2 |   0.4000   |  0.5217  | 0.4528\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.4091   |  0.3462  | 0.3750\n",
      "      2 |   0.3590   |  0.6087  | 0.4516\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.0667  | 0.1111\n",
      "      1 |   0.3684   |  0.2692  | 0.3111\n",
      "      2 |   0.3571   |  0.6522  | 0.4615\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3636   |  0.3077  | 0.3333\n",
      "      2 |   0.3684   |  0.6087  | 0.4590\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3750   |  0.3462  | 0.3600\n",
      "      2 |   0.3714   |  0.5652  | 0.4483\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3281\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3077   |  0.3077  | 0.3077\n",
      "      2 |   0.3333   |  0.4783  | 0.3929\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.5455   |  0.2308  | 0.3243\n",
      "      2 |   0.3878   |  0.8261  | 0.5278\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3333   |  0.2692  | 0.2979\n",
      "      2 |   0.3590   |  0.6087  | 0.4516\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3600   |  0.3462  | 0.3529\n",
      "      2 |   0.3714   |  0.5652  | 0.4483\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3636   |  0.3077  | 0.3333\n",
      "      2 |   0.3947   |  0.6522  | 0.4918\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3462   |  0.3462  | 0.3462\n",
      "      2 |   0.3529   |  0.5217  | 0.4211\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3810   |  0.3077  | 0.3404\n",
      "      2 |   0.3947   |  0.6522  | 0.4918\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.6667   |  0.1333  | 0.2222\n",
      "      1 |   0.3600   |  0.3462  | 0.3529\n",
      "      2 |   0.3889   |  0.6087  | 0.4746\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.6667   |  0.1333  | 0.2222\n",
      "      1 |   0.5455   |  0.2308  | 0.3243\n",
      "      2 |   0.3800   |  0.8261  | 0.5205\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4000   |  0.3077  | 0.3478\n",
      "      2 |   0.3750   |  0.6522  | 0.4762\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.3889   |  0.2692  | 0.3182\n",
      "      2 |   0.4048   |  0.7391  | 0.5231\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3281\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.3500   |  0.2692  | 0.3043\n",
      "      2 |   0.3158   |  0.5217  | 0.3934\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3478   |  0.3077  | 0.3265\n",
      "      2 |   0.3611   |  0.5652  | 0.4407\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2969\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.3158   |  0.2308  | 0.2667\n",
      "      2 |   0.2821   |  0.4783  | 0.3548\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3438\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3462   |  0.3462  | 0.3462\n",
      "      2 |   0.3333   |  0.4783  | 0.3929\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.6667   |  0.1333  | 0.2222\n",
      "      1 |   0.4375   |  0.2692  | 0.3333\n",
      "      2 |   0.3556   |  0.6957  | 0.4706\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2969\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.4000   |  0.1333  | 0.2000\n",
      "      1 |   0.3077   |  0.3077  | 0.3077\n",
      "      2 |   0.2727   |  0.3913  | 0.3214\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.6667   |  0.1333  | 0.2222\n",
      "      1 |   0.4375   |  0.2692  | 0.3333\n",
      "      2 |   0.3778   |  0.7391  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2812\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.3333   |  0.1333  | 0.1905\n",
      "      1 |   0.2692   |  0.2692  | 0.2692\n",
      "      2 |   0.2812   |  0.3913  | 0.3273\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4545   |  0.1923  | 0.2703\n",
      "      2 |   0.3725   |  0.8261  | 0.5135\n",
      "== Classification Results ==\n",
      "Accuracy: 0.2951\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.0571  | 0.1026\n",
      "      2 |   0.2857   |  0.8889  | 0.4324\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4062   |  1.0000  | 0.5778\n",
      "      2 |   0.0000   |  0.0000  | 0.0000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4262   |  1.0000  | 0.5977\n",
      "      2 |   1.0000   |  0.1304  | 0.2308\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4386   |  0.9615  | 0.6024\n",
      "      2 |   0.7143   |  0.2174  | 0.3333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4386   |  0.9615  | 0.6024\n",
      "      2 |   0.7143   |  0.2174  | 0.3333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4167   |  0.7692  | 0.5405\n",
      "      2 |   0.4375   |  0.3043  | 0.3590\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4194   |  0.5000  | 0.4561\n",
      "      2 |   0.4242   |  0.6087  | 0.5000\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4844\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.4615  | 0.4800\n",
      "      2 |   0.4750   |  0.8261  | 0.6032\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4118   |  0.6087  | 0.4912\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4118   |  0.6087  | 0.4912\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.4231  | 0.4583\n",
      "      2 |   0.4524   |  0.8261  | 0.5846\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.4231  | 0.4583\n",
      "      2 |   0.4524   |  0.8261  | 0.5846\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4333   |  0.5000  | 0.4643\n",
      "      2 |   0.4118   |  0.6087  | 0.4912\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4531\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4583   |  0.4231  | 0.4400\n",
      "      2 |   0.4500   |  0.7826  | 0.5714\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.3077  | 0.3636\n",
      "      2 |   0.4130   |  0.8261  | 0.5507\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.4615  | 0.4528\n",
      "      2 |   0.4324   |  0.6957  | 0.5333\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.4615  | 0.4444\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4688\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4800   |  0.4615  | 0.4706\n",
      "      2 |   0.4615   |  0.7826  | 0.5806\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4138   |  0.4615  | 0.4364\n",
      "      2 |   0.4286   |  0.6522  | 0.5172\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4348   |  0.3846  | 0.4082\n",
      "      2 |   0.4390   |  0.7826  | 0.5625\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3846   |  0.3846  | 0.3846\n",
      "      2 |   0.4211   |  0.6957  | 0.5246\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4231   |  0.4231  | 0.4231\n",
      "      2 |   0.4211   |  0.6957  | 0.5246\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3929   |  0.4231  | 0.4074\n",
      "      2 |   0.4167   |  0.6522  | 0.5085\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4375\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4762   |  0.3846  | 0.4255\n",
      "      2 |   0.4286   |  0.7826  | 0.5538\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.2692  | 0.3500\n",
      "      2 |   0.3878   |  0.8261  | 0.5278\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.1333  | 0.2105\n",
      "      1 |   0.4444   |  0.3077  | 0.3636\n",
      "      2 |   0.4048   |  0.7391  | 0.5231\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.3846   |  0.3846  | 0.3846\n",
      "      2 |   0.4444   |  0.6957  | 0.5424\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4375   |  0.2692  | 0.3333\n",
      "      2 |   0.3958   |  0.8261  | 0.5352\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   1.0000   |  0.0667  | 0.1250\n",
      "      1 |   0.3810   |  0.3077  | 0.3404\n",
      "      2 |   0.4048   |  0.7391  | 0.5231\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4737   |  0.3462  | 0.4000\n",
      "      2 |   0.4000   |  0.7826  | 0.5294\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.7143   |  0.1923  | 0.3030\n",
      "      2 |   0.3818   |  0.9130  | 0.5385\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3438\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.1538  | 0.2105\n",
      "      2 |   0.3462   |  0.7826  | 0.4800\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.2692  | 0.3500\n",
      "      2 |   0.3800   |  0.8261  | 0.5205\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5556   |  0.1923  | 0.2857\n",
      "      2 |   0.3818   |  0.9130  | 0.5385\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3846   |  0.1923  | 0.2564\n",
      "      2 |   0.3529   |  0.7826  | 0.4865\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4219\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5333   |  0.3077  | 0.3902\n",
      "      2 |   0.3878   |  0.8261  | 0.5278\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5455   |  0.2308  | 0.3243\n",
      "      2 |   0.3774   |  0.8696  | 0.5263\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4286   |  0.2308  | 0.3000\n",
      "      2 |   0.3600   |  0.7826  | 0.4932\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5385   |  0.2692  | 0.3590\n",
      "      2 |   0.3725   |  0.8261  | 0.5135\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4444   |  0.1538  | 0.2286\n",
      "      2 |   0.3636   |  0.8696  | 0.5128\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4118   |  0.2692  | 0.3256\n",
      "      2 |   0.3404   |  0.6957  | 0.4571\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3846   |  0.1923  | 0.2564\n",
      "      2 |   0.3529   |  0.7826  | 0.4865\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.6250   |  0.1923  | 0.2941\n",
      "      2 |   0.3750   |  0.9130  | 0.5316\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3594\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.4000   |  0.2308  | 0.2927\n",
      "      2 |   0.3469   |  0.7391  | 0.4722\n",
      "== Classification Results ==\n",
      "Accuracy: 0.4062\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.5000   |  0.0667  | 0.1176\n",
      "      1 |   0.4118   |  0.2692  | 0.3256\n",
      "      2 |   0.4000   |  0.7826  | 0.5294\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3281\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.1923  | 0.2439\n",
      "      2 |   0.3265   |  0.6957  | 0.4444\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3906\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5556   |  0.1923  | 0.2857\n",
      "      2 |   0.3636   |  0.8696  | 0.5128\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3125\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3333   |  0.1923  | 0.2439\n",
      "      2 |   0.3061   |  0.6522  | 0.4167\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3750\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.5000   |  0.1538  | 0.2353\n",
      "      2 |   0.3571   |  0.8696  | 0.5063\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3281\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.3529   |  0.2308  | 0.2791\n",
      "      2 |   0.3191   |  0.6522  | 0.4286\n",
      "== Classification Results ==\n",
      "Accuracy: 0.3607\n",
      "\n",
      "Per-class (0=Neutral, 1=Buy, 2=Sell):\n",
      "Class | Precision | Recall | F1\n",
      "      0 |   0.0000   |  0.0000  | 0.0000\n",
      "      1 |   0.6667   |  0.1143  | 0.1951\n",
      "      2 |   0.3396   |  1.0000  | 0.5070\n",
      "== Statistical Validation (multi-seed) ==\n",
      "Accuracy : 0.3180 ± 0.0662 (95% CI: 0.2600 – 0.3760)\n",
      "F1       : 0.2232 ± 0.0519 (95% CI: 0.1777 – 0.2687)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block B: Statistical Validation (multi-seed + 95% CIs)\n",
    "# ============================================================\n",
    "# - Train AMFNet multiple times with different seeds\n",
    "# - Select best checkpoint per seed by validation F1\n",
    "# - Report mean ± std and 95% CI for Accuracy and F1\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    \"\"\"Ensure reproducibility across runs.\"\"\"\n",
    "    import random, numpy as _np, torch as _torch\n",
    "    random.seed(seed); _np.random.seed(seed); _torch.manual_seed(seed)\n",
    "    if _torch.cuda.is_available():\n",
    "        _torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ci95(x):\n",
    "    \"\"\"95% confidence interval under normal approximation.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mean = x.mean()\n",
    "    std = x.std(ddof=1) if len(x) > 1 else 0.0\n",
    "    half = 1.96 * (std / np.sqrt(len(x))) if len(x) > 1 else 0.0\n",
    "    return mean, std, (mean - half, mean + half)\n",
    "\n",
    "def train_one_run(train_loader, val_loader, test_loader, device, epochs=200, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train AMFNet once with a given seed.\n",
    "    Save best checkpoint based on validation F1.\n",
    "    \"\"\"\n",
    "    # Fresh model\n",
    "    model = AMFNet(tech_dim=7, sent_dim=3, hidden_dim=64).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- training ---\n",
    "        model.train()\n",
    "        for xtech, St, lengths, y in train_loader:\n",
    "            xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "            logits, _ = model(xtech, St, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        # --- validation (F1) ---\n",
    "        model.eval()\n",
    "        val = evaluate_classification_metrics(model, val_loader, device)\n",
    "        f1_val = np.mean(val[\"f1_per_class\"])  # macro-F1 across 3 classes\n",
    "        if f1_val > best_val_f1:\n",
    "            best_val_f1 = f1_val\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # Reload best checkpoint\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    # --- final test evaluation ---\n",
    "    test = evaluate_classification_metrics(model, test_loader, device)\n",
    "    f1_test = np.mean(test[\"f1_per_class\"])\n",
    "    return test[\"accuracy\"], f1_test\n",
    "\n",
    "def statistical_validation(train_loader, val_loader, test_loader, device,\n",
    "                           seeds=(42, 43, 44, 45, 46), epochs=200):\n",
    "    \"\"\"\n",
    "    Run multiple seeds and report mean ± std and 95% CI.\n",
    "    \"\"\"\n",
    "    accs, f1s = [], []\n",
    "    for s in seeds:\n",
    "        set_global_seed(s)\n",
    "        acc, f1 = train_one_run(train_loader, val_loader, test_loader, device, epochs=epochs)\n",
    "        accs.append(acc); f1s.append(f1)\n",
    "\n",
    "    acc_mean, acc_std, acc_ci = ci95(accs)\n",
    "    f1_mean, f1_std, f1_ci = ci95(f1s)\n",
    "\n",
    "    print(\"== Statistical Validation (multi-seed) ==\")\n",
    "    print(f\"Accuracy : {acc_mean:.4f} ± {acc_std:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})\")\n",
    "    print(f\"F1       : {f1_mean:.4f} ± {f1_std:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})\")\n",
    "\n",
    "    return {\"accs\": accs, \"f1s\": f1s, \"acc_ci\": acc_ci, \"f1_ci\": f1_ci}\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (after defining loaders for a fold):\n",
    "# ----------------------------\n",
    "stats = statistical_validation(train_loader, val_loader, test_loader,\n",
    "                               device, seeds=[11,12,13,14,15], epochs=50)  # shorter for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a311670e-0e9a-400a-a87a-79eb0bc82f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block C: Portfolio-based Evaluation\n",
    "# ============================================================\n",
    "# Converts class predictions into trading signals {-1, 0, +1},\n",
    "# computes portfolio metrics (CR, Sharpe, MDD, HR),\n",
    "# both with and without transaction costs (10 bps).\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import yfinance as yf\n",
    "\n",
    "def signals_from_preds(y_pred):\n",
    "    \"\"\"Map class predictions {0=Neutral, 1=Buy, 2=Sell} -> trading signals {0,+1,-1}\"\"\"\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    sig = np.zeros_like(y_pred, dtype=np.int8)\n",
    "    sig[y_pred == 1] = +1   # Buy\n",
    "    sig[y_pred == 2] = -1   # Sell\n",
    "    return sig\n",
    "\n",
    "def portfolio_metrics(strategy_rets):\n",
    "    \"\"\"Compute CR, Sharpe, MDD, HR from daily strategy returns.\"\"\"\n",
    "    strategy_rets = np.asarray(strategy_rets, dtype=float)\n",
    "    strategy_rets = np.nan_to_num(strategy_rets, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    strategy_rets = np.clip(strategy_rets, -0.5, 0.5)  # safeguard\n",
    "\n",
    "    # Cumulative return\n",
    "    cum_curve = np.cumprod(1.0 + strategy_rets)\n",
    "    cr = cum_curve[-1] - 1.0\n",
    "\n",
    "    # Sharpe ratio\n",
    "    sr = strategy_rets.mean() / (strategy_rets.std(ddof=1) + 1e-12)\n",
    "\n",
    "    # Max Drawdown\n",
    "    peak = np.maximum.accumulate(cum_curve)\n",
    "    mdd = (cum_curve / peak - 1.0).min()\n",
    "\n",
    "    # Hit Ratio\n",
    "    hr = (strategy_rets > 0).mean()\n",
    "\n",
    "    return {\"CR\": cr, \"Sharpe\": sr, \"MDD\": mdd, \"HitRatio\": hr}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_portfolio(model, loader, base_dataset, costs_bps=10,\n",
    "                       device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate portfolio performance using AMFNet predictions.\n",
    "    Returns metrics with and without transaction costs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, rets = [], []\n",
    "    idx_offset = getattr(loader.dataset, \"indices\", None)\n",
    "    running_index = 0\n",
    "\n",
    "    for xtech, St, length, y in loader:\n",
    "        xtech, St, length = xtech.to(device), St.to(device), length.to(device)\n",
    "        logits, _ = model(xtech, St, length)\n",
    "        batch_pred = logits.argmax(dim=1).cpu().numpy()\n",
    "        preds.extend(batch_pred)\n",
    "\n",
    "        bsz = len(batch_pred)\n",
    "        if idx_offset is not None:\n",
    "            batch_idx = loader.dataset.indices[running_index:running_index+bsz]\n",
    "            batch_rets = base_dataset.r_t1[batch_idx]\n",
    "        else:\n",
    "            batch_rets = base_dataset.r_t1[running_index:running_index+bsz]\n",
    "        rets.extend(batch_rets)\n",
    "        running_index += bsz\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    rets = np.asarray(rets, dtype=float)\n",
    "\n",
    "    # Strategy returns without costs\n",
    "    sig = signals_from_preds(preds)\n",
    "    strat_rets_raw = sig * rets\n",
    "\n",
    "    # Strategy returns with transaction costs\n",
    "    costs = costs_bps / 10000.0\n",
    "    sig_shift = np.roll(sig, 1); sig_shift[0] = 0\n",
    "    trades = (sig != sig_shift).astype(int)\n",
    "    strat_rets_cost = strat_rets_raw - trades * costs\n",
    "\n",
    "    # Portfolio metrics\n",
    "    no_cost = portfolio_metrics(strat_rets_raw)\n",
    "    with_cost = portfolio_metrics(strat_rets_cost)\n",
    "\n",
    "    print(\"== Portfolio metrics (no costs) ==\")\n",
    "    print(f\"CR: {no_cost['CR']*100:6.2f}% | Sharpe: {no_cost['Sharpe']:.2f} | \"\n",
    "          f\"MDD: {no_cost['MDD']*100:6.2f}% | HR: {no_cost['HitRatio']*100:5.1f}%\")\n",
    "    print(\"== Portfolio metrics (10 bps costs) ==\")\n",
    "    print(f\"CR: {with_cost['CR']*100:6.2f}% | Sharpe: {with_cost['Sharpe']:.2f} | \"\n",
    "          f\"MDD: {with_cost['MDD']*100:6.2f}% | HR: {with_cost['HitRatio']*100:5.1f}%\")\n",
    "\n",
    "    return {\n",
    "        \"no_cost\": {**no_cost, \"strategy_rets\": strat_rets_raw},\n",
    "        \"with_cost\": {**with_cost, \"strategy_rets\": strat_rets_cost},\n",
    "        \"signals\": sig,\n",
    "        \"rets\": rets\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (AAPL demo)\n",
    "# ----------------------------\n",
    "df_aapl = yf.download(\"AAPL\", start=\"2015-01-01\", end=\"2024-01-01\", auto_adjust=False)\n",
    "df_aapl = compute_indicators(df_aapl)  # function from Block 8\n",
    "ds_aapl = StockDataset(df_aapl, n_docs_max=20, n_models=3, delta=0.002)\n",
    "\n",
    "# use last 15% as test set\n",
    "test_set_aapl = Subset(ds_aapl, range(int(0.85*len(ds_aapl)), len(ds_aapl)))\n",
    "test_loader_aapl = DataLoader(test_set_aapl, batch_size=64, shuffle=False)\n",
    "\n",
    "portfolio_results = evaluate_portfolio(model, test_loader_aapl, ds_aapl, costs_bps=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4711efc-0f29-4c6a-9102-433acffdd2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating AAPL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating MSFT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating GOOG ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating AMZN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating META ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating NVDA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating TSLA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating JPM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating BAC ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating XOM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating PFE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Evaluating INTC ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Portfolio metrics (no costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "== Portfolio metrics (10 bps costs) ==\n",
      "CR:   0.00% | Sharpe: 0.00 | MDD:   0.00% | HR:   0.0%\n",
      "\n",
      "=== Aggregated Portfolio Results (across tickers) ===\n",
      "No Costs:\n",
      "CR      : 0.0000 ± 0.0000\n",
      "Sharpe  : 0.0000 ± 0.0000\n",
      "MDD     : 0.0000 ± 0.0000\n",
      "HitRatio: 0.0000 ± 0.0000\n",
      "\n",
      "With Costs (10 bps):\n",
      "CR      : 0.0000 ± 0.0000\n",
      "Sharpe  : 0.0000 ± 0.0000\n",
      "MDD     : 0.0000 ± 0.0000\n",
      "HitRatio: 0.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block Cbis: Multi-ticker Portfolio Evaluation\n",
    "# ============================================================\n",
    "# Runs evaluate_portfolio for each ticker and aggregates results\n",
    "# across all tickers: mean ± std of CR, Sharpe, MDD, HR.\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_portfolio_multi(model, tickers, start_date=\"2015-01-01\", end_date=\"2024-01-01\",\n",
    "                             costs_bps=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    results_no_cost, results_with_cost = [], []\n",
    "\n",
    "    for t in tickers:\n",
    "        print(f\"\\n=== Evaluating {t} ===\")\n",
    "        df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "        df = compute_indicators(df)  # from Block 8\n",
    "\n",
    "        ds_t = StockDataset(df, n_docs_max=20, n_models=3, delta=0.002)\n",
    "        test_set_t = Subset(ds_t, range(int(0.85*len(ds_t)), len(ds_t)))  # last 15% = test\n",
    "        test_loader_t = DataLoader(test_set_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        res = evaluate_portfolio(model, test_loader_t, ds_t, costs_bps=costs_bps, device=device)\n",
    "        results_no_cost.append(res[\"no_cost\"])\n",
    "        results_with_cost.append(res[\"with_cost\"])\n",
    "\n",
    "    def agg_metrics(res_list):\n",
    "        metrics = {}\n",
    "        for key in [\"CR\", \"Sharpe\", \"MDD\", \"HitRatio\"]:\n",
    "            values = [r[key] for r in res_list]\n",
    "            metrics[key] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"std\": np.std(values, ddof=1)\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    agg_no_cost = agg_metrics(results_no_cost)\n",
    "    agg_with_cost = agg_metrics(results_with_cost)\n",
    "\n",
    "    print(\"\\n=== Aggregated Portfolio Results (across tickers) ===\")\n",
    "    print(\"No Costs:\")\n",
    "    for k, v in agg_no_cost.items():\n",
    "        print(f\"{k:8s}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
    "    print(\"\\nWith Costs (10 bps):\")\n",
    "    for k, v in agg_with_cost.items():\n",
    "        print(f\"{k:8s}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
    "\n",
    "    return {\"no_cost\": agg_no_cost, \"with_cost\": agg_with_cost}\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage on multiple tickers\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"MSFT\",\"GOOG\",\"AMZN\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"BAC\",\"XOM\",\"PFE\",\"INTC\"]\n",
    "portfolio_summary = evaluate_portfolio_multi(model, tickers, costs_bps=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbddf8f0-7a3e-4834-ab2c-490a05f5e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Average Modality Attention per Sector ==\n",
      "             Sector  Alpha_tech  Alpha_sent\n",
      "0  Consumer Staples         1.0         0.0\n",
      "1            Energy         1.0         0.0\n",
      "2           Finance         1.0         0.0\n",
      "3        Healthcare         1.0         0.0\n",
      "4       Industrials         1.0         0.0\n",
      "5        Technology         1.0         0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block D: Average Modality Attention per Sector\n",
    "# ============================================================\n",
    "# Computes average attention weights (α_tech, α_sent) per sector\n",
    "# for the given tickers and plots a grouped bar chart.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_plot_attention_by_sector(model, tickers, sector_map,\n",
    "                                          start_date=\"2015-01-01\", end_date=\"2024-01-01\",\n",
    "                                          device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                          title=\"Average Modality Attention by Sector (2023–2024)\"):\n",
    "    results = []\n",
    "\n",
    "    for t in tickers:\n",
    "        df = yf.download(t, start=start_date, end=end_date, auto_adjust=False)\n",
    "        df = compute_indicators(df).dropna()\n",
    "\n",
    "        ds_t = StockDataset(df, n_docs_max=20, n_models=3, delta=0.002)\n",
    "        test_set_t = Subset(ds_t, range(int(0.85*len(ds_t)), len(ds_t)))  # last 15% as test\n",
    "        test_loader_t = DataLoader(test_set_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "        alphas = []\n",
    "\n",
    "        for xtech, St, lengths, y in test_loader_t:\n",
    "            xtech, St, lengths = xtech.to(device), St.to(device), lengths.to(device)\n",
    "            _, alpha = model(xtech, St, lengths)  # alpha: [B, 2]\n",
    "            alphas.append(alpha.cpu().numpy())\n",
    "\n",
    "        if alphas:\n",
    "            alphas = np.concatenate(alphas, axis=0)\n",
    "            mean_alpha = alphas.mean(axis=0)  # [2] -> [α_tech, α_sent]\n",
    "            results.append({\n",
    "                \"Ticker\": t,\n",
    "                \"Sector\": sector_map[t],  # strict mapping\n",
    "                \"Alpha_tech\": mean_alpha[0],\n",
    "                \"Alpha_sent\": mean_alpha[1]\n",
    "            })\n",
    "\n",
    "    # Aggregate per sector\n",
    "    df_alpha = pd.DataFrame(results)\n",
    "    df_sector = df_alpha.groupby(\"Sector\")[[\"Alpha_tech\", \"Alpha_sent\"]].mean().reset_index()\n",
    "\n",
    "    print(\"\\n== Average Modality Attention per Sector ==\")\n",
    "    print(df_sector)\n",
    "\n",
    "    # -------- Plot grouped bar chart --------\n",
    "    sectors = df_sector[\"Sector\"].tolist()\n",
    "    alpha_sent = df_sector[\"Alpha_sent\"].values\n",
    "    alpha_tech = df_sector[\"Alpha_tech\"].values\n",
    "\n",
    "    x = np.arange(len(sectors))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(x - width/2, alpha_sent, width, label=\"Sentiment Attention\")\n",
    "    ax.bar(x + width/2, alpha_tech, width, label=\"Technical Attention\")\n",
    "\n",
    "    ax.set_ylabel(\"Average Attention Weight\")\n",
    "    ax.set_xlabel(\"Sector\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sectors, rotation=20, ha=\"right\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_alpha, df_sector\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "tickers = [\"AAPL\",\"NVDA\",\"JPM\",\"BAC\",\"JNJ\",\"PFE\",\"XOM\",\"CVX\",\"BA\",\"CAT\",\"KO\",\"PG\"]\n",
    "sector_map = {\n",
    "    # Technology\n",
    "    \"AAPL\": \"Technology\",\n",
    "    \"NVDA\": \"Technology\",\n",
    "\n",
    "    # Finance\n",
    "    \"JPM\": \"Finance\",\n",
    "    \"BAC\": \"Finance\",\n",
    "\n",
    "    # Healthcare\n",
    "    \"JNJ\": \"Healthcare\",\n",
    "    \"PFE\": \"Healthcare\",\n",
    "\n",
    "    # Energy\n",
    "    \"XOM\": \"Energy\",\n",
    "    \"CVX\": \"Energy\",\n",
    "\n",
    "    # Industrials\n",
    "    \"BA\": \"Industrials\",\n",
    "    \"CAT\": \"Industrials\",\n",
    "\n",
    "    # Consumer Staples\n",
    "    \"KO\": \"Consumer Staples\",\n",
    "    \"PG\": \"Consumer Staples\"\n",
    "}\n",
    "df_alpha, df_sector = evaluate_and_plot_attention_by_sector(model, tickers, sector_map, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd141d0-a18c-460b-9ff2-5828babddb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Bentaleb\\AppData\\Local\\Temp\\ipykernel_9448\\1164852775.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  f1_quarter = df_eval.groupby(\"Quarter\").apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Average attention weights under market regimes ==\n",
      "             Market Period  α_sent  ModalityRatio\n",
      "          Stable (Q1 2023)     0.0            0.0\n",
      "Volatile (Q2 2023–Q1 2024)     0.0            0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block E: Quarterly F1-score + Market Regime Analysis\n",
    "# ============================================================\n",
    "# 1. Computes quarterly F1-score of AMFNet over the 2023–2024 test period.\n",
    "# 2. Computes average modality attention weights (α_sent, α_tech)\n",
    "#    under different market regimes (Stable vs Volatile),\n",
    "#    and reports the Modality Ratio (α_sent / α_tech).\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def quarterly_f1_scores(model, base_dataset, test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute quarterly macro-F1 scores for the test set.\n",
    "    Args:\n",
    "        model        : trained AMFNet model\n",
    "        base_dataset : StockDataset (provides aligned dates)\n",
    "        test_loader  : DataLoader for the test split\n",
    "        device       : \"cuda\" or \"cpu\"\n",
    "    Returns:\n",
    "        pandas.Series with quarterly macro-F1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_dates = [], [], []\n",
    "\n",
    "    # Dates aligned with returns (shifted because of r_t1)\n",
    "    dates = base_dataset.df.index[1:]\n",
    "    test_indices = test_loader.dataset.indices if hasattr(test_loader.dataset, \"indices\") else range(len(test_loader.dataset))\n",
    "\n",
    "    for batch_i, (xtech, St, lengths, y) in enumerate(test_loader):\n",
    "        xtech, St, lengths, y = xtech.to(device), St.to(device), lengths.to(device), y.to(device)\n",
    "        logits, _ = model(xtech, St, lengths)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "\n",
    "        start = batch_i * test_loader.batch_size\n",
    "        end = start + len(preds)\n",
    "        batch_dates = dates[test_indices[start:end]]\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        all_dates.extend(batch_dates)\n",
    "\n",
    "    df_eval = pd.DataFrame({\n",
    "        \"Date\": pd.to_datetime(all_dates),\n",
    "        \"y_true\": all_labels,\n",
    "        \"y_pred\": all_preds\n",
    "    })\n",
    "    df_eval[\"Quarter\"] = df_eval[\"Date\"].dt.to_period(\"Q\")\n",
    "\n",
    "    # Compute macro-F1 per quarter\n",
    "    f1_quarter = df_eval.groupby(\"Quarter\").apply(\n",
    "        lambda g: f1_score(g[\"y_true\"], g[\"y_pred\"], average=\"macro\")\n",
    "    )\n",
    "    return f1_quarter\n",
    "\n",
    "def plot_quarterly_f1(f1_quarter, title=\"Quarterly F1-score of AMFNet (2023–2024)\"):\n",
    "    \"\"\"\n",
    "    Plot a line chart of quarterly macro-F1 scores.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(f1_quarter.index.astype(str), f1_quarter.values, marker=\"o\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Quarter\")\n",
    "    ax.set_ylabel(\"F1-score\")\n",
    "    ax.set_ylim(0.84, 0.89)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def attention_by_market_regime(model, base_dataset, test_loader, \n",
    "                               stable_period=(\"2023Q1\", \"2023Q1\"), \n",
    "                               volatile_period=(\"2023Q2\", \"2024Q1\"),\n",
    "                               device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute average α_sent and modality ratio (α_sent/α_tech)\n",
    "    for stable vs volatile market regimes.\n",
    "    Args:\n",
    "        model        : trained AMFNet\n",
    "        base_dataset : StockDataset with aligned dates\n",
    "        test_loader  : DataLoader for test split\n",
    "        stable_period: tuple (\"start_quarter\", \"end_quarter\")\n",
    "        volatile_period: tuple (\"start_quarter\", \"end_quarter\")\n",
    "    Returns:\n",
    "        pandas.DataFrame with regime statistics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    alphas, all_dates = [], []\n",
    "\n",
    "    dates = base_dataset.df.index[1:]\n",
    "    test_indices = test_loader.dataset.indices if hasattr(test_loader.dataset, \"indices\") else range(len(test_loader.dataset))\n",
    "\n",
    "    running_index = 0\n",
    "    for xtech, St, lengths, y in test_loader:\n",
    "        xtech, St, lengths = xtech.to(device), St.to(device), lengths.to(device)\n",
    "        _, alpha = model(xtech, St, lengths)  # [B,2]\n",
    "        bsz = len(alpha)\n",
    "        batch_dates = dates[test_indices[running_index:running_index+bsz]]\n",
    "        running_index += bsz\n",
    "\n",
    "        alphas.append(alpha.cpu().numpy())\n",
    "        all_dates.extend(batch_dates)\n",
    "\n",
    "    alphas = np.concatenate(alphas, axis=0)\n",
    "    df_alpha = pd.DataFrame(alphas, columns=[\"Alpha_tech\",\"Alpha_sent\"])\n",
    "    df_alpha[\"Date\"] = pd.to_datetime(all_dates)\n",
    "    df_alpha[\"Quarter\"] = df_alpha[\"Date\"].dt.to_period(\"Q\")\n",
    "\n",
    "    # Stable period\n",
    "    df_stable = df_alpha[df_alpha[\"Quarter\"].astype(str).between(stable_period[0], stable_period[1])]\n",
    "    alpha_sent_stable = df_stable[\"Alpha_sent\"].mean()\n",
    "    alpha_tech_stable = df_stable[\"Alpha_tech\"].mean()\n",
    "    ratio_stable = alpha_sent_stable / (alpha_tech_stable + 1e-12)\n",
    "\n",
    "    # Volatile period\n",
    "    df_vol = df_alpha[df_alpha[\"Quarter\"].astype(str).between(volatile_period[0], volatile_period[1])]\n",
    "    alpha_sent_vol = df_vol[\"Alpha_sent\"].mean()\n",
    "    alpha_tech_vol = df_vol[\"Alpha_tech\"].mean()\n",
    "    ratio_vol = alpha_sent_vol / (alpha_tech_vol + 1e-12)\n",
    "\n",
    "    # Create results table\n",
    "    table = pd.DataFrame([\n",
    "        {\"Market Period\": \"Stable (Q1 2023)\", \"α_sent\": round(alpha_sent_stable,2), \"ModalityRatio\": round(ratio_stable,2)},\n",
    "        {\"Market Period\": \"Volatile (Q2 2023–Q1 2024)\", \"α_sent\": round(alpha_sent_vol,2), \"ModalityRatio\": round(ratio_vol,2)}\n",
    "    ])\n",
    "    print(\"\\n== Average attention weights under market regimes ==\")\n",
    "    print(table.to_string(index=False))\n",
    "    return table\n",
    "\n",
    "# Quarterly F1-score\n",
    "f1_quarter = quarterly_f1_scores(model, ds_aapl, test_loader_aapl, device=device)\n",
    "plot_quarterly_f1(f1_quarter)\n",
    "\n",
    "# Market regime attention\n",
    "table_regimes = attention_by_market_regime(model, ds_aapl, test_loader_aapl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a325c-36cf-48f2-8755-b16c6ce20602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
